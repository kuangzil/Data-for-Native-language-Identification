{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4218cfd",
   "metadata": {},
   "source": [
    "My problem remains the same -- Distinguish the English text beding generated by Japanese Native speakers and English Native speakers by BERT embedding and MLP. \n",
    "\n",
    "However the original data was insufficient in quantity: there was 200-400 words for each text and 60 for English native speakers and 60 for Japanese speakers. Therfore, in order to make the data quantity engough for training, I took two steps: 1. I split the each data equally into 10 parts and store them into the dataframe, marking English natives ad 0, Japanese natives as 1. Then, I used GPT 2 fine tuning to generate new text. New generated datasets are 4000 sets in total-- 2000 English natives (labeled as 0) and 2000 Japanese natives(labeled as 1). Then, I combined the original dataset (splited) with the current generated dataset to ctreate a dataset with 5200 data points-- 2600 for English natives (labeled as 0) and 2000 for Japanese natives (labeled as 1).\n",
    "\n",
    "#Dataframes being (original, generated, and combined are all in github: https://github.com/kuangzil/Data-for-Native-language-Identification/tree/Milestone2)\n",
    "\n",
    "The fine tuning code is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8606f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 100/759 [00:31<03:26,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0713, 'grad_norm': 6.13089656829834, 'learning_rate': 1.7364953886693017e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 200/759 [01:05<03:10,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7528, 'grad_norm': 6.864502906799316, 'learning_rate': 1.4729907773386036e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 300/759 [01:40<02:42,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6765, 'grad_norm': 7.461439609527588, 'learning_rate': 1.2094861660079052e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 400/759 [02:18<02:10,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5776, 'grad_norm': 6.685812473297119, 'learning_rate': 9.45981554677207e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 500/759 [02:55<01:40,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5771, 'grad_norm': 5.956602096557617, 'learning_rate': 6.824769433465086e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 600/759 [03:35<00:59,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5333, 'grad_norm': 6.608233451843262, 'learning_rate': 4.1897233201581036e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 700/759 [04:19<00:33,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.544, 'grad_norm': 6.947583198547363, 'learning_rate': 1.5546772068511201e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 759/759 [04:56<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 296.2855, 'train_samples_per_second': 10.237, 'train_steps_per_second': 2.562, 'train_loss': 2.6610089931563428, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We already have the combined dataset with the content and labels 0=English, 1=Japanese\n",
    "df = pd.read_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined.csv\")\n",
    "\n",
    "# add a prefix to the content based on the label (0=English, 1=Japanese)\n",
    "df['conditioned_text'] = df.apply(\n",
    "    lambda x: \"[JPN] \" + x['text'] if x['label'] == 1 else \"[ENG] \" + x['text'],\n",
    "    axis=1#axis=1 means apply the function to each row\n",
    ")\n",
    "\n",
    "# split the dataset\n",
    "train_df, _ = train_test_split(df, test_size=0.1)#test size is 10% of the dataset\n",
    "\n",
    "\n",
    "# loading the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>')# pad token is used to pad the input sequences to the same length\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')# GPT2LMHeadModel is the GPT-2 model with a language modeling head\n",
    "\n",
    "# using the conditioned text as input\n",
    "train_texts = train_df['conditioned_text'].tolist()#converts the column to a list\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "# truncation=True: truncate the input sequences to the maximum length the model can accept\n",
    "# padding=True: pad the input sequences to the same length\n",
    "# max_length=512: maximum length of the input sequences\n",
    "\n",
    "\n",
    "# self-defined dataset class to use the encodings as input\n",
    "class ConditionalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings.input_ids[idx],\n",
    "            \n",
    "            'labels': self.encodings.input_ids[idx]  # LLM will still use the input_ids as labels\n",
    "        }\n",
    "\n",
    "train_dataset = ConditionalDataset(train_encodings)\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-conditional\",\n",
    "    num_train_epochs=3,#number of epochs to train the model\n",
    "    per_device_train_batch_size=4,#batch size for training\n",
    "    logging_steps=100,#number of steps to print the logs\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# fine-tuning\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./gpt2-conditional\")#save the fine-tuned model\n",
    "\n",
    "def generate_samples(model,tokenizer, prefix, num_samples=2000, batch_size=10):#num_samples is the number of samples to generate, batch_size is the number of samples to generate in each batch\n",
    "    generated = []\n",
    "    model.eval()\n",
    "    # move the model to GPU\n",
    "    model=model.to('cuda')\n",
    "    while len(generated) < num_samples:\n",
    "        inputs = tokenizer.encode(prefix, return_tensors='pt').to('cuda')\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,#maximum length of the generated sequences\n",
    "            num_return_sequences=batch_size,\n",
    "            temperature=0.7,#temperature is used to control the randomness of the generated samples\n",
    "            top_p=0.9,#top_p is used to control the diversity of the generated samples\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        for output in outputs:\n",
    "            text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            clean_text = text.replace(prefix, \"\").strip()\n",
    "            if len(clean_text) > 10:  # filtering out the ones that are too short\n",
    "                generated.append(clean_text)\n",
    "                \n",
    "            if len(generated) >= num_samples:\n",
    "                break\n",
    "                \n",
    "    return generated[:num_samples]\n",
    "\n",
    "# Generate Japanese samples\n",
    "japanese_samples = generate_samples(model,tokenizer, \"[JPN]\", num_samples=2000)\n",
    "\n",
    "# Generate English samples\n",
    "english_samples = generate_samples(model,tokenizer, \"[ENG]\", num_samples=2000)\n",
    "\n",
    "# setting up DataFrame and save\n",
    "result_df = pd.DataFrame({\n",
    "    'text': japanese_samples + english_samples,\n",
    "    'label': [0]*2000 + [1]*2000\n",
    "})\n",
    "\n",
    "result_df.to_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\generated_samples.csv\", index=False)\n",
    "\n",
    "\n",
    "oringinal = pd.read_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined.csv\")\n",
    "combined_df= pd.concat([oringinal, result_df], ignore_index=True)\n",
    "combined_df.to_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined_new_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e5b81",
   "metadata": {},
   "source": [
    "As the data quantity is insufficient, I fine tuned GPT 2 for text generation-- first, I split 10%  in the original dataset as validation set and what is left (90%) of orginal data set as the training set.  Then, I tried to tell GPT2 which part of the input sequences should be attended to and which part of the input should be ignored by input_ids.  Then, I trained the model and used it for text generation-- original data set contains 1230 datasets, 615 Japanese Natives and 615 Enlgish Natives. I generated 4000, 2000 for each according to the trained data.  And I combined the generated dataset and the original dataset with their labels-- 0 for English Natives and 1 for Japanese. \n",
    "The following cell is how I combined the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd75d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  file00201.txt 2001-01-16 male 20 Japan 1m stud...      1\n",
      "1  file00202.txt 2001-01-17 female 19 Japan 1m st...      1\n",
      "2  file00203.txt 2001-02-02 male 27 Japan 3 neigh...      1\n",
      "3  file00204.txt 2001-02-02 female 31 Japan 1_12m...      1\n",
      "4  file00205.txt 2001-02-02 female Japan 5 restau...      1\n"
     ]
    }
   ],
   "source": [
    "oringinal = pd.read_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined.csv\")\n",
    "combined_df= pd.concat([oringinal, result_df], ignore_index=True)\n",
    "combined_df.to_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined_new_df.csv\", index=False)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "tensor([-1.8382e-01, -1.9284e-02,  2.4736e-01, -1.4338e-01, -1.3177e-01,\n",
      "        -5.4348e-02,  1.5631e-01,  3.3115e-01, -2.8438e-01, -3.9994e-01,\n",
      "         4.8445e-02, -6.1485e-02,  1.1324e-01,  1.7800e-01, -5.6604e-02,\n",
      "         1.5154e-01,  2.2107e-01,  3.2030e-01,  9.4237e-02,  1.3604e-01,\n",
      "         1.5238e-01, -2.5460e-01,  4.4725e-01,  1.7563e-02, -5.0431e-02,\n",
      "        -8.8167e-02, -7.7577e-02,  1.8742e-01,  3.1459e-02, -8.7420e-02,\n",
      "        -1.3092e-01,  2.4539e-01, -4.1070e-02, -1.9942e-01,  7.3461e-02,\n",
      "        -2.7940e-01,  6.7753e-02, -1.4762e-01,  1.1093e-01,  1.4603e-01,\n",
      "        -8.7253e-02, -3.2385e-02, -2.7490e-01, -3.2436e-01, -1.2498e-02,\n",
      "        -2.6295e-01, -3.2339e+00, -2.8927e-02,  1.5797e-02, -2.5329e-01,\n",
      "         4.7867e-01, -1.0998e-01, -9.2680e-02,  2.0057e-01,  2.7781e-01,\n",
      "         3.7869e-01, -2.3559e-01,  3.4312e-01, -1.5666e-01, -9.3389e-02,\n",
      "         6.3809e-01, -7.8705e-02, -4.3797e-01,  9.1733e-02, -3.3742e-03,\n",
      "        -1.9635e-01, -9.4752e-02,  1.1836e-01, -5.4933e-01,  5.8171e-01,\n",
      "        -3.4312e-01, -1.8989e-01,  2.3130e-01, -1.2505e-01,  7.3023e-02,\n",
      "        -2.4759e-01,  7.7549e-02,  2.1606e-01, -1.3367e-01,  3.3964e-01,\n",
      "         1.4023e-01,  2.2421e-01,  2.3681e-01,  4.1059e-02,  1.8335e-01,\n",
      "         2.1144e-01, -4.5684e-01,  4.8981e-02,  2.6069e-01,  2.9502e-01,\n",
      "         1.0447e-01, -9.1966e-02,  6.1261e-02,  3.8928e-02,  4.5097e-01,\n",
      "        -3.1472e-01, -1.4143e-01, -8.1417e-02, -4.2409e-02,  1.5598e-01,\n",
      "         4.2877e-01, -7.3348e-02,  3.4259e-01, -6.1208e-01,  2.1106e-01,\n",
      "        -2.2427e-01,  2.4047e-02, -3.7539e-01,  4.3739e-02, -1.8024e+00,\n",
      "         6.8962e-02,  3.7035e-01, -2.2033e-02, -5.6534e-01, -9.1482e-02,\n",
      "         4.0372e-02,  2.8563e-01,  3.4057e-03,  2.3437e-01,  2.9887e-01,\n",
      "        -1.0211e-01,  1.3166e-01,  9.9517e-04, -2.8696e-01, -2.0482e-01,\n",
      "         4.6745e-01,  3.0746e-02, -5.2954e-01,  1.5997e-01,  4.4434e-01,\n",
      "         4.2104e-01,  4.2758e-01, -8.1994e-02,  1.6592e-02, -1.5016e-01,\n",
      "         4.1272e-01,  2.1185e-01, -1.0515e-01, -1.7471e-01, -9.5176e-02,\n",
      "        -3.2763e-01, -1.3184e-01, -2.3587e+00,  4.1900e-01,  5.2690e-01,\n",
      "         2.2373e-02, -2.7546e-01,  2.6051e-01, -1.6253e-01,  2.5433e-01,\n",
      "         1.0440e-01, -1.6703e-01, -8.8562e-02,  3.2397e-02, -1.9221e-01,\n",
      "         3.6742e-01, -4.1576e-01, -1.4724e-01,  1.5343e-01,  2.5556e-01,\n",
      "         2.6651e-01, -5.1607e-02, -2.5155e-02,  2.1296e-01,  1.0179e-01,\n",
      "         1.9426e-01,  4.3244e-01, -1.0838e-01,  5.9186e-02, -1.7739e-02,\n",
      "        -1.5021e-01, -4.9513e-02,  6.7119e-01, -3.2936e-01,  1.1023e-01,\n",
      "         1.1345e-01,  1.9627e-01,  2.0044e-01,  2.3713e-01, -1.9186e-01,\n",
      "        -2.5608e-01,  3.8185e-01, -2.1413e-01,  1.1193e-01,  8.4965e-02,\n",
      "         4.8604e-02,  1.2459e-01, -1.5209e-01, -4.0922e-02,  3.4201e-01,\n",
      "        -1.1423e-01, -2.9908e-01,  9.4308e-02, -1.5257e-01,  3.9302e-01,\n",
      "         1.2261e-01,  1.5126e-01, -4.3252e-01, -5.8342e-02,  1.0577e-01,\n",
      "        -2.4024e-01,  9.6909e-02,  1.8378e-01,  9.4561e-02, -1.5598e-01,\n",
      "         3.3232e+00,  2.4416e-01, -3.9634e-02,  1.1962e-01,  5.5616e-01,\n",
      "        -3.8951e-01,  2.1682e-02,  1.0338e-01, -2.2975e-01, -1.7193e-02,\n",
      "         2.8878e-01,  4.4801e-01, -1.0613e-01,  5.4797e-02, -7.8943e-02,\n",
      "         2.5240e-01,  5.1088e-02, -1.9045e-01,  3.3799e-01, -4.4661e-02,\n",
      "         2.6040e-01, -1.4241e-01, -3.0142e-02,  2.3855e-01, -1.4754e+00,\n",
      "         1.4776e-01, -4.2817e-02,  1.6426e-01,  3.7154e-01, -1.7378e-01,\n",
      "        -1.8370e-01,  8.3209e-02, -1.8902e-01,  8.0302e-02, -1.1058e-01,\n",
      "        -1.0277e-01,  3.7630e-01,  2.9235e-01,  1.4883e-01, -2.9081e-01,\n",
      "         4.9593e-01,  1.9404e-01, -3.0815e-01, -1.8906e-02, -2.0451e-01,\n",
      "         4.0694e-01,  1.0175e-01, -1.0424e-01, -2.3332e-01,  2.0833e-01,\n",
      "        -7.5929e-02,  2.2544e-01, -7.7704e-02, -2.2622e-01, -1.4894e-01,\n",
      "        -4.0263e-01, -1.0963e-01,  1.7293e-01,  1.1875e-01, -3.9270e-01,\n",
      "        -8.3210e-02,  4.3184e-02, -4.1549e-01, -1.7351e-02, -9.2530e-02,\n",
      "         2.1737e-02, -1.0369e-01, -6.9507e-01, -2.2929e+00, -2.8161e-01,\n",
      "        -2.0736e-01,  3.5994e-01,  1.5574e-01,  8.2525e-02,  6.7782e-02,\n",
      "         1.7553e-01,  4.3458e-01, -4.4685e-01,  6.0662e-01,  6.3406e-02,\n",
      "        -3.6101e-02,  1.4607e-01, -2.2041e-01,  3.4099e-01, -2.5210e-01,\n",
      "        -1.3637e-01,  1.4708e-02, -3.2433e-01, -6.4448e-02,  2.3805e-01,\n",
      "        -1.5000e-01,  6.0844e-01, -1.2919e-02,  1.4083e-01, -1.2882e-01,\n",
      "        -2.2519e-01, -5.3567e-02,  2.5217e-01,  5.5849e-02, -7.0156e-02,\n",
      "         5.0265e-02, -5.1013e-01,  4.1618e-02, -3.5551e+00, -1.2958e-01,\n",
      "        -1.4183e-01, -1.3242e-01,  4.8104e-02,  1.9583e-01,  7.2729e-01,\n",
      "        -8.4939e-02, -1.5227e-01, -1.0921e-01, -2.0922e-02, -8.1625e-02,\n",
      "         2.4147e-01,  8.9801e-02,  1.0915e-01,  3.1656e-01,  5.8030e-01,\n",
      "        -2.6265e-01,  2.2476e-01,  3.4354e-01, -1.6671e-01, -6.3136e-02,\n",
      "         2.8421e-02,  7.5078e-02,  4.0047e-01,  3.4647e-01, -6.5847e-01,\n",
      "         4.4300e-02, -1.3824e-01, -1.4981e-01,  2.6271e-01, -3.6553e-01,\n",
      "        -1.2976e-01,  7.2872e-02, -3.9683e-01, -1.7486e-01,  1.3213e-01,\n",
      "         9.6601e-02,  5.2782e-01,  1.1771e-01, -4.6667e-03,  5.1623e-01,\n",
      "         1.0662e-01,  3.3942e-02,  6.3144e-01,  8.2099e-02,  1.8897e-01,\n",
      "        -4.6934e-02, -1.2188e-01,  1.1046e-01,  2.3427e-02,  2.3441e-01,\n",
      "         1.1159e+00,  9.0441e-04,  9.0354e-02, -2.5308e-01,  4.1187e-01,\n",
      "         3.6712e-01,  4.2780e-02,  5.0528e-03,  4.5518e-01, -4.0558e-01,\n",
      "         2.3982e-01, -6.8533e-01,  1.2993e-01, -4.0772e-01,  3.1962e-01,\n",
      "        -4.6719e-01,  2.2304e-02,  1.4747e-01, -2.2635e-01,  1.2580e-01,\n",
      "        -8.8040e-02, -8.9025e-01, -1.6609e-01, -6.3939e-02, -3.7805e-02,\n",
      "         9.4033e-02, -3.0718e-01,  4.0332e-01, -3.0448e-01, -1.2942e-01,\n",
      "        -9.8110e-02,  4.7597e-01, -3.8125e-01, -1.0178e-01, -1.0486e-01,\n",
      "         2.2760e-02, -6.4762e-01,  3.2467e-02,  6.7047e-02,  2.6104e-01,\n",
      "         9.7260e-02,  1.5922e-01,  1.8530e-01,  2.1983e-02,  2.9880e-01,\n",
      "        -7.8549e-01,  2.9269e-01, -4.7274e-01,  1.5199e-01, -6.4916e-02,\n",
      "        -2.1526e-01, -8.6993e-02, -1.3724e-01, -4.7623e-02, -1.0705e-01,\n",
      "         3.4857e-01, -1.6549e-01,  1.3919e-01, -2.2457e-01, -1.8174e-01,\n",
      "        -1.2507e-01,  1.5410e-01,  1.0080e+00, -3.0191e-01, -8.9574e-02,\n",
      "         1.5577e-01,  1.5130e-01,  1.7498e-01,  9.7406e-02, -3.9432e-02,\n",
      "        -1.5255e-01, -1.0684e-01, -9.0889e-02, -4.7415e-02, -8.1070e-02,\n",
      "        -1.2127e-01, -4.8339e-01,  1.6388e-01, -3.1690e-03,  1.9342e-01,\n",
      "        -4.1789e-01, -3.5645e-01, -6.6535e-02, -2.7438e-01, -1.7631e-02,\n",
      "         4.3030e-02,  5.0143e-01,  1.2421e-01,  5.8742e-01, -1.7641e-02,\n",
      "        -2.0320e-01,  3.6788e-01, -1.2605e-01,  6.2390e-01, -2.1307e-01,\n",
      "        -2.4641e-01, -1.5261e-01,  2.4691e-01, -4.3934e-02, -3.5773e-01,\n",
      "        -1.6423e-01, -2.3366e-01,  2.6058e-01,  2.1898e-01, -1.9309e-01,\n",
      "        -6.9390e-02, -2.7417e-01, -7.1428e-02,  3.0676e-01, -1.4144e-01,\n",
      "        -1.8362e+00,  3.2156e-01, -1.5274e-02,  9.9809e-02, -3.3359e-02,\n",
      "        -2.8893e-05, -1.6775e-03,  3.8305e-01,  2.2541e-01,  1.5453e-01,\n",
      "        -2.6907e-01, -2.2274e-01, -1.0447e-01,  2.9150e-02,  4.7936e-02,\n",
      "        -6.5578e-02, -3.0147e-01, -8.6046e-02, -1.5227e-01, -5.1820e-02,\n",
      "        -2.2847e-01,  4.3887e-01,  1.6730e-01,  2.6229e-01,  3.9919e-01,\n",
      "        -2.9820e-01, -1.0406e-01,  3.3864e-01,  1.4878e-01,  2.9878e-01,\n",
      "         9.1219e-02, -2.4060e-01, -2.7221e-01, -4.8728e-01,  3.0944e-01,\n",
      "         4.3545e-02, -7.3047e-02,  7.4097e-02,  4.0064e-01,  3.0619e-01,\n",
      "        -4.6610e-01,  1.9713e-01,  2.3226e-01,  4.8887e-02,  4.9213e-01,\n",
      "         2.2575e-01, -3.6758e-02, -1.0180e-01, -1.6268e-01, -5.0666e-02,\n",
      "        -4.9832e-03, -2.2978e-01, -2.1750e-01, -2.0572e-01, -1.8136e-01,\n",
      "        -8.3970e-02, -2.1756e-01,  2.0578e-01, -2.2453e-01, -2.2578e-02,\n",
      "         8.6302e-02, -4.9011e-01, -3.3631e-01,  8.7674e-02, -1.0847e-01,\n",
      "        -9.0826e-01,  6.1589e-03, -7.4672e-02, -1.6170e-01, -2.7732e-01,\n",
      "         4.0566e-01, -7.5973e-02, -3.8219e-01,  1.8852e-01,  5.4476e-04,\n",
      "         5.2610e-02,  1.2707e-02,  3.0857e-01, -9.6280e-02, -2.7437e-01,\n",
      "        -3.3722e-02, -5.9280e-01, -4.5061e-01,  7.2483e-01, -1.7711e-01,\n",
      "         1.5397e-01, -2.0751e-01,  8.5537e-02,  3.0902e-01,  1.0248e-01,\n",
      "        -1.8798e-01, -3.6459e-01, -2.3102e-01,  1.5494e-01, -9.6916e-02,\n",
      "        -5.9222e-02, -1.9632e-02,  2.1916e-02, -1.3024e-01, -2.5748e-01,\n",
      "        -7.8261e-02,  5.5881e-02,  2.6912e-01,  1.8417e-01, -4.5819e-02,\n",
      "         3.4308e-01,  2.5892e-01, -7.9106e-02, -9.8334e-02, -1.0186e-02,\n",
      "        -3.9494e-02, -5.6241e-02, -1.1754e-02,  2.4929e-02, -2.4181e-02,\n",
      "        -2.9697e-01, -2.7679e-01, -4.9819e-01,  1.6653e+00,  4.8711e-01,\n",
      "         1.1486e-01,  3.8793e-02,  2.4545e-01, -1.3527e-01, -1.5292e-01,\n",
      "         2.6975e-01, -2.4566e-01,  4.5307e-01, -2.4246e-01,  2.7491e-01,\n",
      "        -3.6328e-01,  2.1503e-01,  5.2436e-01,  3.8295e-01,  2.3685e-01,\n",
      "        -1.9395e-01, -5.1964e-01, -3.4718e-02, -1.6036e-01,  1.7051e-01,\n",
      "         3.3702e-01,  1.0267e-01,  9.6355e-02,  2.6305e-01,  1.7179e-01,\n",
      "        -5.9840e-02,  2.6475e-01,  1.6066e-01, -7.1203e-02,  4.2335e-02,\n",
      "         4.3523e-01,  4.3829e-01, -3.3588e-01, -3.5301e-01, -8.2339e-02,\n",
      "        -1.5488e-01, -2.6220e-01, -2.0677e-02,  2.8836e-01, -4.2638e-01,\n",
      "         5.0383e-01, -1.0355e-01, -1.4234e-01,  4.4443e-01, -2.0461e-01,\n",
      "        -3.5971e-01,  2.4151e-01,  4.2157e-01,  1.7323e-01,  1.8291e-01,\n",
      "        -3.8625e-01,  2.4646e-01, -5.4392e-02,  9.5676e-02, -2.9310e-01,\n",
      "        -5.4656e-01,  3.9727e-02,  2.2389e-01,  2.3937e-02,  7.1041e-01,\n",
      "         2.2899e-01, -1.0571e-01,  2.5505e-01,  2.0308e-01, -2.5936e-01,\n",
      "         7.6218e-03,  2.5104e-01, -3.1715e-01, -7.7847e-02,  5.5726e-01,\n",
      "         3.2738e-01,  2.5440e-01,  2.3065e-01,  2.8270e-01,  2.3845e-01,\n",
      "        -1.2454e-01, -2.6447e-01, -1.8159e+00,  2.8221e-01,  3.9899e-02,\n",
      "         1.9919e-01,  9.0014e-02,  3.7739e-01,  2.8834e-01, -1.7187e-01,\n",
      "        -9.6476e-03, -1.1949e-01,  2.2578e-01,  4.5270e-01,  4.5515e-01,\n",
      "        -1.8815e-01,  1.4868e-01, -1.7882e-01,  4.0846e-01, -3.0313e-01,\n",
      "        -2.8287e-01, -2.5275e-01,  1.1370e-01,  2.7379e-01,  6.4555e-02,\n",
      "        -4.1284e-01, -6.7881e-01,  3.2774e-01, -7.7657e-03, -4.9224e-01,\n",
      "         2.0662e-01, -7.7880e-02,  7.4304e-02,  3.2603e-01, -2.9144e-01,\n",
      "        -1.7243e-01,  4.3099e-02, -1.9595e-01, -2.9315e-01, -3.5133e-02,\n",
      "        -8.3917e-02,  3.0751e-01, -8.1508e-02,  7.5496e-01, -1.9716e-01,\n",
      "         3.1649e-01, -1.6734e-01,  7.0969e-02,  5.7437e-01, -2.7538e-01,\n",
      "         6.9577e-01, -7.5085e-02, -6.2008e-02,  6.5042e-02,  4.6742e-01,\n",
      "         8.3213e-02,  2.5457e-01,  4.2766e-03,  2.1896e-01, -1.4392e-01,\n",
      "         3.4920e-02, -3.6133e-01, -2.1240e-01, -8.1898e-02, -2.0596e-01,\n",
      "        -7.2248e-02,  1.2811e-01, -1.6549e-01, -3.8976e-01, -7.3805e-02,\n",
      "        -2.4113e-02, -3.1683e-01, -1.9170e-01, -1.8354e-01,  3.5096e-01,\n",
      "         2.2762e-01,  1.6852e-01, -2.0306e-01,  6.5892e-01,  4.1229e-01,\n",
      "         2.5963e-01,  1.3394e-01,  3.8008e-02, -4.5124e-02, -1.6525e-01,\n",
      "        -1.5119e-01,  1.8651e-01, -5.3885e+00, -2.4859e-01, -8.0945e-02,\n",
      "        -2.3165e-01, -3.5898e-01, -4.6186e-01,  1.9510e-01, -5.3509e-01,\n",
      "         4.9915e-02, -2.2266e-01, -1.5394e-02,  4.4939e-02, -2.5610e-01,\n",
      "        -9.9590e-02,  4.6242e-01,  3.4300e-01], device='cuda:0') tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import BertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import umap.umap_ as umap \n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "#loading the pre-trained model tokenizer of BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())\n",
    "# load the pre-trained BERT model to GPU\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model= DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "# do not feed the data directly to the variable called\"model\"! in the training process!\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataset = pd.read_csv(dataframe)\n",
    "        self.texts = self.dataset[['text']].astype(str)\n",
    "        # Convert labels to long\n",
    "        self.labels = self.dataset['label']#map({\"Native English\": 0, \"Native Japanese\": 1}).astype(int) this is no longer needed because I already labeled the df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idex):\n",
    "        text = self.texts.iloc[idex].values[0]\n",
    "        label = torch.tensor(self.labels.iloc[idex], dtype=torch.long).to(device)  # Make sure it's long\n",
    "        tokenized_text = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_text = {key: val.to(device) for key, val in tokenized_text.items() if key != 'token_type_ids'}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**tokenized_text)\n",
    "        \n",
    "        embedding = output.last_hidden_state[:, 0, :].squeeze(0)\n",
    "        embedding = embedding.float()  # Ensure embedding is float\n",
    "        \n",
    "        return embedding, label\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "dataset = CustomDataset(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined_new_df.csv\")\n",
    "embedding, label = dataset[0]\n",
    "print(embedding, label)\n",
    "# Create a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "#batch size too big, will cause memory error on GPU.\n",
    "#we need to reduce the batch size to 4 or 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28377c37",
   "metadata": {},
   "source": [
    "In this cell, I used BERT-base-uncased and distill-bert-uncased for tokenization and embedding transfer. the transfered embedding, with their labels have been store in a dataframe. Then, I created a Dataset class to store the dataset, tokenization and embedding vectorization. Then, I split the dataset into train_loader and test_loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "027d23b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# UMAP demonstration reduction to 3 demensions\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap  # make sure I am using umap-learn\n",
    "import numpy as np\n",
    "# Extract embeddings and labels\n",
    "def get_embeddings_and_labels(loader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for batch_embeddings, batch_labels in loader:\n",
    "        embeddings.append(batch_embeddings.cpu().numpy())  # transfer the embeddings to the CPU and to numpy\n",
    "        labels.append(batch_labels.cpu().numpy())\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.hstack(labels)\n",
    "    return embeddings, labels\n",
    "\n",
    "# Extract embeddings and labels from the train and test loaders\n",
    "train_embeddings, train_labels = get_embeddings_and_labels(train_loader)\n",
    "test_embeddings, test_labels = get_embeddings_and_labels(test_loader)\n",
    "\n",
    "# reduce the embeddings to 20 dimensions\n",
    "umap_model = umap.UMAP(n_components=30, random_state=42)\n",
    "\n",
    "train_embeddings_3d = umap_model.fit_transform(train_embeddings)\n",
    "test_embeddings_3d = umap_model.transform(test_embeddings)  # use the same model to transform the test embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ceea5",
   "metadata": {},
   "source": [
    "## Demension Reduction!\n",
    "As the 768 demension would highly increase the time for computation, I reduced the demension of embeddings into 30 for testing-- so that there would be a bit more information stored and highly increase the speed of computation. Then, I still stored the new training set and testing set into new dataloaders. I firstly transferred them into 3d, but I soon found out that there are too little information being stored and not meaningful for training with that dataset.\n",
    "the class of 30 demension is in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f80c363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a new class of data set that stores the embeddings and labels \n",
    "from torch.utils.data import DataLoader\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx])\n",
    "\n",
    "# setting up the data loaders for training and testing\n",
    "train_embedding_dataset = EmbeddingDataset(train_embeddings_3d, train_labels)\n",
    "test_embedding_dataset = EmbeddingDataset(test_embeddings_3d, test_labels)\n",
    "\n",
    "train_loader_20d = DataLoader(train_embedding_dataset, batch_size=32, shuffle=True)\n",
    "test_loader_20d = DataLoader(test_embedding_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "238e4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the model\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self,hidden_size=185\n",
    "                 ,dropout_rate=0.3267140666269757\n",
    "                 ,learning_rate=0.00025937129423859296\n",
    "                 ):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(768\n",
    "                      , hidden_size),#input layer has 768 neurons which is the size of the BERT embedding (if dealing with 30 d embeddings, the input layer has 30 neurons)\n",
    "            nn.BatchNorm1d(hidden_size),# adding batch normalization\n",
    "            nn.Dropout(dropout_rate),# setting up dropout rate to 0.1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "# I used linear activation functions for the hidden layers and softmax for the output layer\n",
    "# the input layer has 768 neurons which is the size of the BERT embedding\n",
    "# the hidden layer has 100 neurons\n",
    "# the output layer has 2 neurons\n",
    "# I used the ReLU activation function for the hidden layer and the softmax activation function for the output because it is a binary classification problem and non-linear\n",
    "# activation functions are needed for the hidden layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten input if necessary\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x  # Output logits (CrossEntropyLoss will apply softmax internally)\n",
    "    \n",
    "    def fit(self, train_loader,val_loader, epochs,learning_rate=0.01,patience=5):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)# here I used the Adam optimizer because it is better for NLP\n",
    "        best_val_loss= float(\"inf\")#initialize the best validation loss to infinity\n",
    "        patience_counter=0#initialize the patience counter to 0\n",
    "        for epoch in range(epochs):\n",
    "            for i, (data, labels) in enumerate(train_loader):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(data)\n",
    "                loss = criterion(outputs, labels)#calculate the loss\n",
    "                loss.backward()#backpropagation\n",
    "                optimizer.step()\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "            val_loss=self.evaluate(val_loader, criterion)#evaluate the model on the validation set\n",
    "            if val_loss<best_val_loss:\n",
    "                best_val_loss=val_loss#update the best validation loss\n",
    "\n",
    "                patience_counter=0\n",
    "            else:\n",
    "                patience_counter+=1\n",
    "            if patience_counter>patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        return best_val_loss#return the best validation loss\n",
    "    #in this funciton, we do not need to convert the data into tensors again bc we arleady did that\n",
    "    def predict(self, x):\n",
    "      with torch.no_grad():\n",
    "        x = torch.tensor(x).to(device)\n",
    "        outputs = self(x)\n",
    "        y_pred = torch.argmax(outputs, dim=1)\n",
    "        return np.where(y_pred.detach().cpu().numpy()==1, \"Native Japanese\", \"Native English\")\n",
    "      \n",
    "      \n",
    "    def evaluate(self, val_loader,criterion):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = self(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(val_loader)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9246571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.964580  [    0/ 5124]\n",
      "loss: 0.566610  [  400/ 5124]\n",
      "loss: 0.894764  [  800/ 5124]\n",
      "loss: 0.741076  [ 1200/ 5124]\n",
      "loss: 0.510969  [ 1600/ 5124]\n",
      "loss: 0.402552  [ 2000/ 5124]\n",
      "loss: 0.684575  [ 2400/ 5124]\n",
      "loss: 0.250913  [ 2800/ 5124]\n",
      "loss: 0.698820  [ 3200/ 5124]\n",
      "loss: 0.480867  [ 3600/ 5124]\n",
      "loss: 0.509612  [ 4000/ 5124]\n",
      "loss: 0.483319  [ 4400/ 5124]\n",
      "loss: 0.742746  [ 4800/ 5124]\n",
      "Average Training Loss: 0.5555\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.456661 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.509154  [    0/ 5124]\n",
      "loss: 0.479380  [  400/ 5124]\n",
      "loss: 0.626528  [  800/ 5124]\n",
      "loss: 0.397814  [ 1200/ 5124]\n",
      "loss: 0.283879  [ 1600/ 5124]\n",
      "loss: 0.442565  [ 2000/ 5124]\n",
      "loss: 0.603289  [ 2400/ 5124]\n",
      "loss: 0.541206  [ 2800/ 5124]\n",
      "loss: 0.175751  [ 3200/ 5124]\n",
      "loss: 0.303028  [ 3600/ 5124]\n",
      "loss: 0.488987  [ 4000/ 5124]\n",
      "loss: 0.562255  [ 4400/ 5124]\n",
      "loss: 0.382352  [ 4800/ 5124]\n",
      "Average Training Loss: 0.5030\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.431010 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.419009  [    0/ 5124]\n",
      "loss: 0.418206  [  400/ 5124]\n",
      "loss: 0.356795  [  800/ 5124]\n",
      "loss: 0.589700  [ 1200/ 5124]\n",
      "loss: 0.734511  [ 1600/ 5124]\n",
      "loss: 0.643718  [ 2000/ 5124]\n",
      "loss: 0.166568  [ 2400/ 5124]\n",
      "loss: 0.297320  [ 2800/ 5124]\n",
      "loss: 0.486022  [ 3200/ 5124]\n",
      "loss: 0.830818  [ 3600/ 5124]\n",
      "loss: 0.290403  [ 4000/ 5124]\n",
      "loss: 0.466983  [ 4400/ 5124]\n",
      "loss: 0.311620  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4866\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.413182 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.539483  [    0/ 5124]\n",
      "loss: 1.144290  [  400/ 5124]\n",
      "loss: 0.718163  [  800/ 5124]\n",
      "loss: 0.488222  [ 1200/ 5124]\n",
      "loss: 0.569749  [ 1600/ 5124]\n",
      "loss: 0.579155  [ 2000/ 5124]\n",
      "loss: 0.387336  [ 2400/ 5124]\n",
      "loss: 0.402919  [ 2800/ 5124]\n",
      "loss: 0.527018  [ 3200/ 5124]\n",
      "loss: 0.856101  [ 3600/ 5124]\n",
      "loss: 0.725255  [ 4000/ 5124]\n",
      "loss: 0.419660  [ 4400/ 5124]\n",
      "loss: 0.441713  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4895\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.409890 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.714072  [    0/ 5124]\n",
      "loss: 0.200610  [  400/ 5124]\n",
      "loss: 0.232692  [  800/ 5124]\n",
      "loss: 0.279584  [ 1200/ 5124]\n",
      "loss: 0.407229  [ 1600/ 5124]\n",
      "loss: 0.315775  [ 2000/ 5124]\n",
      "loss: 0.185338  [ 2400/ 5124]\n",
      "loss: 0.463466  [ 2800/ 5124]\n",
      "loss: 0.174608  [ 3200/ 5124]\n",
      "loss: 0.407003  [ 3600/ 5124]\n",
      "loss: 0.526659  [ 4000/ 5124]\n",
      "loss: 0.206491  [ 4400/ 5124]\n",
      "loss: 0.326097  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4789\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.394956 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.177265  [    0/ 5124]\n",
      "loss: 0.601011  [  400/ 5124]\n",
      "loss: 0.614746  [  800/ 5124]\n",
      "loss: 0.179650  [ 1200/ 5124]\n",
      "loss: 0.620152  [ 1600/ 5124]\n",
      "loss: 0.516901  [ 2000/ 5124]\n",
      "loss: 0.548317  [ 2400/ 5124]\n",
      "loss: 0.426329  [ 2800/ 5124]\n",
      "loss: 0.228623  [ 3200/ 5124]\n",
      "loss: 0.191290  [ 3600/ 5124]\n",
      "loss: 0.364309  [ 4000/ 5124]\n",
      "loss: 0.342907  [ 4400/ 5124]\n",
      "loss: 0.547998  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4826\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.401017 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.166820  [    0/ 5124]\n",
      "loss: 1.260742  [  400/ 5124]\n",
      "loss: 0.525818  [  800/ 5124]\n",
      "loss: 0.586042  [ 1200/ 5124]\n",
      "loss: 0.514209  [ 1600/ 5124]\n",
      "loss: 0.211115  [ 2000/ 5124]\n",
      "loss: 0.197364  [ 2400/ 5124]\n",
      "loss: 0.680825  [ 2800/ 5124]\n",
      "loss: 0.669799  [ 3200/ 5124]\n",
      "loss: 0.650295  [ 3600/ 5124]\n",
      "loss: 0.464936  [ 4000/ 5124]\n",
      "loss: 0.428140  [ 4400/ 5124]\n",
      "loss: 0.650905  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4730\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.396656 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.425650  [    0/ 5124]\n",
      "loss: 0.254217  [  400/ 5124]\n",
      "loss: 0.207364  [  800/ 5124]\n",
      "loss: 0.498498  [ 1200/ 5124]\n",
      "loss: 0.428254  [ 1600/ 5124]\n",
      "loss: 0.388968  [ 2000/ 5124]\n",
      "loss: 0.197721  [ 2400/ 5124]\n",
      "loss: 1.002814  [ 2800/ 5124]\n",
      "loss: 0.422668  [ 3200/ 5124]\n",
      "loss: 0.193850  [ 3600/ 5124]\n",
      "loss: 0.544201  [ 4000/ 5124]\n",
      "loss: 0.432500  [ 4400/ 5124]\n",
      "loss: 0.383671  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4751\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.392759 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.644381  [    0/ 5124]\n",
      "loss: 0.759023  [  400/ 5124]\n",
      "loss: 0.462409  [  800/ 5124]\n",
      "loss: 0.201476  [ 1200/ 5124]\n",
      "loss: 0.622814  [ 1600/ 5124]\n",
      "loss: 0.186757  [ 2000/ 5124]\n",
      "loss: 0.239833  [ 2400/ 5124]\n",
      "loss: 0.648627  [ 2800/ 5124]\n",
      "loss: 0.325854  [ 3200/ 5124]\n",
      "loss: 0.570607  [ 3600/ 5124]\n",
      "loss: 0.398134  [ 4000/ 5124]\n",
      "loss: 0.553308  [ 4400/ 5124]\n",
      "loss: 0.427013  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4750\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.387519 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.140503  [    0/ 5124]\n",
      "loss: 0.383910  [  400/ 5124]\n",
      "loss: 1.094123  [  800/ 5124]\n",
      "loss: 0.538994  [ 1200/ 5124]\n",
      "loss: 0.399983  [ 1600/ 5124]\n",
      "loss: 1.103492  [ 2000/ 5124]\n",
      "loss: 0.312000  [ 2400/ 5124]\n",
      "loss: 0.605564  [ 2800/ 5124]\n",
      "loss: 0.122769  [ 3200/ 5124]\n",
      "loss: 0.402819  [ 3600/ 5124]\n",
      "loss: 0.660163  [ 4000/ 5124]\n",
      "loss: 0.199081  [ 4400/ 5124]\n",
      "loss: 0.420887  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4671\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.409975 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.464437  [    0/ 5124]\n",
      "loss: 0.202970  [  400/ 5124]\n",
      "loss: 0.937896  [  800/ 5124]\n",
      "loss: 0.536795  [ 1200/ 5124]\n",
      "loss: 0.239167  [ 1600/ 5124]\n",
      "loss: 0.208950  [ 2000/ 5124]\n",
      "loss: 0.720196  [ 2400/ 5124]\n",
      "loss: 0.789542  [ 2800/ 5124]\n",
      "loss: 0.345687  [ 3200/ 5124]\n",
      "loss: 1.177467  [ 3600/ 5124]\n",
      "loss: 0.258705  [ 4000/ 5124]\n",
      "loss: 0.221529  [ 4400/ 5124]\n",
      "loss: 0.257192  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4578\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.405119 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.157470  [    0/ 5124]\n",
      "loss: 0.219364  [  400/ 5124]\n",
      "loss: 0.555849  [  800/ 5124]\n",
      "loss: 0.209340  [ 1200/ 5124]\n",
      "loss: 0.153345  [ 1600/ 5124]\n",
      "loss: 0.143208  [ 2000/ 5124]\n",
      "loss: 0.377593  [ 2400/ 5124]\n",
      "loss: 0.395297  [ 2800/ 5124]\n",
      "loss: 0.636319  [ 3200/ 5124]\n",
      "loss: 0.636417  [ 3600/ 5124]\n",
      "loss: 0.570512  [ 4000/ 5124]\n",
      "loss: 0.435777  [ 4400/ 5124]\n",
      "loss: 0.759445  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4739\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.370154 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.156056  [    0/ 5124]\n",
      "loss: 0.240136  [  400/ 5124]\n",
      "loss: 0.342700  [  800/ 5124]\n",
      "loss: 0.576075  [ 1200/ 5124]\n",
      "loss: 0.410532  [ 1600/ 5124]\n",
      "loss: 0.405235  [ 2000/ 5124]\n",
      "loss: 0.870290  [ 2400/ 5124]\n",
      "loss: 0.397158  [ 2800/ 5124]\n",
      "loss: 1.078806  [ 3200/ 5124]\n",
      "loss: 0.116293  [ 3600/ 5124]\n",
      "loss: 0.387728  [ 4000/ 5124]\n",
      "loss: 0.497171  [ 4400/ 5124]\n",
      "loss: 0.465500  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4669\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.377241 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.201291  [    0/ 5124]\n",
      "loss: 0.162298  [  400/ 5124]\n",
      "loss: 0.493814  [  800/ 5124]\n",
      "loss: 0.412660  [ 1200/ 5124]\n",
      "loss: 0.553948  [ 1600/ 5124]\n",
      "loss: 0.300037  [ 2000/ 5124]\n",
      "loss: 1.091494  [ 2400/ 5124]\n",
      "loss: 0.436663  [ 2800/ 5124]\n",
      "loss: 0.587625  [ 3200/ 5124]\n",
      "loss: 0.368957  [ 3600/ 5124]\n",
      "loss: 0.489290  [ 4000/ 5124]\n",
      "loss: 0.852221  [ 4400/ 5124]\n",
      "loss: 0.398524  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4619\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.383932 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.402521  [    0/ 5124]\n",
      "loss: 0.419230  [  400/ 5124]\n",
      "loss: 0.624341  [  800/ 5124]\n",
      "loss: 0.651516  [ 1200/ 5124]\n",
      "loss: 0.953254  [ 1600/ 5124]\n",
      "loss: 0.895745  [ 2000/ 5124]\n",
      "loss: 0.334823  [ 2400/ 5124]\n",
      "loss: 0.341745  [ 2800/ 5124]\n",
      "loss: 0.123181  [ 3200/ 5124]\n",
      "loss: 0.243697  [ 3600/ 5124]\n",
      "loss: 0.526347  [ 4000/ 5124]\n",
      "loss: 0.549237  [ 4400/ 5124]\n",
      "loss: 0.141106  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4565\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.384186 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.200375  [    0/ 5124]\n",
      "loss: 0.150408  [  400/ 5124]\n",
      "loss: 0.904799  [  800/ 5124]\n",
      "loss: 0.182673  [ 1200/ 5124]\n",
      "loss: 0.422871  [ 1600/ 5124]\n",
      "loss: 0.135011  [ 2000/ 5124]\n",
      "loss: 0.174653  [ 2400/ 5124]\n",
      "loss: 0.136717  [ 2800/ 5124]\n",
      "loss: 0.652730  [ 3200/ 5124]\n",
      "loss: 0.472359  [ 3600/ 5124]\n",
      "loss: 1.047663  [ 4000/ 5124]\n",
      "loss: 0.255432  [ 4400/ 5124]\n",
      "loss: 0.548194  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4489\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.370540 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.373304  [    0/ 5124]\n",
      "loss: 0.414019  [  400/ 5124]\n",
      "loss: 0.230602  [  800/ 5124]\n",
      "loss: 0.642448  [ 1200/ 5124]\n",
      "loss: 0.281261  [ 1600/ 5124]\n",
      "loss: 0.142443  [ 2000/ 5124]\n",
      "loss: 0.333388  [ 2400/ 5124]\n",
      "loss: 0.289652  [ 2800/ 5124]\n",
      "loss: 1.427580  [ 3200/ 5124]\n",
      "loss: 0.197568  [ 3600/ 5124]\n",
      "loss: 0.350238  [ 4000/ 5124]\n",
      "loss: 0.371354  [ 4400/ 5124]\n",
      "loss: 0.266418  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4517\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.383534 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.190013  [    0/ 5124]\n",
      "loss: 0.137407  [  400/ 5124]\n",
      "loss: 0.359530  [  800/ 5124]\n",
      "loss: 0.156581  [ 1200/ 5124]\n",
      "loss: 0.576219  [ 1600/ 5124]\n",
      "loss: 0.140221  [ 2000/ 5124]\n",
      "loss: 0.548568  [ 2400/ 5124]\n",
      "loss: 0.678094  [ 2800/ 5124]\n",
      "loss: 0.401499  [ 3200/ 5124]\n",
      "loss: 0.271796  [ 3600/ 5124]\n",
      "loss: 0.275334  [ 4000/ 5124]\n",
      "loss: 0.410367  [ 4400/ 5124]\n",
      "loss: 0.470829  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4598\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.380574 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.444366  [    0/ 5124]\n",
      "loss: 0.350046  [  400/ 5124]\n",
      "loss: 0.597020  [  800/ 5124]\n",
      "loss: 0.222073  [ 1200/ 5124]\n",
      "loss: 0.532246  [ 1600/ 5124]\n",
      "loss: 0.274556  [ 2000/ 5124]\n",
      "loss: 0.333490  [ 2400/ 5124]\n",
      "loss: 0.341129  [ 2800/ 5124]\n",
      "loss: 0.187968  [ 3200/ 5124]\n",
      "loss: 0.325439  [ 3600/ 5124]\n",
      "loss: 0.184338  [ 4000/ 5124]\n",
      "loss: 0.513304  [ 4400/ 5124]\n",
      "loss: 0.365433  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4649\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.370636 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.650189  [    0/ 5124]\n",
      "loss: 0.637446  [  400/ 5124]\n",
      "loss: 0.157678  [  800/ 5124]\n",
      "loss: 0.346939  [ 1200/ 5124]\n",
      "loss: 0.207064  [ 1600/ 5124]\n",
      "loss: 1.130455  [ 2000/ 5124]\n",
      "loss: 0.150426  [ 2400/ 5124]\n",
      "loss: 0.250125  [ 2800/ 5124]\n",
      "loss: 0.706429  [ 3200/ 5124]\n",
      "loss: 0.404311  [ 3600/ 5124]\n",
      "loss: 0.247973  [ 4000/ 5124]\n",
      "loss: 0.412097  [ 4400/ 5124]\n",
      "loss: 0.444298  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4617\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.383484 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.030410  [    0/ 5124]\n",
      "loss: 0.481160  [  400/ 5124]\n",
      "loss: 0.264780  [  800/ 5124]\n",
      "loss: 0.275205  [ 1200/ 5124]\n",
      "loss: 0.166942  [ 1600/ 5124]\n",
      "loss: 0.397166  [ 2000/ 5124]\n",
      "loss: 0.991826  [ 2400/ 5124]\n",
      "loss: 0.446627  [ 2800/ 5124]\n",
      "loss: 0.543969  [ 3200/ 5124]\n",
      "loss: 0.541909  [ 3600/ 5124]\n",
      "loss: 0.638162  [ 4000/ 5124]\n",
      "loss: 0.608485  [ 4400/ 5124]\n",
      "loss: 0.728835  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4591\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.368019 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.407312  [    0/ 5124]\n",
      "loss: 0.638197  [  400/ 5124]\n",
      "loss: 0.406688  [  800/ 5124]\n",
      "loss: 0.190944  [ 1200/ 5124]\n",
      "loss: 0.303620  [ 1600/ 5124]\n",
      "loss: 0.729570  [ 2000/ 5124]\n",
      "loss: 0.556004  [ 2400/ 5124]\n",
      "loss: 0.186622  [ 2800/ 5124]\n",
      "loss: 0.501906  [ 3200/ 5124]\n",
      "loss: 0.185032  [ 3600/ 5124]\n",
      "loss: 0.186711  [ 4000/ 5124]\n",
      "loss: 0.278813  [ 4400/ 5124]\n",
      "loss: 0.423404  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4458\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.405318 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.518054  [    0/ 5124]\n",
      "loss: 0.403470  [  400/ 5124]\n",
      "loss: 0.343345  [  800/ 5124]\n",
      "loss: 0.472867  [ 1200/ 5124]\n",
      "loss: 0.147572  [ 1600/ 5124]\n",
      "loss: 0.974871  [ 2000/ 5124]\n",
      "loss: 0.456680  [ 2400/ 5124]\n",
      "loss: 0.225358  [ 2800/ 5124]\n",
      "loss: 0.167558  [ 3200/ 5124]\n",
      "loss: 0.224519  [ 3600/ 5124]\n",
      "loss: 0.683650  [ 4000/ 5124]\n",
      "loss: 0.297844  [ 4400/ 5124]\n",
      "loss: 0.365471  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4599\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.359630 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.163537  [    0/ 5124]\n",
      "loss: 0.197625  [  400/ 5124]\n",
      "loss: 0.504867  [  800/ 5124]\n",
      "loss: 0.355010  [ 1200/ 5124]\n",
      "loss: 0.227878  [ 1600/ 5124]\n",
      "loss: 0.447789  [ 2000/ 5124]\n",
      "loss: 0.592663  [ 2400/ 5124]\n",
      "loss: 0.229021  [ 2800/ 5124]\n",
      "loss: 0.538465  [ 3200/ 5124]\n",
      "loss: 0.484858  [ 3600/ 5124]\n",
      "loss: 0.229129  [ 4000/ 5124]\n",
      "loss: 1.316028  [ 4400/ 5124]\n",
      "loss: 0.547894  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4437\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.376399 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.203181  [    0/ 5124]\n",
      "loss: 0.144675  [  400/ 5124]\n",
      "loss: 0.162114  [  800/ 5124]\n",
      "loss: 0.436582  [ 1200/ 5124]\n",
      "loss: 0.420500  [ 1600/ 5124]\n",
      "loss: 0.297482  [ 2000/ 5124]\n",
      "loss: 0.352380  [ 2400/ 5124]\n",
      "loss: 0.423732  [ 2800/ 5124]\n",
      "loss: 0.283580  [ 3200/ 5124]\n",
      "loss: 0.163746  [ 3600/ 5124]\n",
      "loss: 0.544850  [ 4000/ 5124]\n",
      "loss: 0.692473  [ 4400/ 5124]\n",
      "loss: 0.371081  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4395\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.373785 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.601713  [    0/ 5124]\n",
      "loss: 0.529460  [  400/ 5124]\n",
      "loss: 0.458544  [  800/ 5124]\n",
      "loss: 0.248145  [ 1200/ 5124]\n",
      "loss: 0.319057  [ 1600/ 5124]\n",
      "loss: 0.458501  [ 2000/ 5124]\n",
      "loss: 0.439951  [ 2400/ 5124]\n",
      "loss: 0.168051  [ 2800/ 5124]\n",
      "loss: 0.166626  [ 3200/ 5124]\n",
      "loss: 0.510084  [ 3600/ 5124]\n",
      "loss: 0.127705  [ 4000/ 5124]\n",
      "loss: 0.336489  [ 4400/ 5124]\n",
      "loss: 0.109357  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4433\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.380299 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.425181  [    0/ 5124]\n",
      "loss: 0.345782  [  400/ 5124]\n",
      "loss: 0.597393  [  800/ 5124]\n",
      "loss: 0.407027  [ 1200/ 5124]\n",
      "loss: 0.226047  [ 1600/ 5124]\n",
      "loss: 1.021227  [ 2000/ 5124]\n",
      "loss: 0.796028  [ 2400/ 5124]\n",
      "loss: 0.215007  [ 2800/ 5124]\n",
      "loss: 0.345600  [ 3200/ 5124]\n",
      "loss: 0.664434  [ 3600/ 5124]\n",
      "loss: 0.433952  [ 4000/ 5124]\n",
      "loss: 0.132907  [ 4400/ 5124]\n",
      "loss: 0.751834  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4533\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.359692 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.142888  [    0/ 5124]\n",
      "loss: 1.507978  [  400/ 5124]\n",
      "loss: 0.889518  [  800/ 5124]\n",
      "loss: 0.393613  [ 1200/ 5124]\n",
      "loss: 0.133079  [ 1600/ 5124]\n",
      "loss: 0.221690  [ 2000/ 5124]\n",
      "loss: 0.180022  [ 2400/ 5124]\n",
      "loss: 0.416712  [ 2800/ 5124]\n",
      "loss: 0.156952  [ 3200/ 5124]\n",
      "loss: 0.324206  [ 3600/ 5124]\n",
      "loss: 0.353296  [ 4000/ 5124]\n",
      "loss: 0.148303  [ 4400/ 5124]\n",
      "loss: 0.134025  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4429\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.374725 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.407048  [    0/ 5124]\n",
      "loss: 0.277328  [  400/ 5124]\n",
      "loss: 0.103349  [  800/ 5124]\n",
      "loss: 0.644624  [ 1200/ 5124]\n",
      "loss: 0.153120  [ 1600/ 5124]\n",
      "loss: 0.432348  [ 2000/ 5124]\n",
      "loss: 0.591716  [ 2400/ 5124]\n",
      "loss: 0.555612  [ 2800/ 5124]\n",
      "loss: 0.095751  [ 3200/ 5124]\n",
      "loss: 0.396844  [ 3600/ 5124]\n",
      "loss: 0.321725  [ 4000/ 5124]\n",
      "loss: 0.654933  [ 4400/ 5124]\n",
      "loss: 0.236388  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4339\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.379958 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.397938  [    0/ 5124]\n",
      "loss: 0.425958  [  400/ 5124]\n",
      "loss: 0.440705  [  800/ 5124]\n",
      "loss: 0.166300  [ 1200/ 5124]\n",
      "loss: 0.651002  [ 1600/ 5124]\n",
      "loss: 0.594309  [ 2000/ 5124]\n",
      "loss: 0.419491  [ 2400/ 5124]\n",
      "loss: 0.323014  [ 2800/ 5124]\n",
      "loss: 0.135929  [ 3200/ 5124]\n",
      "loss: 0.129557  [ 3600/ 5124]\n",
      "loss: 1.359802  [ 4000/ 5124]\n",
      "loss: 0.386218  [ 4400/ 5124]\n",
      "loss: 0.428138  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4421\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.358680 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.561892  [    0/ 5124]\n",
      "loss: 0.181893  [  400/ 5124]\n",
      "loss: 0.347526  [  800/ 5124]\n",
      "loss: 0.507679  [ 1200/ 5124]\n",
      "loss: 0.665654  [ 1600/ 5124]\n",
      "loss: 0.505365  [ 2000/ 5124]\n",
      "loss: 0.583997  [ 2400/ 5124]\n",
      "loss: 0.535329  [ 2800/ 5124]\n",
      "loss: 0.343353  [ 3200/ 5124]\n",
      "loss: 0.448147  [ 3600/ 5124]\n",
      "loss: 0.350091  [ 4000/ 5124]\n",
      "loss: 0.343665  [ 4400/ 5124]\n",
      "loss: 0.297049  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4371\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.353910 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.543426  [    0/ 5124]\n",
      "loss: 0.467928  [  400/ 5124]\n",
      "loss: 0.469462  [  800/ 5124]\n",
      "loss: 0.654845  [ 1200/ 5124]\n",
      "loss: 0.452498  [ 1600/ 5124]\n",
      "loss: 0.617624  [ 2000/ 5124]\n",
      "loss: 0.496392  [ 2400/ 5124]\n",
      "loss: 1.073250  [ 2800/ 5124]\n",
      "loss: 0.479977  [ 3200/ 5124]\n",
      "loss: 0.298817  [ 3600/ 5124]\n",
      "loss: 0.461962  [ 4000/ 5124]\n",
      "loss: 0.188263  [ 4400/ 5124]\n",
      "loss: 0.205982  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4454\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.359363 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.229391  [    0/ 5124]\n",
      "loss: 0.172010  [  400/ 5124]\n",
      "loss: 0.199975  [  800/ 5124]\n",
      "loss: 0.376920  [ 1200/ 5124]\n",
      "loss: 0.469860  [ 1600/ 5124]\n",
      "loss: 0.141006  [ 2000/ 5124]\n",
      "loss: 0.492023  [ 2400/ 5124]\n",
      "loss: 0.340703  [ 2800/ 5124]\n",
      "loss: 0.636440  [ 3200/ 5124]\n",
      "loss: 0.382634  [ 3600/ 5124]\n",
      "loss: 0.207010  [ 4000/ 5124]\n",
      "loss: 0.981427  [ 4400/ 5124]\n",
      "loss: 0.165360  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4464\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.364140 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.754546  [    0/ 5124]\n",
      "loss: 0.671066  [  400/ 5124]\n",
      "loss: 0.290340  [  800/ 5124]\n",
      "loss: 0.236495  [ 1200/ 5124]\n",
      "loss: 0.319578  [ 1600/ 5124]\n",
      "loss: 1.320667  [ 2000/ 5124]\n",
      "loss: 0.122666  [ 2400/ 5124]\n",
      "loss: 0.178298  [ 2800/ 5124]\n",
      "loss: 1.393070  [ 3200/ 5124]\n",
      "loss: 1.322868  [ 3600/ 5124]\n",
      "loss: 0.122034  [ 4000/ 5124]\n",
      "loss: 0.324583  [ 4400/ 5124]\n",
      "loss: 0.077310  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4306\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.360437 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.396112  [    0/ 5124]\n",
      "loss: 1.312121  [  400/ 5124]\n",
      "loss: 0.561504  [  800/ 5124]\n",
      "loss: 0.570095  [ 1200/ 5124]\n",
      "loss: 0.460168  [ 1600/ 5124]\n",
      "loss: 0.221211  [ 2000/ 5124]\n",
      "loss: 0.152825  [ 2400/ 5124]\n",
      "loss: 0.534537  [ 2800/ 5124]\n",
      "loss: 1.685621  [ 3200/ 5124]\n",
      "loss: 0.278229  [ 3600/ 5124]\n",
      "loss: 0.493878  [ 4000/ 5124]\n",
      "loss: 0.412879  [ 4400/ 5124]\n",
      "loss: 0.416894  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4359\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.357915 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.137000  [    0/ 5124]\n",
      "loss: 0.572177  [  400/ 5124]\n",
      "loss: 0.224718  [  800/ 5124]\n",
      "loss: 0.336114  [ 1200/ 5124]\n",
      "loss: 0.572150  [ 1600/ 5124]\n",
      "loss: 0.167747  [ 2000/ 5124]\n",
      "loss: 0.373327  [ 2400/ 5124]\n",
      "loss: 0.178574  [ 2800/ 5124]\n",
      "loss: 0.868018  [ 3200/ 5124]\n",
      "loss: 0.383905  [ 3600/ 5124]\n",
      "loss: 0.248393  [ 4000/ 5124]\n",
      "loss: 0.231592  [ 4400/ 5124]\n",
      "loss: 0.394880  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4414\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.380902 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.336920  [    0/ 5124]\n",
      "loss: 0.385471  [  400/ 5124]\n",
      "loss: 0.334221  [  800/ 5124]\n",
      "loss: 0.710079  [ 1200/ 5124]\n",
      "loss: 0.259990  [ 1600/ 5124]\n",
      "loss: 0.096630  [ 2000/ 5124]\n",
      "loss: 0.086842  [ 2400/ 5124]\n",
      "loss: 0.705600  [ 2800/ 5124]\n",
      "loss: 0.989034  [ 3200/ 5124]\n",
      "loss: 0.403552  [ 3600/ 5124]\n",
      "loss: 0.754888  [ 4000/ 5124]\n",
      "loss: 0.432784  [ 4400/ 5124]\n",
      "loss: 0.673577  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4300\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.344137 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.363759  [    0/ 5124]\n",
      "loss: 0.166282  [  400/ 5124]\n",
      "loss: 0.678221  [  800/ 5124]\n",
      "loss: 0.225419  [ 1200/ 5124]\n",
      "loss: 0.481209  [ 1600/ 5124]\n",
      "loss: 0.253456  [ 2000/ 5124]\n",
      "loss: 0.452624  [ 2400/ 5124]\n",
      "loss: 0.381832  [ 2800/ 5124]\n",
      "loss: 0.143695  [ 3200/ 5124]\n",
      "loss: 0.154546  [ 3600/ 5124]\n",
      "loss: 0.403956  [ 4000/ 5124]\n",
      "loss: 0.149126  [ 4400/ 5124]\n",
      "loss: 0.762894  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4307\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.364272 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.387098  [    0/ 5124]\n",
      "loss: 0.603024  [  400/ 5124]\n",
      "loss: 0.236238  [  800/ 5124]\n",
      "loss: 0.268125  [ 1200/ 5124]\n",
      "loss: 0.380194  [ 1600/ 5124]\n",
      "loss: 0.448257  [ 2000/ 5124]\n",
      "loss: 0.546780  [ 2400/ 5124]\n",
      "loss: 0.458860  [ 2800/ 5124]\n",
      "loss: 0.288787  [ 3200/ 5124]\n",
      "loss: 0.352554  [ 3600/ 5124]\n",
      "loss: 0.184689  [ 4000/ 5124]\n",
      "loss: 0.181454  [ 4400/ 5124]\n",
      "loss: 0.597902  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4328\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.377519 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.358501  [    0/ 5124]\n",
      "loss: 0.282919  [  400/ 5124]\n",
      "loss: 0.260655  [  800/ 5124]\n",
      "loss: 0.478551  [ 1200/ 5124]\n",
      "loss: 1.365402  [ 1600/ 5124]\n",
      "loss: 0.463108  [ 2000/ 5124]\n",
      "loss: 0.240947  [ 2400/ 5124]\n",
      "loss: 0.649610  [ 2800/ 5124]\n",
      "loss: 0.226110  [ 3200/ 5124]\n",
      "loss: 0.168946  [ 3600/ 5124]\n",
      "loss: 0.333179  [ 4000/ 5124]\n",
      "loss: 0.359115  [ 4400/ 5124]\n",
      "loss: 0.148121  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4399\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.349823 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.376977  [    0/ 5124]\n",
      "loss: 0.718530  [  400/ 5124]\n",
      "loss: 0.466184  [  800/ 5124]\n",
      "loss: 0.268029  [ 1200/ 5124]\n",
      "loss: 0.356212  [ 1600/ 5124]\n",
      "loss: 0.291235  [ 2000/ 5124]\n",
      "loss: 0.397313  [ 2400/ 5124]\n",
      "loss: 0.443578  [ 2800/ 5124]\n",
      "loss: 0.381035  [ 3200/ 5124]\n",
      "loss: 0.229230  [ 3600/ 5124]\n",
      "loss: 0.503844  [ 4000/ 5124]\n",
      "loss: 0.336731  [ 4400/ 5124]\n",
      "loss: 0.572793  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4497\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.355065 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.708303  [    0/ 5124]\n",
      "loss: 0.405361  [  400/ 5124]\n",
      "loss: 0.110247  [  800/ 5124]\n",
      "loss: 0.458770  [ 1200/ 5124]\n",
      "loss: 0.245257  [ 1600/ 5124]\n",
      "loss: 1.697069  [ 2000/ 5124]\n",
      "loss: 0.570159  [ 2400/ 5124]\n",
      "loss: 0.133409  [ 2800/ 5124]\n",
      "loss: 0.464839  [ 3200/ 5124]\n",
      "loss: 0.104727  [ 3600/ 5124]\n",
      "loss: 0.104005  [ 4000/ 5124]\n",
      "loss: 0.170715  [ 4400/ 5124]\n",
      "loss: 0.250432  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4485\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.341109 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.330911  [    0/ 5124]\n",
      "loss: 0.430017  [  400/ 5124]\n",
      "loss: 0.360428  [  800/ 5124]\n",
      "loss: 0.128213  [ 1200/ 5124]\n",
      "loss: 0.136864  [ 1600/ 5124]\n",
      "loss: 0.606010  [ 2000/ 5124]\n",
      "loss: 0.204695  [ 2400/ 5124]\n",
      "loss: 0.295858  [ 2800/ 5124]\n",
      "loss: 0.219383  [ 3200/ 5124]\n",
      "loss: 0.773488  [ 3600/ 5124]\n",
      "loss: 0.459140  [ 4000/ 5124]\n",
      "loss: 0.191438  [ 4400/ 5124]\n",
      "loss: 0.790610  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4388\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.368328 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.623003  [    0/ 5124]\n",
      "loss: 0.373249  [  400/ 5124]\n",
      "loss: 0.227796  [  800/ 5124]\n",
      "loss: 0.144480  [ 1200/ 5124]\n",
      "loss: 0.143418  [ 1600/ 5124]\n",
      "loss: 0.360871  [ 2000/ 5124]\n",
      "loss: 0.196417  [ 2400/ 5124]\n",
      "loss: 0.702079  [ 2800/ 5124]\n",
      "loss: 0.102001  [ 3200/ 5124]\n",
      "loss: 1.306014  [ 3600/ 5124]\n",
      "loss: 0.181471  [ 4000/ 5124]\n",
      "loss: 0.596489  [ 4400/ 5124]\n",
      "loss: 0.655062  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4312\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.351826 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.265299  [    0/ 5124]\n",
      "loss: 0.170762  [  400/ 5124]\n",
      "loss: 0.427850  [  800/ 5124]\n",
      "loss: 0.386609  [ 1200/ 5124]\n",
      "loss: 1.273597  [ 1600/ 5124]\n",
      "loss: 0.567016  [ 2000/ 5124]\n",
      "loss: 0.327371  [ 2400/ 5124]\n",
      "loss: 0.240177  [ 2800/ 5124]\n",
      "loss: 0.512217  [ 3200/ 5124]\n",
      "loss: 0.554351  [ 3600/ 5124]\n",
      "loss: 0.990574  [ 4000/ 5124]\n",
      "loss: 1.422483  [ 4400/ 5124]\n",
      "loss: 0.436186  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4376\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.352020 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.796418  [    0/ 5124]\n",
      "loss: 0.178303  [  400/ 5124]\n",
      "loss: 0.415172  [  800/ 5124]\n",
      "loss: 0.568400  [ 1200/ 5124]\n",
      "loss: 0.574584  [ 1600/ 5124]\n",
      "loss: 0.258842  [ 2000/ 5124]\n",
      "loss: 0.502232  [ 2400/ 5124]\n",
      "loss: 0.198428  [ 2800/ 5124]\n",
      "loss: 0.191214  [ 3200/ 5124]\n",
      "loss: 0.805697  [ 3600/ 5124]\n",
      "loss: 0.330684  [ 4000/ 5124]\n",
      "loss: 0.071372  [ 4400/ 5124]\n",
      "loss: 0.887930  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4335\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.366704 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.500324  [    0/ 5124]\n",
      "loss: 0.378042  [  400/ 5124]\n",
      "loss: 0.430566  [  800/ 5124]\n",
      "loss: 0.365425  [ 1200/ 5124]\n",
      "loss: 0.580632  [ 1600/ 5124]\n",
      "loss: 0.383708  [ 2000/ 5124]\n",
      "loss: 0.535026  [ 2400/ 5124]\n",
      "loss: 0.371487  [ 2800/ 5124]\n",
      "loss: 0.668761  [ 3200/ 5124]\n",
      "loss: 0.695926  [ 3600/ 5124]\n",
      "loss: 0.266135  [ 4000/ 5124]\n",
      "loss: 0.780672  [ 4400/ 5124]\n",
      "loss: 0.157921  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4250\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.348428 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.190822  [    0/ 5124]\n",
      "loss: 0.351527  [  400/ 5124]\n",
      "loss: 0.079200  [  800/ 5124]\n",
      "loss: 0.252829  [ 1200/ 5124]\n",
      "loss: 0.288754  [ 1600/ 5124]\n",
      "loss: 0.512474  [ 2000/ 5124]\n",
      "loss: 0.318302  [ 2400/ 5124]\n",
      "loss: 0.094406  [ 2800/ 5124]\n",
      "loss: 0.657231  [ 3200/ 5124]\n",
      "loss: 0.631029  [ 3600/ 5124]\n",
      "loss: 0.332953  [ 4000/ 5124]\n",
      "loss: 0.296494  [ 4400/ 5124]\n",
      "loss: 0.338044  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4473\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.357513 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.400584  [    0/ 5124]\n",
      "loss: 0.225267  [  400/ 5124]\n",
      "loss: 0.457605  [  800/ 5124]\n",
      "loss: 0.749528  [ 1200/ 5124]\n",
      "loss: 0.799690  [ 1600/ 5124]\n",
      "loss: 0.635630  [ 2000/ 5124]\n",
      "loss: 0.219618  [ 2400/ 5124]\n",
      "loss: 0.206068  [ 2800/ 5124]\n",
      "loss: 0.414359  [ 3200/ 5124]\n",
      "loss: 0.467448  [ 3600/ 5124]\n",
      "loss: 0.311091  [ 4000/ 5124]\n",
      "loss: 0.172122  [ 4400/ 5124]\n",
      "loss: 1.166838  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4269\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.337112 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.443210  [    0/ 5124]\n",
      "loss: 0.544461  [  400/ 5124]\n",
      "loss: 0.361292  [  800/ 5124]\n",
      "loss: 0.202847  [ 1200/ 5124]\n",
      "loss: 0.318980  [ 1600/ 5124]\n",
      "loss: 0.405808  [ 2000/ 5124]\n",
      "loss: 0.441572  [ 2400/ 5124]\n",
      "loss: 0.452528  [ 2800/ 5124]\n",
      "loss: 0.150588  [ 3200/ 5124]\n",
      "loss: 0.315180  [ 3600/ 5124]\n",
      "loss: 0.515853  [ 4000/ 5124]\n",
      "loss: 0.157105  [ 4400/ 5124]\n",
      "loss: 0.169562  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4256\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.340912 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.092387  [    0/ 5124]\n",
      "loss: 0.261191  [  400/ 5124]\n",
      "loss: 0.474762  [  800/ 5124]\n",
      "loss: 0.897012  [ 1200/ 5124]\n",
      "loss: 0.663598  [ 1600/ 5124]\n",
      "loss: 0.130050  [ 2000/ 5124]\n",
      "loss: 0.395539  [ 2400/ 5124]\n",
      "loss: 1.352638  [ 2800/ 5124]\n",
      "loss: 0.293382  [ 3200/ 5124]\n",
      "loss: 0.305961  [ 3600/ 5124]\n",
      "loss: 0.214678  [ 4000/ 5124]\n",
      "loss: 0.116481  [ 4400/ 5124]\n",
      "loss: 0.170649  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4314\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.343629 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.381525  [    0/ 5124]\n",
      "loss: 0.411314  [  400/ 5124]\n",
      "loss: 0.169672  [  800/ 5124]\n",
      "loss: 0.116578  [ 1200/ 5124]\n",
      "loss: 0.335887  [ 1600/ 5124]\n",
      "loss: 0.873530  [ 2000/ 5124]\n",
      "loss: 1.320240  [ 2400/ 5124]\n",
      "loss: 0.154167  [ 2800/ 5124]\n",
      "loss: 0.128107  [ 3200/ 5124]\n",
      "loss: 0.086518  [ 3600/ 5124]\n",
      "loss: 0.306091  [ 4000/ 5124]\n",
      "loss: 0.694845  [ 4400/ 5124]\n",
      "loss: 0.203889  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4145\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.348939 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.746756  [    0/ 5124]\n",
      "loss: 0.273404  [  400/ 5124]\n",
      "loss: 0.324567  [  800/ 5124]\n",
      "loss: 0.283342  [ 1200/ 5124]\n",
      "loss: 0.403601  [ 1600/ 5124]\n",
      "loss: 0.149584  [ 2000/ 5124]\n",
      "loss: 0.113553  [ 2400/ 5124]\n",
      "loss: 0.442179  [ 2800/ 5124]\n",
      "loss: 0.260559  [ 3200/ 5124]\n",
      "loss: 0.180238  [ 3600/ 5124]\n",
      "loss: 1.001434  [ 4000/ 5124]\n",
      "loss: 0.120558  [ 4400/ 5124]\n",
      "loss: 0.400442  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4302\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.333649 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.474822  [    0/ 5124]\n",
      "loss: 0.379467  [  400/ 5124]\n",
      "loss: 0.441452  [  800/ 5124]\n",
      "loss: 1.675081  [ 1200/ 5124]\n",
      "loss: 0.193536  [ 1600/ 5124]\n",
      "loss: 0.283152  [ 2000/ 5124]\n",
      "loss: 0.417833  [ 2400/ 5124]\n",
      "loss: 0.619249  [ 2800/ 5124]\n",
      "loss: 0.441624  [ 3200/ 5124]\n",
      "loss: 0.722781  [ 3600/ 5124]\n",
      "loss: 0.532731  [ 4000/ 5124]\n",
      "loss: 0.388061  [ 4400/ 5124]\n",
      "loss: 0.549074  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4310\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.345708 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.179860  [    0/ 5124]\n",
      "loss: 0.346119  [  400/ 5124]\n",
      "loss: 0.879498  [  800/ 5124]\n",
      "loss: 0.785734  [ 1200/ 5124]\n",
      "loss: 0.116960  [ 1600/ 5124]\n",
      "loss: 0.411679  [ 2000/ 5124]\n",
      "loss: 0.553632  [ 2400/ 5124]\n",
      "loss: 0.345691  [ 2800/ 5124]\n",
      "loss: 0.658268  [ 3200/ 5124]\n",
      "loss: 0.200328  [ 3600/ 5124]\n",
      "loss: 0.548600  [ 4000/ 5124]\n",
      "loss: 0.580796  [ 4400/ 5124]\n",
      "loss: 0.470407  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4400\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.342337 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.455244  [    0/ 5124]\n",
      "loss: 0.321425  [  400/ 5124]\n",
      "loss: 0.225469  [  800/ 5124]\n",
      "loss: 0.342766  [ 1200/ 5124]\n",
      "loss: 0.143277  [ 1600/ 5124]\n",
      "loss: 0.534346  [ 2000/ 5124]\n",
      "loss: 0.644536  [ 2400/ 5124]\n",
      "loss: 0.321012  [ 2800/ 5124]\n",
      "loss: 0.821923  [ 3200/ 5124]\n",
      "loss: 0.581015  [ 3600/ 5124]\n",
      "loss: 0.404529  [ 4000/ 5124]\n",
      "loss: 0.265412  [ 4400/ 5124]\n",
      "loss: 0.839999  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4269\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.340718 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.355878  [    0/ 5124]\n",
      "loss: 0.255837  [  400/ 5124]\n",
      "loss: 0.097925  [  800/ 5124]\n",
      "loss: 0.201546  [ 1200/ 5124]\n",
      "loss: 0.533969  [ 1600/ 5124]\n",
      "loss: 0.411990  [ 2000/ 5124]\n",
      "loss: 0.336248  [ 2400/ 5124]\n",
      "loss: 0.600866  [ 2800/ 5124]\n",
      "loss: 0.579605  [ 3200/ 5124]\n",
      "loss: 0.166997  [ 3600/ 5124]\n",
      "loss: 0.276088  [ 4000/ 5124]\n",
      "loss: 0.215650  [ 4400/ 5124]\n",
      "loss: 1.183543  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4207\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.337006 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.192440  [    0/ 5124]\n",
      "loss: 0.715969  [  400/ 5124]\n",
      "loss: 0.436038  [  800/ 5124]\n",
      "loss: 0.632074  [ 1200/ 5124]\n",
      "loss: 0.922613  [ 1600/ 5124]\n",
      "loss: 0.292485  [ 2000/ 5124]\n",
      "loss: 0.321760  [ 2400/ 5124]\n",
      "loss: 0.559092  [ 2800/ 5124]\n",
      "loss: 0.396134  [ 3200/ 5124]\n",
      "loss: 0.611754  [ 3600/ 5124]\n",
      "loss: 0.271472  [ 4000/ 5124]\n",
      "loss: 0.667242  [ 4400/ 5124]\n",
      "loss: 0.657691  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4257\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.344916 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.165424  [    0/ 5124]\n",
      "loss: 0.193442  [  400/ 5124]\n",
      "loss: 0.176991  [  800/ 5124]\n",
      "loss: 0.307055  [ 1200/ 5124]\n",
      "loss: 0.593323  [ 1600/ 5124]\n",
      "loss: 0.618973  [ 2000/ 5124]\n",
      "loss: 0.461732  [ 2400/ 5124]\n",
      "loss: 0.857227  [ 2800/ 5124]\n",
      "loss: 0.579478  [ 3200/ 5124]\n",
      "loss: 0.285208  [ 3600/ 5124]\n",
      "loss: 0.247324  [ 4000/ 5124]\n",
      "loss: 0.248198  [ 4400/ 5124]\n",
      "loss: 0.239574  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4196\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.333192 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.194563  [    0/ 5124]\n",
      "loss: 0.316724  [  400/ 5124]\n",
      "loss: 0.196224  [  800/ 5124]\n",
      "loss: 0.298172  [ 1200/ 5124]\n",
      "loss: 1.166799  [ 1600/ 5124]\n",
      "loss: 0.320199  [ 2000/ 5124]\n",
      "loss: 0.155151  [ 2400/ 5124]\n",
      "loss: 1.193744  [ 2800/ 5124]\n",
      "loss: 0.119863  [ 3200/ 5124]\n",
      "loss: 0.338929  [ 3600/ 5124]\n",
      "loss: 0.756185  [ 4000/ 5124]\n",
      "loss: 0.481885  [ 4400/ 5124]\n",
      "loss: 0.086432  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4321\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.365869 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.637987  [    0/ 5124]\n",
      "loss: 0.392299  [  400/ 5124]\n",
      "loss: 0.134897  [  800/ 5124]\n",
      "loss: 0.398037  [ 1200/ 5124]\n",
      "loss: 0.261131  [ 1600/ 5124]\n",
      "loss: 0.438633  [ 2000/ 5124]\n",
      "loss: 0.123608  [ 2400/ 5124]\n",
      "loss: 0.275915  [ 2800/ 5124]\n",
      "loss: 0.387008  [ 3200/ 5124]\n",
      "loss: 0.506302  [ 3600/ 5124]\n",
      "loss: 0.239045  [ 4000/ 5124]\n",
      "loss: 0.858873  [ 4400/ 5124]\n",
      "loss: 0.187085  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4349\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.337761 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.286946  [    0/ 5124]\n",
      "loss: 0.391150  [  400/ 5124]\n",
      "loss: 0.581672  [  800/ 5124]\n",
      "loss: 0.709317  [ 1200/ 5124]\n",
      "loss: 0.334812  [ 1600/ 5124]\n",
      "loss: 0.311432  [ 2000/ 5124]\n",
      "loss: 0.205272  [ 2400/ 5124]\n",
      "loss: 0.442178  [ 2800/ 5124]\n",
      "loss: 0.525462  [ 3200/ 5124]\n",
      "loss: 0.446454  [ 3600/ 5124]\n",
      "loss: 0.360075  [ 4000/ 5124]\n",
      "loss: 0.293942  [ 4400/ 5124]\n",
      "loss: 0.503739  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4258\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.325353 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.328620  [    0/ 5124]\n",
      "loss: 0.144691  [  400/ 5124]\n",
      "loss: 0.100336  [  800/ 5124]\n",
      "loss: 0.188540  [ 1200/ 5124]\n",
      "loss: 0.352734  [ 1600/ 5124]\n",
      "loss: 0.796756  [ 2000/ 5124]\n",
      "loss: 0.424245  [ 2400/ 5124]\n",
      "loss: 0.569435  [ 2800/ 5124]\n",
      "loss: 0.260410  [ 3200/ 5124]\n",
      "loss: 0.773849  [ 3600/ 5124]\n",
      "loss: 0.964050  [ 4000/ 5124]\n",
      "loss: 0.180861  [ 4400/ 5124]\n",
      "loss: 0.422109  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4236\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.341848 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.639926  [    0/ 5124]\n",
      "loss: 0.508975  [  400/ 5124]\n",
      "loss: 0.121805  [  800/ 5124]\n",
      "loss: 0.512755  [ 1200/ 5124]\n",
      "loss: 0.398920  [ 1600/ 5124]\n",
      "loss: 0.368191  [ 2000/ 5124]\n",
      "loss: 0.799534  [ 2400/ 5124]\n",
      "loss: 0.190661  [ 2800/ 5124]\n",
      "loss: 0.457330  [ 3200/ 5124]\n",
      "loss: 0.351021  [ 3600/ 5124]\n",
      "loss: 0.429677  [ 4000/ 5124]\n",
      "loss: 0.517831  [ 4400/ 5124]\n",
      "loss: 0.272416  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4240\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.332685 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.792661  [    0/ 5124]\n",
      "loss: 0.775257  [  400/ 5124]\n",
      "loss: 0.555307  [  800/ 5124]\n",
      "loss: 0.672355  [ 1200/ 5124]\n",
      "loss: 0.384241  [ 1600/ 5124]\n",
      "loss: 0.267827  [ 2000/ 5124]\n",
      "loss: 0.491206  [ 2400/ 5124]\n",
      "loss: 0.344242  [ 2800/ 5124]\n",
      "loss: 0.495079  [ 3200/ 5124]\n",
      "loss: 0.256980  [ 3600/ 5124]\n",
      "loss: 0.564278  [ 4000/ 5124]\n",
      "loss: 0.753286  [ 4400/ 5124]\n",
      "loss: 0.121486  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4182\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.347521 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.445670  [    0/ 5124]\n",
      "loss: 0.132326  [  400/ 5124]\n",
      "loss: 0.459006  [  800/ 5124]\n",
      "loss: 1.376118  [ 1200/ 5124]\n",
      "loss: 0.343201  [ 1600/ 5124]\n",
      "loss: 0.084875  [ 2000/ 5124]\n",
      "loss: 0.179997  [ 2400/ 5124]\n",
      "loss: 0.267088  [ 2800/ 5124]\n",
      "loss: 0.588596  [ 3200/ 5124]\n",
      "loss: 0.536861  [ 3600/ 5124]\n",
      "loss: 0.465346  [ 4000/ 5124]\n",
      "loss: 1.670718  [ 4400/ 5124]\n",
      "loss: 0.337743  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4206\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.337304 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.352752  [    0/ 5124]\n",
      "loss: 0.073372  [  400/ 5124]\n",
      "loss: 0.464338  [  800/ 5124]\n",
      "loss: 0.886733  [ 1200/ 5124]\n",
      "loss: 0.325863  [ 1600/ 5124]\n",
      "loss: 0.101845  [ 2000/ 5124]\n",
      "loss: 0.193796  [ 2400/ 5124]\n",
      "loss: 0.257273  [ 2800/ 5124]\n",
      "loss: 0.415132  [ 3200/ 5124]\n",
      "loss: 1.060817  [ 3600/ 5124]\n",
      "loss: 0.246058  [ 4000/ 5124]\n",
      "loss: 0.408993  [ 4400/ 5124]\n",
      "loss: 0.363346  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4348\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.323963 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.108947  [    0/ 5124]\n",
      "loss: 0.622472  [  400/ 5124]\n",
      "loss: 0.174564  [  800/ 5124]\n",
      "loss: 0.299045  [ 1200/ 5124]\n",
      "loss: 0.087619  [ 1600/ 5124]\n",
      "loss: 0.177872  [ 2000/ 5124]\n",
      "loss: 0.317362  [ 2400/ 5124]\n",
      "loss: 0.238076  [ 2800/ 5124]\n",
      "loss: 0.304295  [ 3200/ 5124]\n",
      "loss: 1.218604  [ 3600/ 5124]\n",
      "loss: 0.159729  [ 4000/ 5124]\n",
      "loss: 0.365299  [ 4400/ 5124]\n",
      "loss: 0.788790  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4199\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.321828 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.343008  [    0/ 5124]\n",
      "loss: 0.152446  [  400/ 5124]\n",
      "loss: 0.706856  [  800/ 5124]\n",
      "loss: 0.536895  [ 1200/ 5124]\n",
      "loss: 0.536037  [ 1600/ 5124]\n",
      "loss: 0.165069  [ 2000/ 5124]\n",
      "loss: 0.844203  [ 2400/ 5124]\n",
      "loss: 0.219448  [ 2800/ 5124]\n",
      "loss: 0.489372  [ 3200/ 5124]\n",
      "loss: 0.339293  [ 3600/ 5124]\n",
      "loss: 0.568291  [ 4000/ 5124]\n",
      "loss: 0.949190  [ 4400/ 5124]\n",
      "loss: 0.827202  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4240\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.326702 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.524884  [    0/ 5124]\n",
      "loss: 0.396075  [  400/ 5124]\n",
      "loss: 0.867146  [  800/ 5124]\n",
      "loss: 0.524520  [ 1200/ 5124]\n",
      "loss: 0.133540  [ 1600/ 5124]\n",
      "loss: 0.656029  [ 2000/ 5124]\n",
      "loss: 0.105485  [ 2400/ 5124]\n",
      "loss: 0.338956  [ 2800/ 5124]\n",
      "loss: 0.381984  [ 3200/ 5124]\n",
      "loss: 0.525986  [ 3600/ 5124]\n",
      "loss: 0.160387  [ 4000/ 5124]\n",
      "loss: 0.149753  [ 4400/ 5124]\n",
      "loss: 0.383883  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4245\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.334219 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.352709  [    0/ 5124]\n",
      "loss: 0.152673  [  400/ 5124]\n",
      "loss: 0.949698  [  800/ 5124]\n",
      "loss: 0.392352  [ 1200/ 5124]\n",
      "loss: 0.724756  [ 1600/ 5124]\n",
      "loss: 0.546705  [ 2000/ 5124]\n",
      "loss: 0.281797  [ 2400/ 5124]\n",
      "loss: 0.364918  [ 2800/ 5124]\n",
      "loss: 0.730631  [ 3200/ 5124]\n",
      "loss: 0.368604  [ 3600/ 5124]\n",
      "loss: 0.289092  [ 4000/ 5124]\n",
      "loss: 0.296103  [ 4400/ 5124]\n",
      "loss: 0.429396  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4241\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.325673 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.288432  [    0/ 5124]\n",
      "loss: 0.703249  [  400/ 5124]\n",
      "loss: 0.230787  [  800/ 5124]\n",
      "loss: 0.110596  [ 1200/ 5124]\n",
      "loss: 0.615938  [ 1600/ 5124]\n",
      "loss: 0.260315  [ 2000/ 5124]\n",
      "loss: 0.181788  [ 2400/ 5124]\n",
      "loss: 0.627020  [ 2800/ 5124]\n",
      "loss: 0.584853  [ 3200/ 5124]\n",
      "loss: 0.309649  [ 3600/ 5124]\n",
      "loss: 0.256779  [ 4000/ 5124]\n",
      "loss: 0.752372  [ 4400/ 5124]\n",
      "loss: 0.252195  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4166\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.316766 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.181078  [    0/ 5124]\n",
      "loss: 0.804599  [  400/ 5124]\n",
      "loss: 0.304406  [  800/ 5124]\n",
      "loss: 0.817464  [ 1200/ 5124]\n",
      "loss: 0.346894  [ 1600/ 5124]\n",
      "loss: 0.317728  [ 2000/ 5124]\n",
      "loss: 0.345641  [ 2400/ 5124]\n",
      "loss: 0.687350  [ 2800/ 5124]\n",
      "loss: 0.442378  [ 3200/ 5124]\n",
      "loss: 0.061237  [ 3600/ 5124]\n",
      "loss: 0.114794  [ 4000/ 5124]\n",
      "loss: 0.158724  [ 4400/ 5124]\n",
      "loss: 0.390837  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4282\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.321252 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.672382  [    0/ 5124]\n",
      "loss: 0.143372  [  400/ 5124]\n",
      "loss: 0.387532  [  800/ 5124]\n",
      "loss: 0.207587  [ 1200/ 5124]\n",
      "loss: 0.488334  [ 1600/ 5124]\n",
      "loss: 0.615484  [ 2000/ 5124]\n",
      "loss: 0.122603  [ 2400/ 5124]\n",
      "loss: 0.334326  [ 2800/ 5124]\n",
      "loss: 0.093920  [ 3200/ 5124]\n",
      "loss: 0.376644  [ 3600/ 5124]\n",
      "loss: 0.168267  [ 4000/ 5124]\n",
      "loss: 0.148295  [ 4400/ 5124]\n",
      "loss: 0.229429  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4233\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.319017 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.753862  [    0/ 5124]\n",
      "loss: 0.248895  [  400/ 5124]\n",
      "loss: 0.263795  [  800/ 5124]\n",
      "loss: 0.210336  [ 1200/ 5124]\n",
      "loss: 0.145300  [ 1600/ 5124]\n",
      "loss: 0.581512  [ 2000/ 5124]\n",
      "loss: 0.214263  [ 2400/ 5124]\n",
      "loss: 0.249040  [ 2800/ 5124]\n",
      "loss: 0.430713  [ 3200/ 5124]\n",
      "loss: 0.276815  [ 3600/ 5124]\n",
      "loss: 0.387382  [ 4000/ 5124]\n",
      "loss: 0.556774  [ 4400/ 5124]\n",
      "loss: 0.094440  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4251\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.319482 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.583936  [    0/ 5124]\n",
      "loss: 0.931831  [  400/ 5124]\n",
      "loss: 0.459009  [  800/ 5124]\n",
      "loss: 0.261412  [ 1200/ 5124]\n",
      "loss: 0.793077  [ 1600/ 5124]\n",
      "loss: 0.309186  [ 2000/ 5124]\n",
      "loss: 0.126138  [ 2400/ 5124]\n",
      "loss: 0.584465  [ 2800/ 5124]\n",
      "loss: 0.118313  [ 3200/ 5124]\n",
      "loss: 0.432828  [ 3600/ 5124]\n",
      "loss: 0.311440  [ 4000/ 5124]\n",
      "loss: 0.352272  [ 4400/ 5124]\n",
      "loss: 0.621936  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4246\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.307876 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.178387  [    0/ 5124]\n",
      "loss: 0.573665  [  400/ 5124]\n",
      "loss: 0.240390  [  800/ 5124]\n",
      "loss: 0.332104  [ 1200/ 5124]\n",
      "loss: 0.152689  [ 1600/ 5124]\n",
      "loss: 0.394235  [ 2000/ 5124]\n",
      "loss: 0.117821  [ 2400/ 5124]\n",
      "loss: 0.527218  [ 2800/ 5124]\n",
      "loss: 0.369701  [ 3200/ 5124]\n",
      "loss: 0.446439  [ 3600/ 5124]\n",
      "loss: 0.383904  [ 4000/ 5124]\n",
      "loss: 0.222032  [ 4400/ 5124]\n",
      "loss: 0.504634  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4045\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.349792 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.599892  [    0/ 5124]\n",
      "loss: 0.435710  [  400/ 5124]\n",
      "loss: 0.346197  [  800/ 5124]\n",
      "loss: 0.429400  [ 1200/ 5124]\n",
      "loss: 0.458541  [ 1600/ 5124]\n",
      "loss: 0.935282  [ 2000/ 5124]\n",
      "loss: 0.568725  [ 2400/ 5124]\n",
      "loss: 0.341807  [ 2800/ 5124]\n",
      "loss: 0.912627  [ 3200/ 5124]\n",
      "loss: 0.795889  [ 3600/ 5124]\n",
      "loss: 0.204593  [ 4000/ 5124]\n",
      "loss: 0.473751  [ 4400/ 5124]\n",
      "loss: 0.204292  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4119\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.310758 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.405331  [    0/ 5124]\n",
      "loss: 0.101509  [  400/ 5124]\n",
      "loss: 0.364168  [  800/ 5124]\n",
      "loss: 0.399184  [ 1200/ 5124]\n",
      "loss: 0.370897  [ 1600/ 5124]\n",
      "loss: 0.409813  [ 2000/ 5124]\n",
      "loss: 0.130069  [ 2400/ 5124]\n",
      "loss: 0.173216  [ 2800/ 5124]\n",
      "loss: 0.504991  [ 3200/ 5124]\n",
      "loss: 0.461687  [ 3600/ 5124]\n",
      "loss: 0.182035  [ 4000/ 5124]\n",
      "loss: 0.196635  [ 4400/ 5124]\n",
      "loss: 0.348693  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4217\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.326296 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.509594  [    0/ 5124]\n",
      "loss: 0.400726  [  400/ 5124]\n",
      "loss: 0.671897  [  800/ 5124]\n",
      "loss: 0.430413  [ 1200/ 5124]\n",
      "loss: 0.707893  [ 1600/ 5124]\n",
      "loss: 0.087027  [ 2000/ 5124]\n",
      "loss: 0.590717  [ 2400/ 5124]\n",
      "loss: 0.099596  [ 2800/ 5124]\n",
      "loss: 0.148468  [ 3200/ 5124]\n",
      "loss: 0.515896  [ 3600/ 5124]\n",
      "loss: 0.338850  [ 4000/ 5124]\n",
      "loss: 0.632915  [ 4400/ 5124]\n",
      "loss: 0.112942  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4277\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.312224 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.268020  [    0/ 5124]\n",
      "loss: 0.173749  [  400/ 5124]\n",
      "loss: 0.399246  [  800/ 5124]\n",
      "loss: 0.551444  [ 1200/ 5124]\n",
      "loss: 0.463408  [ 1600/ 5124]\n",
      "loss: 0.387044  [ 2000/ 5124]\n",
      "loss: 0.299445  [ 2400/ 5124]\n",
      "loss: 0.098687  [ 2800/ 5124]\n",
      "loss: 0.127810  [ 3200/ 5124]\n",
      "loss: 0.466411  [ 3600/ 5124]\n",
      "loss: 1.060803  [ 4000/ 5124]\n",
      "loss: 0.237333  [ 4400/ 5124]\n",
      "loss: 0.334154  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4248\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.309189 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.485992  [    0/ 5124]\n",
      "loss: 0.100604  [  400/ 5124]\n",
      "loss: 0.553408  [  800/ 5124]\n",
      "loss: 0.542938  [ 1200/ 5124]\n",
      "loss: 0.460053  [ 1600/ 5124]\n",
      "loss: 0.298647  [ 2000/ 5124]\n",
      "loss: 0.559394  [ 2400/ 5124]\n",
      "loss: 0.314576  [ 2800/ 5124]\n",
      "loss: 1.053899  [ 3200/ 5124]\n",
      "loss: 0.448556  [ 3600/ 5124]\n",
      "loss: 0.170721  [ 4000/ 5124]\n",
      "loss: 0.238106  [ 4400/ 5124]\n",
      "loss: 0.447504  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4103\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.314677 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.243070  [    0/ 5124]\n",
      "loss: 0.426330  [  400/ 5124]\n",
      "loss: 0.775278  [  800/ 5124]\n",
      "loss: 0.113683  [ 1200/ 5124]\n",
      "loss: 0.481049  [ 1600/ 5124]\n",
      "loss: 0.204631  [ 2000/ 5124]\n",
      "loss: 1.287388  [ 2400/ 5124]\n",
      "loss: 0.794221  [ 2800/ 5124]\n",
      "loss: 0.646681  [ 3200/ 5124]\n",
      "loss: 0.289478  [ 3600/ 5124]\n",
      "loss: 0.104208  [ 4000/ 5124]\n",
      "loss: 0.391229  [ 4400/ 5124]\n",
      "loss: 0.157355  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4207\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.327617 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.138897  [    0/ 5124]\n",
      "loss: 0.382360  [  400/ 5124]\n",
      "loss: 0.251918  [  800/ 5124]\n",
      "loss: 0.359257  [ 1200/ 5124]\n",
      "loss: 0.706413  [ 1600/ 5124]\n",
      "loss: 0.509274  [ 2000/ 5124]\n",
      "loss: 1.009208  [ 2400/ 5124]\n",
      "loss: 0.172040  [ 2800/ 5124]\n",
      "loss: 0.192093  [ 3200/ 5124]\n",
      "loss: 0.260147  [ 3600/ 5124]\n",
      "loss: 1.089746  [ 4000/ 5124]\n",
      "loss: 0.532326  [ 4400/ 5124]\n",
      "loss: 0.141504  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4218\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.308196 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.428072  [    0/ 5124]\n",
      "loss: 0.914947  [  400/ 5124]\n",
      "loss: 0.206247  [  800/ 5124]\n",
      "loss: 0.553350  [ 1200/ 5124]\n",
      "loss: 0.141070  [ 1600/ 5124]\n",
      "loss: 0.395705  [ 2000/ 5124]\n",
      "loss: 0.280948  [ 2400/ 5124]\n",
      "loss: 0.267618  [ 2800/ 5124]\n",
      "loss: 0.156111  [ 3200/ 5124]\n",
      "loss: 0.422560  [ 3600/ 5124]\n",
      "loss: 0.162181  [ 4000/ 5124]\n",
      "loss: 0.241113  [ 4400/ 5124]\n",
      "loss: 0.311550  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4236\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.340542 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.360020  [    0/ 5124]\n",
      "loss: 0.491704  [  400/ 5124]\n",
      "loss: 0.721404  [  800/ 5124]\n",
      "loss: 0.235674  [ 1200/ 5124]\n",
      "loss: 0.311555  [ 1600/ 5124]\n",
      "loss: 0.098443  [ 2000/ 5124]\n",
      "loss: 0.305835  [ 2400/ 5124]\n",
      "loss: 0.864321  [ 2800/ 5124]\n",
      "loss: 0.276018  [ 3200/ 5124]\n",
      "loss: 0.503774  [ 3600/ 5124]\n",
      "loss: 0.455452  [ 4000/ 5124]\n",
      "loss: 0.450206  [ 4400/ 5124]\n",
      "loss: 0.238551  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4204\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.315352 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.675592  [    0/ 5124]\n",
      "loss: 0.345123  [  400/ 5124]\n",
      "loss: 1.763894  [  800/ 5124]\n",
      "loss: 0.653576  [ 1200/ 5124]\n",
      "loss: 0.358316  [ 1600/ 5124]\n",
      "loss: 0.651396  [ 2000/ 5124]\n",
      "loss: 0.118344  [ 2400/ 5124]\n",
      "loss: 1.377180  [ 2800/ 5124]\n",
      "loss: 0.562919  [ 3200/ 5124]\n",
      "loss: 0.955196  [ 3600/ 5124]\n",
      "loss: 0.870986  [ 4000/ 5124]\n",
      "loss: 0.648922  [ 4400/ 5124]\n",
      "loss: 0.702263  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4127\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.303056 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.104636  [    0/ 5124]\n",
      "loss: 0.202215  [  400/ 5124]\n",
      "loss: 0.232840  [  800/ 5124]\n",
      "loss: 0.839177  [ 1200/ 5124]\n",
      "loss: 1.508198  [ 1600/ 5124]\n",
      "loss: 0.422688  [ 2000/ 5124]\n",
      "loss: 0.492027  [ 2400/ 5124]\n",
      "loss: 0.315638  [ 2800/ 5124]\n",
      "loss: 0.178870  [ 3200/ 5124]\n",
      "loss: 0.327922  [ 3600/ 5124]\n",
      "loss: 0.281930  [ 4000/ 5124]\n",
      "loss: 0.119501  [ 4400/ 5124]\n",
      "loss: 0.817786  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4183\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.323847 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.709221  [    0/ 5124]\n",
      "loss: 0.774321  [  400/ 5124]\n",
      "loss: 0.171819  [  800/ 5124]\n",
      "loss: 0.128715  [ 1200/ 5124]\n",
      "loss: 0.177892  [ 1600/ 5124]\n",
      "loss: 0.114262  [ 2000/ 5124]\n",
      "loss: 0.221401  [ 2400/ 5124]\n",
      "loss: 0.220677  [ 2800/ 5124]\n",
      "loss: 0.891189  [ 3200/ 5124]\n",
      "loss: 0.284903  [ 3600/ 5124]\n",
      "loss: 0.330393  [ 4000/ 5124]\n",
      "loss: 0.179908  [ 4400/ 5124]\n",
      "loss: 0.342940  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4116\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.321967 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.329116  [    0/ 5124]\n",
      "loss: 0.230436  [  400/ 5124]\n",
      "loss: 0.175554  [  800/ 5124]\n",
      "loss: 0.211234  [ 1200/ 5124]\n",
      "loss: 0.729263  [ 1600/ 5124]\n",
      "loss: 0.282892  [ 2000/ 5124]\n",
      "loss: 0.761915  [ 2400/ 5124]\n",
      "loss: 0.107939  [ 2800/ 5124]\n",
      "loss: 0.173724  [ 3200/ 5124]\n",
      "loss: 0.280446  [ 3600/ 5124]\n",
      "loss: 0.579788  [ 4000/ 5124]\n",
      "loss: 0.304029  [ 4400/ 5124]\n",
      "loss: 0.949503  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4253\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.310189 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.221947  [    0/ 5124]\n",
      "loss: 0.157556  [  400/ 5124]\n",
      "loss: 1.138593  [  800/ 5124]\n",
      "loss: 0.360516  [ 1200/ 5124]\n",
      "loss: 0.941286  [ 1600/ 5124]\n",
      "loss: 0.502871  [ 2000/ 5124]\n",
      "loss: 0.474096  [ 2400/ 5124]\n",
      "loss: 0.849212  [ 2800/ 5124]\n",
      "loss: 0.835846  [ 3200/ 5124]\n",
      "loss: 0.516684  [ 3600/ 5124]\n",
      "loss: 0.344491  [ 4000/ 5124]\n",
      "loss: 0.415641  [ 4400/ 5124]\n",
      "loss: 0.245005  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4217\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.310098 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.230333  [    0/ 5124]\n",
      "loss: 0.098532  [  400/ 5124]\n",
      "loss: 0.213520  [  800/ 5124]\n",
      "loss: 0.096677  [ 1200/ 5124]\n",
      "loss: 0.636803  [ 1600/ 5124]\n",
      "loss: 0.279122  [ 2000/ 5124]\n",
      "loss: 0.401873  [ 2400/ 5124]\n",
      "loss: 0.126782  [ 2800/ 5124]\n",
      "loss: 0.193467  [ 3200/ 5124]\n",
      "loss: 0.319091  [ 3600/ 5124]\n",
      "loss: 1.225352  [ 4000/ 5124]\n",
      "loss: 0.474696  [ 4400/ 5124]\n",
      "loss: 0.347693  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4185\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.333691 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.711160  [    0/ 5124]\n",
      "loss: 0.858327  [  400/ 5124]\n",
      "loss: 0.360040  [  800/ 5124]\n",
      "loss: 1.168343  [ 1200/ 5124]\n",
      "loss: 0.394301  [ 1600/ 5124]\n",
      "loss: 0.702150  [ 2000/ 5124]\n",
      "loss: 0.588611  [ 2400/ 5124]\n",
      "loss: 0.257900  [ 2800/ 5124]\n",
      "loss: 0.184189  [ 3200/ 5124]\n",
      "loss: 0.554653  [ 3600/ 5124]\n",
      "loss: 0.219946  [ 4000/ 5124]\n",
      "loss: 0.210002  [ 4400/ 5124]\n",
      "loss: 0.273783  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4129\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.319059 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.182090  [    0/ 5124]\n",
      "loss: 0.487032  [  400/ 5124]\n",
      "loss: 0.481008  [  800/ 5124]\n",
      "loss: 0.607430  [ 1200/ 5124]\n",
      "loss: 0.377060  [ 1600/ 5124]\n",
      "loss: 0.446288  [ 2000/ 5124]\n",
      "loss: 1.375723  [ 2400/ 5124]\n",
      "loss: 0.514620  [ 2800/ 5124]\n",
      "loss: 1.054127  [ 3200/ 5124]\n",
      "loss: 0.151429  [ 3600/ 5124]\n",
      "loss: 0.488955  [ 4000/ 5124]\n",
      "loss: 0.678041  [ 4400/ 5124]\n",
      "loss: 0.104943  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4227\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.312192 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.511529  [    0/ 5124]\n",
      "loss: 0.253737  [  400/ 5124]\n",
      "loss: 0.120088  [  800/ 5124]\n",
      "loss: 0.131261  [ 1200/ 5124]\n",
      "loss: 0.242142  [ 1600/ 5124]\n",
      "loss: 0.448353  [ 2000/ 5124]\n",
      "loss: 0.574757  [ 2400/ 5124]\n",
      "loss: 0.588751  [ 2800/ 5124]\n",
      "loss: 0.237089  [ 3200/ 5124]\n",
      "loss: 0.157633  [ 3600/ 5124]\n",
      "loss: 0.257441  [ 4000/ 5124]\n",
      "loss: 0.516505  [ 4400/ 5124]\n",
      "loss: 0.344501  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4117\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.323039 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.090618  [    0/ 5124]\n",
      "loss: 0.626671  [  400/ 5124]\n",
      "loss: 0.258800  [  800/ 5124]\n",
      "loss: 0.510578  [ 1200/ 5124]\n",
      "loss: 0.089375  [ 1600/ 5124]\n",
      "loss: 1.168947  [ 2000/ 5124]\n",
      "loss: 0.135113  [ 2400/ 5124]\n",
      "loss: 0.090414  [ 2800/ 5124]\n",
      "loss: 0.448054  [ 3200/ 5124]\n",
      "loss: 0.165901  [ 3600/ 5124]\n",
      "loss: 0.213349  [ 4000/ 5124]\n",
      "loss: 0.645862  [ 4400/ 5124]\n",
      "loss: 0.360279  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4215\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.296014 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.275768  [    0/ 5124]\n",
      "loss: 0.290855  [  400/ 5124]\n",
      "loss: 0.153652  [  800/ 5124]\n",
      "loss: 0.423555  [ 1200/ 5124]\n",
      "loss: 0.331374  [ 1600/ 5124]\n",
      "loss: 0.539117  [ 2000/ 5124]\n",
      "loss: 0.607709  [ 2400/ 5124]\n",
      "loss: 0.683248  [ 2800/ 5124]\n",
      "loss: 0.221443  [ 3200/ 5124]\n",
      "loss: 0.149411  [ 3600/ 5124]\n",
      "loss: 0.821107  [ 4000/ 5124]\n",
      "loss: 0.466709  [ 4400/ 5124]\n",
      "loss: 0.575903  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4066\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.306089 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.096795  [    0/ 5124]\n",
      "loss: 0.256345  [  400/ 5124]\n",
      "loss: 0.199550  [  800/ 5124]\n",
      "loss: 0.464092  [ 1200/ 5124]\n",
      "loss: 0.157108  [ 1600/ 5124]\n",
      "loss: 0.377905  [ 2000/ 5124]\n",
      "loss: 0.894441  [ 2400/ 5124]\n",
      "loss: 0.478614  [ 2800/ 5124]\n",
      "loss: 0.263656  [ 3200/ 5124]\n",
      "loss: 0.093814  [ 3600/ 5124]\n",
      "loss: 0.378472  [ 4000/ 5124]\n",
      "loss: 0.202555  [ 4400/ 5124]\n",
      "loss: 0.216206  [ 4800/ 5124]\n",
      "Average Training Loss: 0.3976\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.334028 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.155757  [    0/ 5124]\n",
      "loss: 0.111985  [  400/ 5124]\n",
      "loss: 0.259633  [  800/ 5124]\n",
      "loss: 0.595656  [ 1200/ 5124]\n",
      "loss: 0.374771  [ 1600/ 5124]\n",
      "loss: 0.327112  [ 2000/ 5124]\n",
      "loss: 0.478609  [ 2400/ 5124]\n",
      "loss: 0.088952  [ 2800/ 5124]\n",
      "loss: 0.186570  [ 3200/ 5124]\n",
      "loss: 0.464093  [ 3600/ 5124]\n",
      "loss: 0.344192  [ 4000/ 5124]\n",
      "loss: 1.130114  [ 4400/ 5124]\n",
      "loss: 0.258926  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4029\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.300157 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.469154  [    0/ 5124]\n",
      "loss: 0.494206  [  400/ 5124]\n",
      "loss: 0.359499  [  800/ 5124]\n",
      "loss: 0.155263  [ 1200/ 5124]\n",
      "loss: 0.656432  [ 1600/ 5124]\n",
      "loss: 0.181689  [ 2000/ 5124]\n",
      "loss: 0.847799  [ 2400/ 5124]\n",
      "loss: 0.223660  [ 2800/ 5124]\n",
      "loss: 0.263440  [ 3200/ 5124]\n",
      "loss: 0.269052  [ 3600/ 5124]\n",
      "loss: 0.229378  [ 4000/ 5124]\n",
      "loss: 0.105960  [ 4400/ 5124]\n",
      "loss: 0.115433  [ 4800/ 5124]\n",
      "Average Training Loss: 0.4304\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.318552 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model \n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Runs one full training epoch on the given model using the provided dataloader.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of training data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "        device (torch.device): The device to run computations on (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move data to the specified device\n",
    "        X, y = X.to(device), y.to(device,dtype=torch.long)\n",
    "        y=y.long()\n",
    "        X=X.float()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of test data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "        device (torch.device): The device to run computations on (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device,dtype=torch.long) # Move data to the specified device\n",
    "            X=X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    avg_loss = test_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "batch_size = 4\n",
    "# we set up batch size as 1 to avoid compurational catastrophy\n",
    "# Instantiate custom model\n",
    "my_model = MyNeuralNetwork().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.00019466370761380482)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, my_model, loss_fn, optimizer, device)\n",
    "    test_loop(test_loader, my_model, loss_fn, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f49806",
   "metadata": {},
   "source": [
    "#Initial result report\n",
    "\n",
    "In this step, I built up my MLP and tried to training the model with the hidden_layer of 768 first (because I converted the data into BERT embeddings), but in order to deal with 30 demension embeddings, I converted the Linear function input as 30. \n",
    "first 30 d embedding training hidden_size=203,dropout_rate=0.1,learning_rate=0.0001\n",
    "The result for the 100 epoch shows that :\n",
    "\n",
    "Epoch 1\n",
    "-------------------------------\n",
    "loss: 0.584721  [    0/ 5124]\n",
    "loss: 0.519337  [ 3200/ 5124]\n",
    "Average Training Loss: 0.5618\n",
    "Test Error: \n",
    " Accuracy: 74.0%, Avg loss: 0.511170 \n",
    "\n",
    "Epoch 2\n",
    "-------------------------------\n",
    "loss: 0.497146  [    0/ 5124]\n",
    "loss: 0.446626  [ 3200/ 5124]\n",
    "Average Training Loss: 0.5025\n",
    "Test Error: \n",
    " Accuracy: 74.5%, Avg loss: 0.481287 \n",
    "\n",
    "Epoch 3\n",
    "-------------------------------\n",
    "loss: 0.492960  [    0/ 5124]\n",
    "loss: 0.443114  [ 3200/ 5124]\n",
    "Average Training Loss: 0.4762\n",
    "Test Error: \n",
    " Accuracy: 75.3%, Avg loss: 0.466697 \n",
    "\n",
    "Epoch 4\n",
    "...\n",
    "Average Training Loss: 0.4308\n",
    "Test Error: \n",
    " Accuracy: 78.5%, Avg loss: 0.425507 \n",
    "\n",
    "\n",
    "Accuracy improved from 74.0% to 78.5% for 100 epochs, and loss changed from 0.51 - 0.43. \n",
    "\n",
    "\n",
    "As this is the result for 30d data, the result is different from 768 d embedding training outcome: Underthe same circumsatnce, the result for 768 bert embedding is: accuracy: 79% --> 86.4% loss: 0.472- 0.373 Which means that there are obviously more information contained in the 768 demension result, however it takes 240 minutes for training while 30d used 27 seconds. This is because less demensions will cost less in computational resources  but wil lost more information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b1be9",
   "metadata": {},
   "source": [
    "When I set up the drop out rate as 0.2, the training speed is a bit faster than 0.1 of Drop out rate. But the loss is likely to increase in some of the epoch. For Dropout rate of 0.1, and batch norm to 100, it takes 30 minutes to train 30 epoches. But with drop out rate of 0.2, the trainig times is 28mingutes. However, the improvement in accuracy is also different-- for the former one with 0.1 dropout rate, the accuracy is from 79% --> 86.4%, while for the later one with drop out rate of 0.2, the improvement is 80% --> 83.3%. The change in loss is also different-- in the dropout of 0.1, the loss change is from 0.51-0.44 while for the drop out rate of 0.2, the loss change is from 0.472- 0.373. It is significant that lower drop out rate would make loss drop higher bacause there are less information lost.  I set up early stopping with the patience of 5, but the training process still proceed to the last.\n",
    "\n",
    "Therefore, conclusion should be drawn that lower drop out rate (0.1 for example) could be the optimized solution to my MLP training because less information is likely to be droped and thus more information would be learned. On the contray, higher dropout rate (0.2) would result more information to be lost and inhibit the learning process. \n",
    "\n",
    "Then, I tried a different learning rate-- for the lr of 0.01, the learning speed is faster than the lr of 0.001. But under sthe same circumstance, higher learning rate would result less accurate result: under the same circunstance(10 epoch, dropout=10, same batch norm), lr of 0.01 would result 86.4% accuracy, while lr of 0.001 would result in 87.5% accuarcy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:16:46,019] A new study created in memory with name: no-name-96810c6a-9540-4917-8093-c41bb9691905\n",
      "C:\\Users\\likua\\AppData\\Local\\Temp\\ipykernel_34484\\3251040904.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)  # Learning rate between 1e-5 and 1e-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6610797643661499\n",
      "Epoch: 0, Loss: 0.359302818775177\n",
      "Epoch: 1, Loss: 0.3919389247894287\n",
      "Epoch: 1, Loss: 0.4813230335712433\n",
      "Epoch: 2, Loss: 0.3904144763946533\n",
      "Epoch: 2, Loss: 0.23281002044677734\n",
      "Epoch: 3, Loss: 0.3842809200286865\n",
      "Epoch: 3, Loss: 0.4154621958732605\n",
      "Epoch: 4, Loss: 0.36802324652671814\n",
      "Epoch: 4, Loss: 0.30847716331481934\n",
      "Epoch: 5, Loss: 0.589328944683075\n",
      "Epoch: 5, Loss: 0.27581489086151123\n",
      "Epoch: 6, Loss: 0.5492057800292969\n",
      "Epoch: 6, Loss: 0.4563789367675781\n",
      "Epoch: 7, Loss: 0.4560481905937195\n",
      "Epoch: 7, Loss: 0.4762313663959503\n",
      "Epoch: 8, Loss: 0.36396971344947815\n",
      "Epoch: 8, Loss: 0.37339210510253906\n",
      "Epoch: 9, Loss: 0.40023931860923767\n",
      "Epoch: 9, Loss: 0.5566477179527283\n",
      "Epoch: 10, Loss: 0.465040385723114\n",
      "Epoch: 10, Loss: 0.3177785277366638\n",
      "Epoch: 11, Loss: 0.38588738441467285\n",
      "Epoch: 11, Loss: 0.3154771625995636\n",
      "Epoch: 12, Loss: 0.43869513273239136\n",
      "Epoch: 12, Loss: 0.4519065022468567\n",
      "Epoch: 13, Loss: 0.46726712584495544\n",
      "Epoch: 13, Loss: 0.5844473838806152\n",
      "Epoch: 14, Loss: 0.4296005964279175\n",
      "Epoch: 14, Loss: 0.3689238429069519\n",
      "Epoch: 15, Loss: 0.4027900993824005\n",
      "Epoch: 15, Loss: 0.41378143429756165\n",
      "Epoch: 16, Loss: 0.4063512086868286\n",
      "Epoch: 16, Loss: 0.4619036316871643\n",
      "Epoch: 17, Loss: 0.4046646058559418\n",
      "Epoch: 17, Loss: 0.42004090547561646\n",
      "Epoch: 18, Loss: 0.42025038599967957\n",
      "Epoch: 18, Loss: 0.4973379671573639\n",
      "Epoch: 19, Loss: 0.37547025084495544\n",
      "Epoch: 19, Loss: 0.7299563884735107\n",
      "Epoch: 20, Loss: 0.3528546094894409\n",
      "Epoch: 20, Loss: 0.3848477900028229\n",
      "Epoch: 21, Loss: 0.4148179292678833\n",
      "Epoch: 21, Loss: 0.6785810589790344\n",
      "Epoch: 22, Loss: 0.3589233160018921\n",
      "Epoch: 22, Loss: 0.2929307520389557\n",
      "Epoch: 23, Loss: 0.256643146276474\n",
      "Epoch: 23, Loss: 0.3247496783733368\n",
      "Epoch: 24, Loss: 0.5073831677436829\n",
      "Epoch: 24, Loss: 0.29048505425453186\n",
      "Epoch: 25, Loss: 0.7344244122505188\n",
      "Epoch: 25, Loss: 0.5318684577941895\n",
      "Epoch: 26, Loss: 0.5039071440696716\n",
      "Epoch: 26, Loss: 0.39971640706062317\n",
      "Epoch: 27, Loss: 0.36222124099731445\n",
      "Epoch: 27, Loss: 0.47250375151634216\n",
      "Epoch: 28, Loss: 0.591758131980896\n",
      "Epoch: 28, Loss: 0.38373637199401855\n",
      "Epoch: 29, Loss: 0.5080652832984924\n",
      "Epoch: 29, Loss: 0.5227903723716736\n",
      "Epoch: 30, Loss: 0.4185022711753845\n",
      "Epoch: 30, Loss: 0.347037672996521\n",
      "Epoch: 31, Loss: 0.33682331442832947\n",
      "Epoch: 31, Loss: 0.38361069560050964\n",
      "Epoch: 32, Loss: 0.33528968691825867\n",
      "Epoch: 32, Loss: 0.4775211811065674\n",
      "Epoch: 33, Loss: 0.5280169248580933\n",
      "Epoch: 33, Loss: 0.3756541907787323\n",
      "Epoch: 34, Loss: 0.33980000019073486\n",
      "Epoch: 34, Loss: 0.36215683817863464\n",
      "Epoch: 35, Loss: 0.4075082540512085\n",
      "Epoch: 35, Loss: 0.5576645135879517\n",
      "Epoch: 36, Loss: 0.3372824490070343\n",
      "Epoch: 36, Loss: 0.4030742049217224\n",
      "Epoch: 37, Loss: 0.331613689661026\n",
      "Epoch: 37, Loss: 0.2467210292816162\n",
      "Epoch: 38, Loss: 0.48882055282592773\n",
      "Epoch: 38, Loss: 0.3719918429851532\n",
      "Epoch: 39, Loss: 0.419741690158844\n",
      "Epoch: 39, Loss: 0.3895449936389923\n",
      "Epoch: 40, Loss: 0.5489055514335632\n",
      "Epoch: 40, Loss: 0.35781779885292053\n",
      "Epoch: 41, Loss: 0.5301536917686462\n",
      "Epoch: 41, Loss: 0.3238474726676941\n",
      "Epoch: 42, Loss: 0.5106865763664246\n",
      "Epoch: 42, Loss: 0.5458664298057556\n",
      "Epoch: 43, Loss: 0.5751108527183533\n",
      "Epoch: 43, Loss: 0.5461251139640808\n",
      "Epoch: 44, Loss: 0.3914226293563843\n",
      "Epoch: 44, Loss: 0.3663862645626068\n",
      "Epoch: 45, Loss: 0.39429453015327454\n",
      "Epoch: 45, Loss: 0.37187403440475464\n",
      "Epoch: 46, Loss: 0.4386330246925354\n",
      "Epoch: 46, Loss: 0.38269472122192383\n",
      "Epoch: 47, Loss: 0.43180739879608154\n",
      "Epoch: 47, Loss: 0.39913010597229004\n",
      "Epoch: 48, Loss: 0.5988662242889404\n",
      "Epoch: 48, Loss: 0.3913980722427368\n",
      "Epoch: 49, Loss: 0.3781917691230774\n",
      "Epoch: 49, Loss: 0.6957628726959229\n",
      "Epoch: 50, Loss: 0.40079134702682495\n",
      "Epoch: 50, Loss: 0.4582323729991913\n",
      "Epoch: 51, Loss: 0.5914221405982971\n",
      "Epoch: 51, Loss: 0.3675890266895294\n",
      "Epoch: 52, Loss: 0.605871319770813\n",
      "Epoch: 52, Loss: 0.3724803328514099\n",
      "Epoch: 53, Loss: 0.3952348828315735\n",
      "Epoch: 53, Loss: 0.24330174922943115\n",
      "Epoch: 54, Loss: 0.41631975769996643\n",
      "Epoch: 54, Loss: 0.38324227929115295\n",
      "Epoch: 55, Loss: 0.2982296049594879\n",
      "Epoch: 55, Loss: 0.3856753408908844\n",
      "Epoch: 56, Loss: 0.4904119074344635\n",
      "Epoch: 56, Loss: 0.6562522649765015\n",
      "Epoch: 57, Loss: 0.3934112787246704\n",
      "Epoch: 57, Loss: 0.36221981048583984\n",
      "Epoch: 58, Loss: 0.3204387426376343\n",
      "Epoch: 58, Loss: 0.5728526711463928\n",
      "Epoch: 59, Loss: 0.33765149116516113\n",
      "Epoch: 59, Loss: 0.4935776889324188\n",
      "Epoch: 60, Loss: 0.3504938781261444\n",
      "Epoch: 60, Loss: 0.41626137495040894\n",
      "Epoch: 61, Loss: 0.36138105392456055\n",
      "Epoch: 61, Loss: 0.3928993046283722\n",
      "Epoch: 62, Loss: 0.4559234380722046\n",
      "Epoch: 62, Loss: 0.3540275990962982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:03,108] Trial 0 finished with value: 0.41486448658809577 and parameters: {'hidden_size': 276, 'dropout': 0.32243513364407383, 'lr': 0.00010612609591523706}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7873765230178833\n",
      "Epoch: 0, Loss: 0.3893071711063385\n",
      "Epoch: 1, Loss: 0.4720616340637207\n",
      "Epoch: 1, Loss: 0.4370037615299225\n",
      "Epoch: 2, Loss: 0.4287753105163574\n",
      "Epoch: 2, Loss: 0.33902209997177124\n",
      "Epoch: 3, Loss: 0.25430965423583984\n",
      "Epoch: 3, Loss: 0.3418639004230499\n",
      "Epoch: 4, Loss: 0.5559511184692383\n",
      "Epoch: 4, Loss: 0.4378761649131775\n",
      "Epoch: 5, Loss: 0.45901980996131897\n",
      "Epoch: 5, Loss: 0.4202910363674164\n",
      "Epoch: 6, Loss: 0.4501282274723053\n",
      "Epoch: 6, Loss: 0.3754114806652069\n",
      "Epoch: 7, Loss: 0.46161001920700073\n",
      "Epoch: 7, Loss: 0.4323728084564209\n",
      "Epoch: 8, Loss: 0.46950238943099976\n",
      "Epoch: 8, Loss: 0.3815102279186249\n",
      "Epoch: 9, Loss: 0.5094742178916931\n",
      "Epoch: 9, Loss: 0.37362152338027954\n",
      "Epoch: 10, Loss: 0.4727054834365845\n",
      "Epoch: 10, Loss: 0.5619612336158752\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:06,078] Trial 1 finished with value: 0.42835562489926815 and parameters: {'hidden_size': 79, 'dropout': 0.3451948693871122, 'lr': 0.008500581192678849}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6433467864990234\n",
      "Epoch: 0, Loss: 0.5729934573173523\n",
      "Epoch: 1, Loss: 0.5291832089424133\n",
      "Epoch: 1, Loss: 0.47834378480911255\n",
      "Epoch: 2, Loss: 0.3652757704257965\n",
      "Epoch: 2, Loss: 0.6150302290916443\n",
      "Epoch: 3, Loss: 0.4373672306537628\n",
      "Epoch: 3, Loss: 0.4923796057701111\n",
      "Epoch: 4, Loss: 0.2466956228017807\n",
      "Epoch: 4, Loss: 0.34649384021759033\n",
      "Epoch: 5, Loss: 0.2832242548465729\n",
      "Epoch: 5, Loss: 0.3837044835090637\n",
      "Epoch: 6, Loss: 0.40010660886764526\n",
      "Epoch: 6, Loss: 0.5498492121696472\n",
      "Epoch: 7, Loss: 0.469841867685318\n",
      "Epoch: 7, Loss: 0.37640300393104553\n",
      "Epoch: 8, Loss: 0.4417385458946228\n",
      "Epoch: 8, Loss: 0.3398676812648773\n",
      "Epoch: 9, Loss: 0.5727018117904663\n",
      "Epoch: 9, Loss: 0.47158825397491455\n",
      "Epoch: 10, Loss: 0.4600713551044464\n",
      "Epoch: 10, Loss: 0.5080996751785278\n",
      "Epoch: 11, Loss: 0.4219558537006378\n",
      "Epoch: 11, Loss: 0.4118492007255554\n",
      "Epoch: 12, Loss: 0.3231121897697449\n",
      "Epoch: 12, Loss: 0.4543934166431427\n",
      "Epoch: 13, Loss: 0.38539689779281616\n",
      "Epoch: 13, Loss: 0.26545077562332153\n",
      "Epoch: 14, Loss: 0.47870713472366333\n",
      "Epoch: 14, Loss: 0.5410369038581848\n",
      "Epoch: 15, Loss: 0.4460437297821045\n",
      "Epoch: 15, Loss: 0.3684704601764679\n",
      "Epoch: 16, Loss: 0.40332603454589844\n",
      "Epoch: 16, Loss: 0.35460662841796875\n",
      "Epoch: 17, Loss: 0.345440149307251\n",
      "Epoch: 17, Loss: 0.5218550562858582\n",
      "Epoch: 18, Loss: 0.4038452208042145\n",
      "Epoch: 18, Loss: 0.6297708749771118\n",
      "Epoch: 19, Loss: 0.5055885314941406\n",
      "Epoch: 19, Loss: 0.3725050389766693\n",
      "Epoch: 20, Loss: 0.38832589983940125\n",
      "Epoch: 20, Loss: 0.6056690812110901\n",
      "Epoch: 21, Loss: 0.3750227987766266\n",
      "Epoch: 21, Loss: 0.40474703907966614\n",
      "Epoch: 22, Loss: 0.2633814811706543\n",
      "Epoch: 22, Loss: 0.5962730646133423\n",
      "Epoch: 23, Loss: 0.4498595595359802\n",
      "Epoch: 23, Loss: 0.26030462980270386\n",
      "Epoch: 24, Loss: 0.3084101378917694\n",
      "Epoch: 24, Loss: 0.5938844680786133\n",
      "Epoch: 25, Loss: 0.40728944540023804\n",
      "Epoch: 25, Loss: 0.3883834183216095\n",
      "Epoch: 26, Loss: 0.4093945324420929\n",
      "Epoch: 26, Loss: 0.4425647556781769\n",
      "Epoch: 27, Loss: 0.36043524742126465\n",
      "Epoch: 27, Loss: 0.36629873514175415\n",
      "Epoch: 28, Loss: 0.27649396657943726\n",
      "Epoch: 28, Loss: 0.31989461183547974\n",
      "Epoch: 29, Loss: 0.4764128029346466\n",
      "Epoch: 29, Loss: 0.43517446517944336\n",
      "Epoch: 30, Loss: 0.44949647784233093\n",
      "Epoch: 30, Loss: 0.47709769010543823\n",
      "Epoch: 31, Loss: 0.3200552463531494\n",
      "Epoch: 31, Loss: 0.5554361939430237\n",
      "Epoch: 32, Loss: 0.41763997077941895\n",
      "Epoch: 32, Loss: 0.3499877452850342\n",
      "Epoch: 33, Loss: 0.4437277913093567\n",
      "Epoch: 33, Loss: 0.4365541934967041\n",
      "Epoch: 34, Loss: 0.3151046931743622\n",
      "Epoch: 34, Loss: 0.41693904995918274\n",
      "Epoch: 35, Loss: 0.4348469376564026\n",
      "Epoch: 35, Loss: 0.3530895411968231\n",
      "Epoch: 36, Loss: 0.6893242001533508\n",
      "Epoch: 36, Loss: 0.33094531297683716\n",
      "Epoch: 37, Loss: 0.34876012802124023\n",
      "Epoch: 37, Loss: 0.3836291432380676\n",
      "Epoch: 38, Loss: 0.32407626509666443\n",
      "Epoch: 38, Loss: 0.49373728036880493\n",
      "Epoch: 39, Loss: 0.8236182928085327\n",
      "Epoch: 39, Loss: 0.44184979796409607\n",
      "Epoch: 40, Loss: 0.36687251925468445\n",
      "Epoch: 40, Loss: 0.3296221196651459\n",
      "Epoch: 41, Loss: 0.315312922000885\n",
      "Epoch: 41, Loss: 0.47263047099113464\n",
      "Epoch: 42, Loss: 0.4490480422973633\n",
      "Epoch: 42, Loss: 0.36974865198135376\n",
      "Epoch: 43, Loss: 0.4992961287498474\n",
      "Epoch: 43, Loss: 0.36564916372299194\n",
      "Epoch: 44, Loss: 0.41621407866477966\n",
      "Epoch: 44, Loss: 0.3779107332229614\n",
      "Epoch: 45, Loss: 0.36195623874664307\n",
      "Epoch: 45, Loss: 0.3503836989402771\n",
      "Epoch: 46, Loss: 0.37724098563194275\n",
      "Epoch: 46, Loss: 0.4142431318759918\n",
      "Epoch: 47, Loss: 0.28782907128334045\n",
      "Epoch: 47, Loss: 0.4552106261253357\n",
      "Epoch: 48, Loss: 0.4994857907295227\n",
      "Epoch: 48, Loss: 0.46427321434020996\n",
      "Epoch: 49, Loss: 0.5399309992790222\n",
      "Epoch: 49, Loss: 0.33512163162231445\n",
      "Epoch: 50, Loss: 0.3506902754306793\n",
      "Epoch: 50, Loss: 0.5244365930557251\n",
      "Epoch: 51, Loss: 0.4296547770500183\n",
      "Epoch: 51, Loss: 0.4581063687801361\n",
      "Epoch: 52, Loss: 0.4841635525226593\n",
      "Epoch: 52, Loss: 0.37529075145721436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:20,141] Trial 2 finished with value: 0.42247116812128827 and parameters: {'hidden_size': 216, 'dropout': 0.3031781303938849, 'lr': 2.8907368681019724e-05}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6483162045478821\n",
      "Epoch: 0, Loss: 0.5228817462921143\n",
      "Epoch: 1, Loss: 0.5839260816574097\n",
      "Epoch: 1, Loss: 0.4297340214252472\n",
      "Epoch: 2, Loss: 0.41195178031921387\n",
      "Epoch: 2, Loss: 0.3473753035068512\n",
      "Epoch: 3, Loss: 0.4846174120903015\n",
      "Epoch: 3, Loss: 0.4251614511013031\n",
      "Epoch: 4, Loss: 0.36572766304016113\n",
      "Epoch: 4, Loss: 0.4534202218055725\n",
      "Epoch: 5, Loss: 0.29655036330223083\n",
      "Epoch: 5, Loss: 0.4139351546764374\n",
      "Epoch: 6, Loss: 0.4279824197292328\n",
      "Epoch: 6, Loss: 0.4848511815071106\n",
      "Epoch: 7, Loss: 0.4137151539325714\n",
      "Epoch: 7, Loss: 0.4215328097343445\n",
      "Epoch: 8, Loss: 0.5295987129211426\n",
      "Epoch: 8, Loss: 0.3201828598976135\n",
      "Epoch: 9, Loss: 0.4394865036010742\n",
      "Epoch: 9, Loss: 0.5376125574111938\n",
      "Epoch: 10, Loss: 0.41306471824645996\n",
      "Epoch: 10, Loss: 0.4973316192626953\n",
      "Epoch: 11, Loss: 0.38854366540908813\n",
      "Epoch: 11, Loss: 0.3204967677593231\n",
      "Epoch: 12, Loss: 0.4624144434928894\n",
      "Epoch: 12, Loss: 0.38626012206077576\n",
      "Epoch: 13, Loss: 0.416127473115921\n",
      "Epoch: 13, Loss: 0.40228271484375\n",
      "Epoch: 14, Loss: 0.3499399423599243\n",
      "Epoch: 14, Loss: 0.5041641592979431\n",
      "Epoch: 15, Loss: 0.4930947422981262\n",
      "Epoch: 15, Loss: 0.5207732319831848\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:24,454] Trial 3 finished with value: 0.42912354520435286 and parameters: {'hidden_size': 244, 'dropout': 0.02722354189216162, 'lr': 0.0032767732254658736}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7046705484390259\n",
      "Epoch: 0, Loss: 0.38033294677734375\n",
      "Epoch: 1, Loss: 0.3254362642765045\n",
      "Epoch: 1, Loss: 0.5360750555992126\n",
      "Epoch: 2, Loss: 0.36039987206459045\n",
      "Epoch: 2, Loss: 0.4685310125350952\n",
      "Epoch: 3, Loss: 0.327156662940979\n",
      "Epoch: 3, Loss: 0.42355847358703613\n",
      "Epoch: 4, Loss: 0.5116345286369324\n",
      "Epoch: 4, Loss: 0.3695792257785797\n",
      "Epoch: 5, Loss: 0.4358178377151489\n",
      "Epoch: 5, Loss: 0.5143343210220337\n",
      "Epoch: 6, Loss: 0.4505024254322052\n",
      "Epoch: 6, Loss: 0.5024691820144653\n",
      "Epoch: 7, Loss: 0.3116553723812103\n",
      "Epoch: 7, Loss: 0.4786997437477112\n",
      "Epoch: 8, Loss: 0.4937686026096344\n",
      "Epoch: 8, Loss: 0.4049917757511139\n",
      "Epoch: 9, Loss: 0.29544341564178467\n",
      "Epoch: 9, Loss: 0.5307707786560059\n",
      "Epoch: 10, Loss: 0.2894406020641327\n",
      "Epoch: 10, Loss: 0.43763595819473267\n",
      "Epoch: 11, Loss: 0.3721563518047333\n",
      "Epoch: 11, Loss: 0.4146885275840759\n",
      "Epoch: 12, Loss: 0.37271687388420105\n",
      "Epoch: 12, Loss: 0.482521653175354\n",
      "Epoch: 13, Loss: 0.37661999464035034\n",
      "Epoch: 13, Loss: 0.34285619854927063\n",
      "Epoch: 14, Loss: 0.49419477581977844\n",
      "Epoch: 14, Loss: 0.4591246545314789\n",
      "Epoch: 15, Loss: 0.43448948860168457\n",
      "Epoch: 15, Loss: 0.5244237780570984\n",
      "Epoch: 16, Loss: 0.4319112300872803\n",
      "Epoch: 16, Loss: 0.4457044005393982\n",
      "Epoch: 17, Loss: 0.31517812609672546\n",
      "Epoch: 17, Loss: 0.4273412525653839\n",
      "Epoch: 18, Loss: 0.3770211935043335\n",
      "Epoch: 18, Loss: 0.31501859426498413\n",
      "Epoch: 19, Loss: 0.3294094502925873\n",
      "Epoch: 19, Loss: 0.3678819239139557\n",
      "Epoch: 20, Loss: 0.24242451786994934\n",
      "Epoch: 20, Loss: 0.5651082992553711\n",
      "Epoch: 21, Loss: 0.4074181318283081\n",
      "Epoch: 21, Loss: 0.4185181260108948\n",
      "Epoch: 22, Loss: 0.4857514500617981\n",
      "Epoch: 22, Loss: 0.5324949026107788\n",
      "Epoch: 23, Loss: 0.3589445948600769\n",
      "Epoch: 23, Loss: 0.462674617767334\n",
      "Epoch: 24, Loss: 0.3311837911605835\n",
      "Epoch: 24, Loss: 0.40707385540008545\n",
      "Epoch: 25, Loss: 0.25017234683036804\n",
      "Epoch: 25, Loss: 0.400027871131897\n",
      "Epoch: 26, Loss: 0.48063498735427856\n",
      "Epoch: 26, Loss: 0.4117285907268524\n",
      "Epoch: 27, Loss: 0.412891149520874\n",
      "Epoch: 27, Loss: 0.3781167268753052\n",
      "Epoch: 28, Loss: 0.5792290568351746\n",
      "Epoch: 28, Loss: 0.45244136452674866\n",
      "Epoch: 29, Loss: 0.3375491201877594\n",
      "Epoch: 29, Loss: 0.369869202375412\n",
      "Epoch: 30, Loss: 0.4349829852581024\n",
      "Epoch: 30, Loss: 0.41901054978370667\n",
      "Epoch: 31, Loss: 0.45858296751976013\n",
      "Epoch: 31, Loss: 0.5573241114616394\n",
      "Epoch: 32, Loss: 0.3354308009147644\n",
      "Epoch: 32, Loss: 0.45176559686660767\n",
      "Epoch: 33, Loss: 0.32276487350463867\n",
      "Epoch: 33, Loss: 0.37422072887420654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:33,477] Trial 4 finished with value: 0.4185571836261368 and parameters: {'hidden_size': 280, 'dropout': 0.23753126236814082, 'lr': 0.0002065541188907276}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.9639874696731567\n",
      "Epoch: 0, Loss: 0.6848726272583008\n",
      "Epoch: 1, Loss: 0.4060063660144806\n",
      "Epoch: 1, Loss: 0.4940294027328491\n",
      "Epoch: 2, Loss: 0.3850947916507721\n",
      "Epoch: 2, Loss: 0.47048091888427734\n",
      "Epoch: 3, Loss: 0.3995610177516937\n",
      "Epoch: 3, Loss: 0.5083147287368774\n",
      "Epoch: 4, Loss: 0.3476550281047821\n",
      "Epoch: 4, Loss: 0.4739144444465637\n",
      "Epoch: 5, Loss: 0.4010412395000458\n",
      "Epoch: 5, Loss: 0.35744374990463257\n",
      "Epoch: 6, Loss: 0.33869868516921997\n",
      "Epoch: 6, Loss: 0.3735336661338806\n",
      "Epoch: 7, Loss: 0.3155152499675751\n",
      "Epoch: 7, Loss: 0.5070491433143616\n",
      "Epoch: 8, Loss: 0.5141992568969727\n",
      "Epoch: 8, Loss: 0.3800053894519806\n",
      "Epoch: 9, Loss: 0.41521382331848145\n",
      "Epoch: 9, Loss: 0.5556289553642273\n",
      "Epoch: 10, Loss: 0.5769093632698059\n",
      "Epoch: 10, Loss: 0.581944465637207\n",
      "Epoch: 11, Loss: 0.43508225679397583\n",
      "Epoch: 11, Loss: 0.4869530498981476\n",
      "Epoch: 12, Loss: 0.5863324999809265\n",
      "Epoch: 12, Loss: 0.2743832468986511\n",
      "Epoch: 13, Loss: 0.49231693148612976\n",
      "Epoch: 13, Loss: 0.4339175820350647\n",
      "Epoch: 14, Loss: 0.40546029806137085\n",
      "Epoch: 14, Loss: 0.41371044516563416\n",
      "Epoch: 15, Loss: 0.3792038559913635\n",
      "Epoch: 15, Loss: 0.3474845588207245\n",
      "Epoch: 16, Loss: 0.48834243416786194\n",
      "Epoch: 16, Loss: 0.38617050647735596\n",
      "Epoch: 17, Loss: 0.4831208288669586\n",
      "Epoch: 17, Loss: 0.40938761830329895\n",
      "Epoch: 18, Loss: 0.4217859208583832\n",
      "Epoch: 18, Loss: 0.3384508192539215\n",
      "Epoch: 19, Loss: 0.4756108820438385\n",
      "Epoch: 19, Loss: 0.3039008378982544\n",
      "Epoch: 20, Loss: 0.3744569718837738\n",
      "Epoch: 20, Loss: 0.4167555868625641\n",
      "Epoch: 21, Loss: 0.35203948616981506\n",
      "Epoch: 21, Loss: 0.34025269746780396\n",
      "Epoch: 22, Loss: 0.30835893750190735\n",
      "Epoch: 22, Loss: 0.4476298987865448\n",
      "Epoch: 23, Loss: 0.5387942790985107\n",
      "Epoch: 23, Loss: 0.38673388957977295\n",
      "Epoch: 24, Loss: 0.40856340527534485\n",
      "Epoch: 24, Loss: 0.40108609199523926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:40,192] Trial 5 finished with value: 0.4226763029322491 and parameters: {'hidden_size': 273, 'dropout': 0.3894075709482305, 'lr': 0.00035976090834079764}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.8899600505828857\n",
      "Epoch: 0, Loss: 0.48138144612312317\n",
      "Epoch: 1, Loss: 0.4835922420024872\n",
      "Epoch: 1, Loss: 0.43936553597450256\n",
      "Epoch: 2, Loss: 0.6205148100852966\n",
      "Epoch: 2, Loss: 0.39827850461006165\n",
      "Epoch: 3, Loss: 0.3913358449935913\n",
      "Epoch: 3, Loss: 0.4341740310192108\n",
      "Epoch: 4, Loss: 0.25634512305259705\n",
      "Epoch: 4, Loss: 0.4734777510166168\n",
      "Epoch: 5, Loss: 0.47679388523101807\n",
      "Epoch: 5, Loss: 0.6991392970085144\n",
      "Epoch: 6, Loss: 0.43504783511161804\n",
      "Epoch: 6, Loss: 0.3362903892993927\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:42,143] Trial 6 finished with value: 0.4288132814285548 and parameters: {'hidden_size': 232, 'dropout': 0.08963430979297632, 'lr': 0.0020660001496150397}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7919474840164185\n",
      "Epoch: 0, Loss: 0.45493096113204956\n",
      "Epoch: 1, Loss: 0.4282335937023163\n",
      "Epoch: 1, Loss: 0.5750783681869507\n",
      "Epoch: 2, Loss: 0.4590735137462616\n",
      "Epoch: 2, Loss: 0.4501896798610687\n",
      "Epoch: 3, Loss: 0.39051002264022827\n",
      "Epoch: 3, Loss: 0.4097910225391388\n",
      "Epoch: 4, Loss: 0.3000686764717102\n",
      "Epoch: 4, Loss: 0.37978073954582214\n",
      "Epoch: 5, Loss: 0.41562727093696594\n",
      "Epoch: 5, Loss: 0.3415859341621399\n",
      "Epoch: 6, Loss: 0.41267332434654236\n",
      "Epoch: 6, Loss: 0.35324394702911377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:17:44,096] Trial 7 finished with value: 0.43175713639266744 and parameters: {'hidden_size': 127, 'dropout': 0.07619745997492564, 'lr': 0.0024839339929338114}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7771283984184265\n",
      "Epoch: 0, Loss: 0.7320212721824646\n",
      "Epoch: 1, Loss: 0.6200347542762756\n",
      "Epoch: 1, Loss: 0.5355891585350037\n",
      "Epoch: 2, Loss: 0.5786080956459045\n",
      "Epoch: 2, Loss: 0.5199492573738098\n",
      "Epoch: 3, Loss: 0.35497209429740906\n",
      "Epoch: 3, Loss: 0.32549887895584106\n",
      "Epoch: 4, Loss: 0.2767884433269501\n",
      "Epoch: 4, Loss: 0.39492568373680115\n",
      "Epoch: 5, Loss: 0.5188320875167847\n",
      "Epoch: 5, Loss: 0.4147818684577942\n",
      "Epoch: 6, Loss: 0.45281505584716797\n",
      "Epoch: 6, Loss: 0.4035623371601105\n",
      "Epoch: 7, Loss: 0.36716145277023315\n",
      "Epoch: 7, Loss: 0.5523009896278381\n",
      "Epoch: 8, Loss: 0.3125231862068176\n",
      "Epoch: 8, Loss: 0.37042516469955444\n",
      "Epoch: 9, Loss: 0.47180959582328796\n",
      "Epoch: 9, Loss: 0.32543566823005676\n",
      "Epoch: 10, Loss: 0.39589521288871765\n",
      "Epoch: 10, Loss: 0.45778602361679077\n",
      "Epoch: 11, Loss: 0.4043230414390564\n",
      "Epoch: 11, Loss: 0.5270802974700928\n",
      "Epoch: 12, Loss: 0.44816604256629944\n",
      "Epoch: 12, Loss: 0.43912002444267273\n",
      "Epoch: 13, Loss: 0.5107749104499817\n",
      "Epoch: 13, Loss: 0.2972259223461151\n",
      "Epoch: 14, Loss: 0.3520672023296356\n",
      "Epoch: 14, Loss: 0.5239061713218689\n",
      "Epoch: 15, Loss: 0.5573668479919434\n",
      "Epoch: 15, Loss: 0.4564859867095947\n",
      "Epoch: 16, Loss: 0.29644665122032166\n",
      "Epoch: 16, Loss: 0.31224825978279114\n",
      "Epoch: 17, Loss: 0.224604994058609\n",
      "Epoch: 17, Loss: 0.4312353730201721\n",
      "Epoch: 18, Loss: 0.36940494179725647\n",
      "Epoch: 18, Loss: 0.42708808183670044\n",
      "Epoch: 19, Loss: 0.3407667279243469\n",
      "Epoch: 19, Loss: 0.5233078598976135\n",
      "Epoch: 20, Loss: 0.504389226436615\n",
      "Epoch: 20, Loss: 0.32458555698394775\n",
      "Epoch: 21, Loss: 0.3497123718261719\n",
      "Epoch: 21, Loss: 0.4562409818172455\n",
      "Epoch: 22, Loss: 0.3054782450199127\n",
      "Epoch: 22, Loss: 0.41490912437438965\n",
      "Epoch: 23, Loss: 0.35114800930023193\n",
      "Epoch: 23, Loss: 0.47380125522613525\n",
      "Epoch: 24, Loss: 0.48018765449523926\n",
      "Epoch: 24, Loss: 0.31048890948295593\n",
      "Epoch: 25, Loss: 0.516533374786377\n",
      "Epoch: 25, Loss: 0.3821735084056854\n",
      "Epoch: 26, Loss: 0.47252362966537476\n",
      "Epoch: 26, Loss: 0.3508685529232025\n",
      "Epoch: 27, Loss: 0.44239750504493713\n",
      "Epoch: 27, Loss: 0.6289108395576477\n",
      "Epoch: 28, Loss: 0.4670636057853699\n",
      "Epoch: 28, Loss: 0.51054447889328\n",
      "Epoch: 29, Loss: 0.35793042182922363\n",
      "Epoch: 29, Loss: 0.49330177903175354\n",
      "Epoch: 30, Loss: 0.35060620307922363\n",
      "Epoch: 30, Loss: 0.4151764512062073\n",
      "Epoch: 31, Loss: 0.31199783086776733\n",
      "Epoch: 31, Loss: 0.2768954634666443\n",
      "Epoch: 32, Loss: 0.6038862466812134\n",
      "Epoch: 32, Loss: 0.4052329957485199\n",
      "Epoch: 33, Loss: 0.4476839303970337\n",
      "Epoch: 33, Loss: 0.5764711499214172\n",
      "Epoch: 34, Loss: 0.3464422821998596\n",
      "Epoch: 34, Loss: 0.3298870325088501\n",
      "Epoch: 35, Loss: 0.26001405715942383\n",
      "Epoch: 35, Loss: 0.5438904762268066\n",
      "Epoch: 36, Loss: 0.47630879282951355\n",
      "Epoch: 36, Loss: 0.42046108841896057\n",
      "Epoch: 37, Loss: 0.4595228433609009\n",
      "Epoch: 37, Loss: 0.40004366636276245\n",
      "Epoch: 38, Loss: 0.27125075459480286\n",
      "Epoch: 38, Loss: 0.30262866616249084\n",
      "Epoch: 39, Loss: 0.40778234601020813\n",
      "Epoch: 39, Loss: 0.40807777643203735\n",
      "Epoch: 40, Loss: 0.4647388160228729\n",
      "Epoch: 40, Loss: 0.5762066841125488\n",
      "Epoch: 41, Loss: 0.46832478046417236\n",
      "Epoch: 41, Loss: 0.32318490743637085\n",
      "Epoch: 42, Loss: 0.2525123357772827\n",
      "Epoch: 42, Loss: 0.42467036843299866\n",
      "Epoch: 43, Loss: 0.2737096846103668\n",
      "Epoch: 43, Loss: 0.5351171493530273\n",
      "Epoch: 44, Loss: 0.5061221122741699\n",
      "Epoch: 44, Loss: 0.4237159788608551\n",
      "Epoch: 45, Loss: 0.3604554235935211\n",
      "Epoch: 45, Loss: 0.44144847989082336\n",
      "Epoch: 46, Loss: 0.6300848722457886\n",
      "Epoch: 46, Loss: 0.3855152428150177\n",
      "Epoch: 47, Loss: 0.4034791588783264\n",
      "Epoch: 47, Loss: 0.3196895122528076\n",
      "Epoch: 48, Loss: 0.5730432271957397\n",
      "Epoch: 48, Loss: 0.48346608877182007\n",
      "Epoch: 49, Loss: 0.29099369049072266\n",
      "Epoch: 49, Loss: 0.5075992345809937\n",
      "Epoch: 50, Loss: 0.42920729517936707\n",
      "Epoch: 50, Loss: 0.42620307207107544\n",
      "Epoch: 51, Loss: 0.5917603373527527\n",
      "Epoch: 51, Loss: 0.39933210611343384\n",
      "Epoch: 52, Loss: 0.3313719630241394\n",
      "Epoch: 52, Loss: 0.5908765196800232\n",
      "Epoch: 53, Loss: 0.33295297622680664\n",
      "Epoch: 53, Loss: 0.7913192510604858\n",
      "Epoch: 54, Loss: 0.3414742648601532\n",
      "Epoch: 54, Loss: 0.40975937247276306\n",
      "Epoch: 55, Loss: 0.3785877823829651\n",
      "Epoch: 55, Loss: 0.3860008120536804\n",
      "Epoch: 56, Loss: 0.4511817991733551\n",
      "Epoch: 56, Loss: 0.3490545153617859\n",
      "Epoch: 57, Loss: 0.4190504550933838\n",
      "Epoch: 57, Loss: 0.2944110035896301\n",
      "Epoch: 58, Loss: 0.40125179290771484\n",
      "Epoch: 58, Loss: 0.3925406336784363\n",
      "Epoch: 59, Loss: 0.44213971495628357\n",
      "Epoch: 59, Loss: 0.45580461621284485\n",
      "Epoch: 60, Loss: 0.5076589584350586\n",
      "Epoch: 60, Loss: 0.22787483036518097\n",
      "Epoch: 61, Loss: 0.33159470558166504\n",
      "Epoch: 61, Loss: 0.40292245149612427\n",
      "Epoch: 62, Loss: 0.33314579725265503\n",
      "Epoch: 62, Loss: 0.2820799946784973\n",
      "Epoch: 63, Loss: 0.3678704798221588\n",
      "Epoch: 63, Loss: 0.41697466373443604\n",
      "Epoch: 64, Loss: 0.40394529700279236\n",
      "Epoch: 64, Loss: 0.29820704460144043\n",
      "Epoch: 65, Loss: 0.3302782475948334\n",
      "Epoch: 65, Loss: 0.6093022227287292\n",
      "Epoch: 66, Loss: 0.40006837248802185\n",
      "Epoch: 66, Loss: 0.40777650475502014\n",
      "Epoch: 67, Loss: 0.33623161911964417\n",
      "Epoch: 67, Loss: 0.3882855772972107\n",
      "Epoch: 68, Loss: 0.4382258951663971\n",
      "Epoch: 68, Loss: 0.3562653064727783\n",
      "Epoch: 69, Loss: 0.2666785717010498\n",
      "Epoch: 69, Loss: 0.40772390365600586\n",
      "Epoch: 70, Loss: 0.45443984866142273\n",
      "Epoch: 70, Loss: 0.24933233857154846\n",
      "Epoch: 71, Loss: 0.4569628834724426\n",
      "Epoch: 71, Loss: 0.49492236971855164\n",
      "Epoch: 72, Loss: 0.3731028735637665\n",
      "Epoch: 72, Loss: 0.37963706254959106\n",
      "Epoch: 73, Loss: 0.41330891847610474\n",
      "Epoch: 73, Loss: 0.6366141438484192\n",
      "Epoch: 74, Loss: 0.4196431636810303\n",
      "Epoch: 74, Loss: 0.5046044588088989\n",
      "Epoch: 75, Loss: 0.3247661590576172\n",
      "Epoch: 75, Loss: 0.547900378704071\n",
      "Epoch: 76, Loss: 0.454371839761734\n",
      "Epoch: 76, Loss: 0.2659018933773041\n",
      "Epoch: 77, Loss: 0.44171732664108276\n",
      "Epoch: 77, Loss: 0.44578585028648376\n",
      "Epoch: 78, Loss: 0.5135392546653748\n",
      "Epoch: 78, Loss: 0.35282161831855774\n",
      "Epoch: 79, Loss: 0.4741140305995941\n",
      "Epoch: 79, Loss: 0.38136276602745056\n",
      "Epoch: 80, Loss: 0.5086721777915955\n",
      "Epoch: 80, Loss: 0.2559022903442383\n",
      "Epoch: 81, Loss: 0.31471937894821167\n",
      "Epoch: 81, Loss: 0.36493560671806335\n",
      "Epoch: 82, Loss: 0.4335158169269562\n",
      "Epoch: 82, Loss: 0.4315302073955536\n",
      "Epoch: 83, Loss: 0.38134023547172546\n",
      "Epoch: 83, Loss: 0.4735870659351349\n",
      "Epoch: 84, Loss: 0.3149639368057251\n",
      "Epoch: 84, Loss: 0.31147679686546326\n",
      "Epoch: 85, Loss: 0.4891839325428009\n",
      "Epoch: 85, Loss: 0.5488744378089905\n",
      "Epoch: 86, Loss: 0.5307037234306335\n",
      "Epoch: 86, Loss: 0.3528529405593872\n",
      "Epoch: 87, Loss: 0.633038341999054\n",
      "Epoch: 87, Loss: 0.3028683662414551\n",
      "Epoch: 88, Loss: 0.45901691913604736\n",
      "Epoch: 88, Loss: 0.4625813663005829\n",
      "Epoch: 89, Loss: 0.3530025780200958\n",
      "Epoch: 89, Loss: 0.3059012293815613\n",
      "Epoch: 90, Loss: 0.343822717666626\n",
      "Epoch: 90, Loss: 0.5406094789505005\n",
      "Epoch: 91, Loss: 0.36876100301742554\n",
      "Epoch: 91, Loss: 0.32001596689224243\n",
      "Epoch: 92, Loss: 0.2972646951675415\n",
      "Epoch: 92, Loss: 0.5132530927658081\n",
      "Epoch: 93, Loss: 0.4223555028438568\n",
      "Epoch: 93, Loss: 0.3819392919540405\n",
      "Epoch: 94, Loss: 0.4509887099266052\n",
      "Epoch: 94, Loss: 0.5971178412437439\n",
      "Epoch: 95, Loss: 0.45663687586784363\n",
      "Epoch: 95, Loss: 0.34819984436035156\n",
      "Epoch: 96, Loss: 0.5179447531700134\n",
      "Epoch: 96, Loss: 0.4842342734336853\n",
      "Epoch: 97, Loss: 0.5061273574829102\n",
      "Epoch: 97, Loss: 0.5541219115257263\n",
      "Epoch: 98, Loss: 0.2868037819862366\n",
      "Epoch: 98, Loss: 0.45734965801239014\n",
      "Epoch: 99, Loss: 0.4889124035835266\n",
      "Epoch: 99, Loss: 0.49900147318840027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:10,730] Trial 8 finished with value: 0.4225041931668609 and parameters: {'hidden_size': 225, 'dropout': 0.21401860080089574, 'lr': 1.0760417239776477e-05}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5365692973136902\n",
      "Epoch: 0, Loss: 0.4873289167881012\n",
      "Epoch: 1, Loss: 0.40874215960502625\n",
      "Epoch: 1, Loss: 0.3081992566585541\n",
      "Epoch: 2, Loss: 0.4716368019580841\n",
      "Epoch: 2, Loss: 0.20308806002140045\n",
      "Epoch: 3, Loss: 0.42617639899253845\n",
      "Epoch: 3, Loss: 0.38605886697769165\n",
      "Epoch: 4, Loss: 0.3586467504501343\n",
      "Epoch: 4, Loss: 0.39913827180862427\n",
      "Epoch: 5, Loss: 0.39245977997779846\n",
      "Epoch: 5, Loss: 0.5048075914382935\n",
      "Epoch: 6, Loss: 0.3786649703979492\n",
      "Epoch: 6, Loss: 0.4066137373447418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:12,682] Trial 9 finished with value: 0.4277834020126958 and parameters: {'hidden_size': 166, 'dropout': 0.2277040599028521, 'lr': 0.0027199565531543695}. Best is trial 0 with value: 0.41486448658809577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7395080924034119\n",
      "Epoch: 0, Loss: 0.5559110641479492\n",
      "Epoch: 1, Loss: 0.6205818057060242\n",
      "Epoch: 1, Loss: 0.5356339812278748\n",
      "Epoch: 2, Loss: 0.5461162328720093\n",
      "Epoch: 2, Loss: 0.5175850987434387\n",
      "Epoch: 3, Loss: 0.46954095363616943\n",
      "Epoch: 3, Loss: 0.42248237133026123\n",
      "Epoch: 4, Loss: 0.37047305703163147\n",
      "Epoch: 4, Loss: 0.40590766072273254\n",
      "Epoch: 5, Loss: 0.3037531077861786\n",
      "Epoch: 5, Loss: 0.32385462522506714\n",
      "Epoch: 6, Loss: 0.2861565053462982\n",
      "Epoch: 6, Loss: 0.439493328332901\n",
      "Epoch: 7, Loss: 0.44626203179359436\n",
      "Epoch: 7, Loss: 0.4393170475959778\n",
      "Epoch: 8, Loss: 0.38163626194000244\n",
      "Epoch: 8, Loss: 0.28124281764030457\n",
      "Epoch: 9, Loss: 0.3638584017753601\n",
      "Epoch: 9, Loss: 0.3609023690223694\n",
      "Epoch: 10, Loss: 0.3651049733161926\n",
      "Epoch: 10, Loss: 0.32449933886528015\n",
      "Epoch: 11, Loss: 0.31639713048934937\n",
      "Epoch: 11, Loss: 0.4749342203140259\n",
      "Epoch: 12, Loss: 0.42023515701293945\n",
      "Epoch: 12, Loss: 0.3410148620605469\n",
      "Epoch: 13, Loss: 0.3069429099559784\n",
      "Epoch: 13, Loss: 0.385041207075119\n",
      "Epoch: 14, Loss: 0.44743406772613525\n",
      "Epoch: 14, Loss: 0.48828747868537903\n",
      "Epoch: 15, Loss: 0.3713892996311188\n",
      "Epoch: 15, Loss: 0.4111163914203644\n",
      "Epoch: 16, Loss: 0.5638043284416199\n",
      "Epoch: 16, Loss: 0.432378351688385\n",
      "Epoch: 17, Loss: 0.24847841262817383\n",
      "Epoch: 17, Loss: 0.24997571110725403\n",
      "Epoch: 18, Loss: 0.4550211727619171\n",
      "Epoch: 18, Loss: 0.4169551134109497\n",
      "Epoch: 19, Loss: 0.48998287320137024\n",
      "Epoch: 19, Loss: 0.31479474902153015\n",
      "Epoch: 20, Loss: 0.47083285450935364\n",
      "Epoch: 20, Loss: 0.27529409527778625\n",
      "Epoch: 21, Loss: 0.419209361076355\n",
      "Epoch: 21, Loss: 0.48950207233428955\n",
      "Epoch: 22, Loss: 0.36502790451049805\n",
      "Epoch: 22, Loss: 0.5363242030143738\n",
      "Epoch: 23, Loss: 0.38797929883003235\n",
      "Epoch: 23, Loss: 0.3945466876029968\n",
      "Epoch: 24, Loss: 0.46548664569854736\n",
      "Epoch: 24, Loss: 0.5035542249679565\n",
      "Epoch: 25, Loss: 0.44592615962028503\n",
      "Epoch: 25, Loss: 0.2697071135044098\n",
      "Epoch: 26, Loss: 0.42900556325912476\n",
      "Epoch: 26, Loss: 0.2509646415710449\n",
      "Epoch: 27, Loss: 0.3385329246520996\n",
      "Epoch: 27, Loss: 0.4571089744567871\n",
      "Epoch: 28, Loss: 0.510997474193573\n",
      "Epoch: 28, Loss: 0.383750319480896\n",
      "Epoch: 29, Loss: 0.45725876092910767\n",
      "Epoch: 29, Loss: 0.39370864629745483\n",
      "Epoch: 30, Loss: 0.4124097526073456\n",
      "Epoch: 30, Loss: 0.4436799883842468\n",
      "Epoch: 31, Loss: 0.35336634516716003\n",
      "Epoch: 31, Loss: 0.45974740386009216\n",
      "Epoch: 32, Loss: 0.4625760018825531\n",
      "Epoch: 32, Loss: 0.3297295868396759\n",
      "Epoch: 33, Loss: 0.494546502828598\n",
      "Epoch: 33, Loss: 0.38420143723487854\n",
      "Epoch: 34, Loss: 0.4272976815700531\n",
      "Epoch: 34, Loss: 0.3372396230697632\n",
      "Epoch: 35, Loss: 0.4864649176597595\n",
      "Epoch: 35, Loss: 0.5034159421920776\n",
      "Epoch: 36, Loss: 0.517710268497467\n",
      "Epoch: 36, Loss: 0.3032495081424713\n",
      "Epoch: 37, Loss: 0.516035258769989\n",
      "Epoch: 37, Loss: 0.46690112352371216\n",
      "Epoch: 38, Loss: 0.344644159078598\n",
      "Epoch: 38, Loss: 0.5795426368713379\n",
      "Epoch: 39, Loss: 0.41150224208831787\n",
      "Epoch: 39, Loss: 0.3910847306251526\n",
      "Epoch: 40, Loss: 0.4528384208679199\n",
      "Epoch: 40, Loss: 0.39860087633132935\n",
      "Epoch: 41, Loss: 0.38150545954704285\n",
      "Epoch: 41, Loss: 0.39335504174232483\n",
      "Epoch: 42, Loss: 0.38765108585357666\n",
      "Epoch: 42, Loss: 0.4332660734653473\n",
      "Epoch: 43, Loss: 0.42760753631591797\n",
      "Epoch: 43, Loss: 0.4835921823978424\n",
      "Epoch: 44, Loss: 0.316389262676239\n",
      "Epoch: 44, Loss: 0.4173375964164734\n",
      "Epoch: 45, Loss: 0.33491581678390503\n",
      "Epoch: 45, Loss: 0.39976081252098083\n",
      "Epoch: 46, Loss: 0.26952847838401794\n",
      "Epoch: 46, Loss: 0.33221039175987244\n",
      "Epoch: 47, Loss: 0.3079011142253876\n",
      "Epoch: 47, Loss: 0.36977246403694153\n",
      "Epoch: 48, Loss: 0.43493595719337463\n",
      "Epoch: 48, Loss: 0.5636343359947205\n",
      "Epoch: 49, Loss: 0.37566208839416504\n",
      "Epoch: 49, Loss: 0.36499932408332825\n",
      "Epoch: 50, Loss: 0.31083816289901733\n",
      "Epoch: 50, Loss: 0.3204288184642792\n",
      "Epoch: 51, Loss: 0.2898584306240082\n",
      "Epoch: 51, Loss: 0.44065427780151367\n",
      "Epoch: 52, Loss: 0.39328837394714355\n",
      "Epoch: 52, Loss: 0.40087994933128357\n",
      "Epoch: 53, Loss: 0.414831280708313\n",
      "Epoch: 53, Loss: 0.4992007613182068\n",
      "Epoch: 54, Loss: 0.3273370862007141\n",
      "Epoch: 54, Loss: 0.3941136598587036\n",
      "Epoch: 55, Loss: 0.2919645607471466\n",
      "Epoch: 55, Loss: 0.46000009775161743\n",
      "Epoch: 56, Loss: 0.30075109004974365\n",
      "Epoch: 56, Loss: 0.42396867275238037\n",
      "Epoch: 57, Loss: 0.31402039527893066\n",
      "Epoch: 57, Loss: 0.5075880885124207\n",
      "Epoch: 58, Loss: 0.47222447395324707\n",
      "Epoch: 58, Loss: 0.4479939341545105\n",
      "Epoch: 59, Loss: 0.35429590940475464\n",
      "Epoch: 59, Loss: 0.3230721950531006\n",
      "Epoch: 60, Loss: 0.42444145679473877\n",
      "Epoch: 60, Loss: 0.6444891691207886\n",
      "Epoch: 61, Loss: 0.48697879910469055\n",
      "Epoch: 61, Loss: 0.37420904636383057\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:29,390] Trial 10 finished with value: 0.4133858236452414 and parameters: {'hidden_size': 300, 'dropout': 0.2967865657055977, 'lr': 0.00014369557899949069}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.004586100578308\n",
      "Epoch: 0, Loss: 0.42078542709350586\n",
      "Epoch: 1, Loss: 0.4542127847671509\n",
      "Epoch: 1, Loss: 0.4524120092391968\n",
      "Epoch: 2, Loss: 0.3745706379413605\n",
      "Epoch: 2, Loss: 0.41589680314064026\n",
      "Epoch: 3, Loss: 0.38115304708480835\n",
      "Epoch: 3, Loss: 0.3823171555995941\n",
      "Epoch: 4, Loss: 0.35770782828330994\n",
      "Epoch: 4, Loss: 0.38616943359375\n",
      "Epoch: 5, Loss: 0.4078293442726135\n",
      "Epoch: 5, Loss: 0.5789992213249207\n",
      "Epoch: 6, Loss: 0.23404334485530853\n",
      "Epoch: 6, Loss: 0.44808611273765564\n",
      "Epoch: 7, Loss: 0.3701275587081909\n",
      "Epoch: 7, Loss: 0.3648543357849121\n",
      "Epoch: 8, Loss: 0.3626590371131897\n",
      "Epoch: 8, Loss: 0.2811112105846405\n",
      "Epoch: 9, Loss: 0.3297313451766968\n",
      "Epoch: 9, Loss: 0.3975798487663269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:32,216] Trial 11 finished with value: 0.4247560945819624 and parameters: {'hidden_size': 298, 'dropout': 0.28274653860042326, 'lr': 0.00011751463112875217}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.8036384582519531\n",
      "Epoch: 0, Loss: 0.4594501852989197\n",
      "Epoch: 1, Loss: 0.5593197345733643\n",
      "Epoch: 1, Loss: 0.28362226486206055\n",
      "Epoch: 2, Loss: 0.3400469422340393\n",
      "Epoch: 2, Loss: 0.48734167218208313\n",
      "Epoch: 3, Loss: 0.3207254111766815\n",
      "Epoch: 3, Loss: 0.4796919822692871\n",
      "Epoch: 4, Loss: 0.40167859196662903\n",
      "Epoch: 4, Loss: 0.5429617762565613\n",
      "Epoch: 5, Loss: 0.42417216300964355\n",
      "Epoch: 5, Loss: 0.6379126906394958\n",
      "Epoch: 6, Loss: 0.47054678201675415\n",
      "Epoch: 6, Loss: 0.38319119811058044\n",
      "Epoch: 7, Loss: 0.4014938771724701\n",
      "Epoch: 7, Loss: 0.302167683839798\n",
      "Epoch: 8, Loss: 0.30896204710006714\n",
      "Epoch: 8, Loss: 0.3964707851409912\n",
      "Epoch: 9, Loss: 0.3888130486011505\n",
      "Epoch: 9, Loss: 0.45942333340644836\n",
      "Epoch: 10, Loss: 0.33320510387420654\n",
      "Epoch: 10, Loss: 0.4733026325702667\n",
      "Epoch: 11, Loss: 0.4474271237850189\n",
      "Epoch: 11, Loss: 0.3468940854072571\n",
      "Epoch: 12, Loss: 0.44823896884918213\n",
      "Epoch: 12, Loss: 0.4502667784690857\n",
      "Epoch: 13, Loss: 0.5377280116081238\n",
      "Epoch: 13, Loss: 0.7613068222999573\n",
      "Epoch: 14, Loss: 0.2572038471698761\n",
      "Epoch: 14, Loss: 0.4256401062011719\n",
      "Epoch: 15, Loss: 0.428862988948822\n",
      "Epoch: 15, Loss: 0.34362688660621643\n",
      "Epoch: 16, Loss: 0.4100056290626526\n",
      "Epoch: 16, Loss: 0.6114482879638672\n",
      "Epoch: 17, Loss: 0.2661244869232178\n",
      "Epoch: 17, Loss: 0.41747015714645386\n",
      "Epoch: 18, Loss: 0.3967238962650299\n",
      "Epoch: 18, Loss: 0.563839852809906\n",
      "Epoch: 19, Loss: 0.2574427127838135\n",
      "Epoch: 19, Loss: 0.4653981924057007\n",
      "Epoch: 20, Loss: 0.4993895888328552\n",
      "Epoch: 20, Loss: 0.33315926790237427\n",
      "Epoch: 21, Loss: 0.402471661567688\n",
      "Epoch: 21, Loss: 0.4425366222858429\n",
      "Epoch: 22, Loss: 0.3440542221069336\n",
      "Epoch: 22, Loss: 0.4497235417366028\n",
      "Epoch: 23, Loss: 0.3018825650215149\n",
      "Epoch: 23, Loss: 0.4338957667350769\n",
      "Epoch: 24, Loss: 0.4602077305316925\n",
      "Epoch: 24, Loss: 0.2952374517917633\n",
      "Epoch: 25, Loss: 0.5286417603492737\n",
      "Epoch: 25, Loss: 0.6236078143119812\n",
      "Epoch: 26, Loss: 0.35688120126724243\n",
      "Epoch: 26, Loss: 0.41936594247817993\n",
      "Epoch: 27, Loss: 0.45786452293395996\n",
      "Epoch: 27, Loss: 0.5071460604667664\n",
      "Epoch: 28, Loss: 0.40770184993743896\n",
      "Epoch: 28, Loss: 0.45742958784103394\n",
      "Epoch: 29, Loss: 0.5321290493011475\n",
      "Epoch: 29, Loss: 0.31560641527175903\n",
      "Epoch: 30, Loss: 0.47633612155914307\n",
      "Epoch: 30, Loss: 0.4003668427467346\n",
      "Epoch: 31, Loss: 0.426306813955307\n",
      "Epoch: 31, Loss: 0.2831346392631531\n",
      "Epoch: 32, Loss: 0.3474712073802948\n",
      "Epoch: 32, Loss: 0.3578396737575531\n",
      "Epoch: 33, Loss: 0.3889900743961334\n",
      "Epoch: 33, Loss: 0.35604622960090637\n",
      "Epoch: 34, Loss: 0.5154144167900085\n",
      "Epoch: 34, Loss: 0.41447460651397705\n",
      "Epoch: 35, Loss: 0.45694035291671753\n",
      "Epoch: 35, Loss: 0.3750549554824829\n",
      "Epoch: 36, Loss: 0.47839048504829407\n",
      "Epoch: 36, Loss: 0.5985572934150696\n",
      "Epoch: 37, Loss: 0.3713175356388092\n",
      "Epoch: 37, Loss: 0.38781365752220154\n",
      "Epoch: 38, Loss: 0.3429395854473114\n",
      "Epoch: 38, Loss: 0.5036590099334717\n",
      "Epoch: 39, Loss: 0.5509408116340637\n",
      "Epoch: 39, Loss: 0.35465896129608154\n",
      "Epoch: 40, Loss: 0.5951483249664307\n",
      "Epoch: 40, Loss: 0.2815420627593994\n",
      "Epoch: 41, Loss: 0.3334384262561798\n",
      "Epoch: 41, Loss: 0.30984240770339966\n",
      "Epoch: 42, Loss: 0.37416380643844604\n",
      "Epoch: 42, Loss: 0.5097459554672241\n",
      "Epoch: 43, Loss: 0.3563922643661499\n",
      "Epoch: 43, Loss: 0.5738099813461304\n",
      "Epoch: 44, Loss: 0.27834320068359375\n",
      "Epoch: 44, Loss: 0.3503314256668091\n",
      "Epoch: 45, Loss: 0.4699019491672516\n",
      "Epoch: 45, Loss: 0.41234856843948364\n",
      "Epoch: 46, Loss: 0.3749060332775116\n",
      "Epoch: 46, Loss: 0.48952198028564453\n",
      "Epoch: 47, Loss: 0.4637061357498169\n",
      "Epoch: 47, Loss: 0.4859907627105713\n",
      "Epoch: 48, Loss: 0.5301433205604553\n",
      "Epoch: 48, Loss: 0.5529469847679138\n",
      "Epoch: 49, Loss: 0.45774704217910767\n",
      "Epoch: 49, Loss: 0.5964989066123962\n",
      "Epoch: 50, Loss: 0.3768933415412903\n",
      "Epoch: 50, Loss: 0.4210546612739563\n",
      "Epoch: 51, Loss: 0.3757597804069519\n",
      "Epoch: 51, Loss: 0.4101671874523163\n",
      "Epoch: 52, Loss: 0.3150250315666199\n",
      "Epoch: 52, Loss: 0.27588826417922974\n",
      "Epoch: 53, Loss: 0.3239355683326721\n",
      "Epoch: 53, Loss: 0.3708803653717041\n",
      "Epoch: 54, Loss: 0.44990766048431396\n",
      "Epoch: 54, Loss: 0.4467763900756836\n",
      "Epoch: 55, Loss: 0.3994186222553253\n",
      "Epoch: 55, Loss: 0.39651912450790405\n",
      "Epoch: 56, Loss: 0.3294121026992798\n",
      "Epoch: 56, Loss: 0.5774380564689636\n",
      "Epoch: 57, Loss: 0.4619349241256714\n",
      "Epoch: 57, Loss: 0.5051950812339783\n",
      "Epoch: 58, Loss: 0.4633941650390625\n",
      "Epoch: 58, Loss: 0.4108998775482178\n",
      "Epoch: 59, Loss: 0.33286264538764954\n",
      "Epoch: 59, Loss: 0.4934101700782776\n",
      "Epoch: 60, Loss: 0.39309900999069214\n",
      "Epoch: 60, Loss: 0.5698639154434204\n",
      "Epoch: 61, Loss: 0.3874821662902832\n",
      "Epoch: 61, Loss: 0.5037778615951538\n",
      "Epoch: 62, Loss: 0.48874330520629883\n",
      "Epoch: 62, Loss: 0.36442092061042786\n",
      "Epoch: 63, Loss: 0.3955576419830322\n",
      "Epoch: 63, Loss: 0.42390260100364685\n",
      "Epoch: 64, Loss: 0.33076030015945435\n",
      "Epoch: 64, Loss: 0.6053372025489807\n",
      "Epoch: 65, Loss: 0.3033747375011444\n",
      "Epoch: 65, Loss: 0.4062766134738922\n",
      "Epoch: 66, Loss: 0.42622920870780945\n",
      "Epoch: 66, Loss: 0.4422031044960022\n",
      "Epoch: 67, Loss: 0.46450039744377136\n",
      "Epoch: 67, Loss: 0.3899439573287964\n",
      "Epoch: 68, Loss: 0.4541897773742676\n",
      "Epoch: 68, Loss: 0.3734402358531952\n",
      "Epoch: 69, Loss: 0.4399745464324951\n",
      "Epoch: 69, Loss: 0.3578524887561798\n",
      "Epoch: 70, Loss: 0.44635891914367676\n",
      "Epoch: 70, Loss: 0.3477710783481598\n",
      "Epoch: 71, Loss: 0.4259096384048462\n",
      "Epoch: 71, Loss: 0.45971599221229553\n",
      "Epoch: 72, Loss: 0.5152490735054016\n",
      "Epoch: 72, Loss: 0.43473002314567566\n",
      "Epoch: 73, Loss: 0.3892710208892822\n",
      "Epoch: 73, Loss: 0.4371533691883087\n",
      "Epoch: 74, Loss: 0.5386270880699158\n",
      "Epoch: 74, Loss: 0.4789460003376007\n",
      "Epoch: 75, Loss: 0.4493030309677124\n",
      "Epoch: 75, Loss: 0.3926071524620056\n",
      "Epoch: 76, Loss: 0.46517452597618103\n",
      "Epoch: 76, Loss: 0.3667045831680298\n",
      "Epoch: 77, Loss: 0.47108620405197144\n",
      "Epoch: 77, Loss: 0.28372809290885925\n",
      "Epoch: 78, Loss: 0.2870328724384308\n",
      "Epoch: 78, Loss: 0.32771119475364685\n",
      "Epoch: 79, Loss: 0.3501414358615875\n",
      "Epoch: 79, Loss: 0.35935765504837036\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:53,967] Trial 12 finished with value: 0.41534855802722787 and parameters: {'hidden_size': 180, 'dropout': 0.39626789920793365, 'lr': 7.010576176035169e-05}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6661069393157959\n",
      "Epoch: 0, Loss: 0.47565510869026184\n",
      "Epoch: 1, Loss: 0.4368438720703125\n",
      "Epoch: 1, Loss: 0.33039557933807373\n",
      "Epoch: 2, Loss: 0.49433231353759766\n",
      "Epoch: 2, Loss: 0.5172950625419617\n",
      "Epoch: 3, Loss: 0.41988593339920044\n",
      "Epoch: 3, Loss: 0.7242680788040161\n",
      "Epoch: 4, Loss: 0.4836803078651428\n",
      "Epoch: 4, Loss: 0.49080049991607666\n",
      "Epoch: 5, Loss: 0.33690476417541504\n",
      "Epoch: 5, Loss: 0.24353189766407013\n",
      "Epoch: 6, Loss: 0.30004823207855225\n",
      "Epoch: 6, Loss: 0.3157042860984802\n",
      "Epoch: 7, Loss: 0.4565933644771576\n",
      "Epoch: 7, Loss: 0.42397618293762207\n",
      "Epoch: 8, Loss: 0.4555257260799408\n",
      "Epoch: 8, Loss: 0.3449230492115021\n",
      "Epoch: 9, Loss: 0.5253820419311523\n",
      "Epoch: 9, Loss: 0.4792752265930176\n",
      "Epoch: 10, Loss: 0.32220718264579773\n",
      "Epoch: 10, Loss: 0.4086326062679291\n",
      "Epoch: 11, Loss: 0.3858264684677124\n",
      "Epoch: 11, Loss: 0.5343315601348877\n",
      "Epoch: 12, Loss: 0.401411235332489\n",
      "Epoch: 12, Loss: 0.3316209614276886\n",
      "Epoch: 13, Loss: 0.36230796575546265\n",
      "Epoch: 13, Loss: 0.36699140071868896\n",
      "Epoch: 14, Loss: 0.5272839069366455\n",
      "Epoch: 14, Loss: 0.3086065351963043\n",
      "Epoch: 15, Loss: 0.4496268332004547\n",
      "Epoch: 15, Loss: 0.6050530076026917\n",
      "Epoch: 16, Loss: 0.46905696392059326\n",
      "Epoch: 16, Loss: 0.5177318453788757\n",
      "Epoch: 17, Loss: 0.4188709259033203\n",
      "Epoch: 17, Loss: 0.41139939427375793\n",
      "Epoch: 18, Loss: 0.4700232446193695\n",
      "Epoch: 18, Loss: 0.345287024974823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:18:59,241] Trial 13 finished with value: 0.4238980970119838 and parameters: {'hidden_size': 263, 'dropout': 0.15834712227931433, 'lr': 0.0005498785972102752}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.62332683801651\n",
      "Epoch: 0, Loss: 0.43177056312561035\n",
      "Epoch: 1, Loss: 0.2918297052383423\n",
      "Epoch: 1, Loss: 0.45235565304756165\n",
      "Epoch: 2, Loss: 0.47511613368988037\n",
      "Epoch: 2, Loss: 0.3874140977859497\n",
      "Epoch: 3, Loss: 0.503330409526825\n",
      "Epoch: 3, Loss: 0.317443311214447\n",
      "Epoch: 4, Loss: 0.35227638483047485\n",
      "Epoch: 4, Loss: 0.3041534423828125\n",
      "Epoch: 5, Loss: 0.5568457245826721\n",
      "Epoch: 5, Loss: 0.5207281112670898\n",
      "Epoch: 6, Loss: 0.5454759001731873\n",
      "Epoch: 6, Loss: 0.5044186115264893\n",
      "Epoch: 7, Loss: 0.35122308135032654\n",
      "Epoch: 7, Loss: 0.40714094042778015\n",
      "Epoch: 8, Loss: 0.44723865389823914\n",
      "Epoch: 8, Loss: 0.4941204786300659\n",
      "Epoch: 9, Loss: 0.4171912968158722\n",
      "Epoch: 9, Loss: 0.5914026498794556\n",
      "Epoch: 10, Loss: 0.5114032626152039\n",
      "Epoch: 10, Loss: 0.40606531500816345\n",
      "Epoch: 11, Loss: 0.42446401715278625\n",
      "Epoch: 11, Loss: 0.41051024198532104\n",
      "Epoch: 12, Loss: 0.4267750680446625\n",
      "Epoch: 12, Loss: 0.4042692184448242\n",
      "Epoch: 13, Loss: 0.4675194323062897\n",
      "Epoch: 13, Loss: 0.44125479459762573\n",
      "Epoch: 14, Loss: 0.24225707352161407\n",
      "Epoch: 14, Loss: 0.3958187699317932\n",
      "Epoch: 15, Loss: 0.39339950680732727\n",
      "Epoch: 15, Loss: 0.3661831021308899\n",
      "Epoch: 16, Loss: 0.573982834815979\n",
      "Epoch: 16, Loss: 0.4462220370769501\n",
      "Epoch: 17, Loss: 0.5271369814872742\n",
      "Epoch: 17, Loss: 0.32758331298828125\n",
      "Epoch: 18, Loss: 0.27547597885131836\n",
      "Epoch: 18, Loss: 0.3221026659011841\n",
      "Epoch: 19, Loss: 0.38238999247550964\n",
      "Epoch: 19, Loss: 0.2896159589290619\n",
      "Epoch: 20, Loss: 0.5295047760009766\n",
      "Epoch: 20, Loss: 0.398520827293396\n",
      "Epoch: 21, Loss: 0.6238414645195007\n",
      "Epoch: 21, Loss: 0.4566415250301361\n",
      "Epoch: 22, Loss: 0.4441766142845154\n",
      "Epoch: 22, Loss: 0.4836604595184326\n",
      "Epoch: 23, Loss: 0.26233065128326416\n",
      "Epoch: 23, Loss: 0.4417574405670166\n",
      "Epoch: 24, Loss: 0.5959281325340271\n",
      "Epoch: 24, Loss: 0.35975489020347595\n",
      "Epoch: 25, Loss: 0.2970416247844696\n",
      "Epoch: 25, Loss: 0.32229068875312805\n",
      "Epoch: 26, Loss: 0.37302905321121216\n",
      "Epoch: 26, Loss: 0.24399083852767944\n",
      "Epoch: 27, Loss: 0.2968755066394806\n",
      "Epoch: 27, Loss: 0.5671559572219849\n",
      "Epoch: 28, Loss: 0.39417600631713867\n",
      "Epoch: 28, Loss: 0.4751826524734497\n",
      "Epoch: 29, Loss: 0.6096504330635071\n",
      "Epoch: 29, Loss: 0.44347015023231506\n",
      "Epoch: 30, Loss: 0.35898229479789734\n",
      "Epoch: 30, Loss: 0.3342452049255371\n",
      "Epoch: 31, Loss: 0.41335543990135193\n",
      "Epoch: 31, Loss: 0.2926950454711914\n",
      "Epoch: 32, Loss: 0.33705082535743713\n",
      "Epoch: 32, Loss: 0.48842182755470276\n",
      "Epoch: 33, Loss: 0.5038053393363953\n",
      "Epoch: 33, Loss: 0.4297124147415161\n",
      "Epoch: 34, Loss: 0.5049280524253845\n",
      "Epoch: 34, Loss: 0.3410853147506714\n",
      "Epoch: 35, Loss: 0.39279845356941223\n",
      "Epoch: 35, Loss: 0.4517216682434082\n",
      "Epoch: 36, Loss: 0.37466633319854736\n",
      "Epoch: 36, Loss: 0.5382420420646667\n",
      "Epoch: 37, Loss: 0.5569689273834229\n",
      "Epoch: 37, Loss: 0.35230812430381775\n",
      "Epoch: 38, Loss: 0.46773096919059753\n",
      "Epoch: 38, Loss: 0.46223536133766174\n",
      "Epoch: 39, Loss: 0.451170951128006\n",
      "Epoch: 39, Loss: 0.44856321811676025\n",
      "Epoch: 40, Loss: 0.4222005307674408\n",
      "Epoch: 40, Loss: 0.4734959602355957\n",
      "Epoch: 41, Loss: 0.4166833460330963\n",
      "Epoch: 41, Loss: 0.43737971782684326\n",
      "Epoch: 42, Loss: 0.4481218755245209\n",
      "Epoch: 42, Loss: 0.3656804859638214\n",
      "Epoch: 43, Loss: 0.3038119077682495\n",
      "Epoch: 43, Loss: 0.45658037066459656\n",
      "Epoch: 44, Loss: 0.5516650080680847\n",
      "Epoch: 44, Loss: 0.4388224184513092\n",
      "Epoch: 45, Loss: 0.4238233268260956\n",
      "Epoch: 45, Loss: 0.37535592913627625\n",
      "Epoch: 46, Loss: 0.2990541458129883\n",
      "Epoch: 46, Loss: 0.39309898018836975\n",
      "Epoch: 47, Loss: 0.4438108503818512\n",
      "Epoch: 47, Loss: 0.43805640935897827\n",
      "Epoch: 48, Loss: 0.46958091855049133\n",
      "Epoch: 48, Loss: 0.3254221975803375\n",
      "Epoch: 49, Loss: 0.2864213287830353\n",
      "Epoch: 49, Loss: 0.3550679087638855\n",
      "Epoch: 50, Loss: 0.21506331861019135\n",
      "Epoch: 50, Loss: 0.4387350082397461\n",
      "Epoch: 51, Loss: 0.4216543138027191\n",
      "Epoch: 51, Loss: 0.4079344868659973\n",
      "Epoch: 52, Loss: 0.33506080508232117\n",
      "Epoch: 52, Loss: 0.47717511653900146\n",
      "Epoch: 53, Loss: 0.4062409996986389\n",
      "Epoch: 53, Loss: 0.5159594416618347\n",
      "Epoch: 54, Loss: 0.35239630937576294\n",
      "Epoch: 54, Loss: 0.3896651864051819\n",
      "Epoch: 55, Loss: 0.3879532217979431\n",
      "Epoch: 55, Loss: 0.37823235988616943\n",
      "Epoch: 56, Loss: 0.49825039505958557\n",
      "Epoch: 56, Loss: 0.499775767326355\n",
      "Epoch: 57, Loss: 0.40571102499961853\n",
      "Epoch: 57, Loss: 0.4863487482070923\n",
      "Epoch: 58, Loss: 0.408420592546463\n",
      "Epoch: 58, Loss: 0.3782687485218048\n",
      "Epoch: 59, Loss: 0.29521816968917847\n",
      "Epoch: 59, Loss: 0.5056667327880859\n",
      "Epoch: 60, Loss: 0.4713563323020935\n",
      "Epoch: 60, Loss: 0.29371878504753113\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:19:15,975] Trial 14 finished with value: 0.4181390768524421 and parameters: {'hidden_size': 297, 'dropout': 0.31074849320931847, 'lr': 6.645702073206894e-05}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.523016095161438\n",
      "Epoch: 0, Loss: 0.3666870594024658\n",
      "Epoch: 1, Loss: 0.4307761788368225\n",
      "Epoch: 1, Loss: 0.4885629117488861\n",
      "Epoch: 2, Loss: 0.41319262981414795\n",
      "Epoch: 2, Loss: 0.4827874004840851\n",
      "Epoch: 3, Loss: 0.3862878680229187\n",
      "Epoch: 3, Loss: 0.43648457527160645\n",
      "Epoch: 4, Loss: 0.4394247829914093\n",
      "Epoch: 4, Loss: 0.3239324688911438\n",
      "Epoch: 5, Loss: 0.35483241081237793\n",
      "Epoch: 5, Loss: 0.5887032151222229\n",
      "Epoch: 6, Loss: 0.2612144351005554\n",
      "Epoch: 6, Loss: 0.4795770049095154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:19:17,990] Trial 15 finished with value: 0.42653757952709004 and parameters: {'hidden_size': 53, 'dropout': 0.16247875908470438, 'lr': 0.000835291979747401}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7512915134429932\n",
      "Epoch: 0, Loss: 0.6663556098937988\n",
      "Epoch: 1, Loss: 0.63459312915802\n",
      "Epoch: 1, Loss: 0.4164237082004547\n",
      "Epoch: 2, Loss: 0.4374808073043823\n",
      "Epoch: 2, Loss: 0.4268292784690857\n",
      "Epoch: 3, Loss: 0.46152064204216003\n",
      "Epoch: 3, Loss: 0.35178038477897644\n",
      "Epoch: 4, Loss: 0.46853747963905334\n",
      "Epoch: 4, Loss: 0.36249175667762756\n",
      "Epoch: 5, Loss: 0.4455724358558655\n",
      "Epoch: 5, Loss: 0.3785977363586426\n",
      "Epoch: 6, Loss: 0.41844993829727173\n",
      "Epoch: 6, Loss: 0.36574333906173706\n",
      "Epoch: 7, Loss: 0.37235110998153687\n",
      "Epoch: 7, Loss: 0.43003028631210327\n",
      "Epoch: 8, Loss: 0.34285852313041687\n",
      "Epoch: 8, Loss: 0.4014495313167572\n",
      "Epoch: 9, Loss: 0.297600120306015\n",
      "Epoch: 9, Loss: 0.30571961402893066\n",
      "Epoch: 10, Loss: 0.4077049791812897\n",
      "Epoch: 10, Loss: 0.49549028277397156\n",
      "Epoch: 11, Loss: 0.5398162007331848\n",
      "Epoch: 11, Loss: 0.41734376549720764\n",
      "Epoch: 12, Loss: 0.3961869776248932\n",
      "Epoch: 12, Loss: 0.3813062906265259\n",
      "Epoch: 13, Loss: 0.5153713822364807\n",
      "Epoch: 13, Loss: 0.4609590172767639\n",
      "Epoch: 14, Loss: 0.4881060719490051\n",
      "Epoch: 14, Loss: 0.31051546335220337\n",
      "Epoch: 15, Loss: 0.428650826215744\n",
      "Epoch: 15, Loss: 0.4930814802646637\n",
      "Epoch: 16, Loss: 0.5413509607315063\n",
      "Epoch: 16, Loss: 0.42010384798049927\n",
      "Epoch: 17, Loss: 0.5277101993560791\n",
      "Epoch: 17, Loss: 0.4534066915512085\n",
      "Epoch: 18, Loss: 0.3047581613063812\n",
      "Epoch: 18, Loss: 0.27634578943252563\n",
      "Epoch: 19, Loss: 0.4560144245624542\n",
      "Epoch: 19, Loss: 0.3917672336101532\n",
      "Epoch: 20, Loss: 0.28135377168655396\n",
      "Epoch: 20, Loss: 0.44381383061408997\n",
      "Epoch: 21, Loss: 0.49938422441482544\n",
      "Epoch: 21, Loss: 0.5534291863441467\n",
      "Epoch: 22, Loss: 0.40602704882621765\n",
      "Epoch: 22, Loss: 0.25100260972976685\n",
      "Epoch: 23, Loss: 0.40608835220336914\n",
      "Epoch: 23, Loss: 0.4348686635494232\n",
      "Epoch: 24, Loss: 0.5212960839271545\n",
      "Epoch: 24, Loss: 0.4320579171180725\n",
      "Epoch: 25, Loss: 0.2533082067966461\n",
      "Epoch: 25, Loss: 0.408220499753952\n",
      "Epoch: 26, Loss: 0.3670876622200012\n",
      "Epoch: 26, Loss: 0.5928699970245361\n",
      "Epoch: 27, Loss: 0.32671472430229187\n",
      "Epoch: 27, Loss: 0.42911413311958313\n",
      "Epoch: 28, Loss: 0.39392125606536865\n",
      "Epoch: 28, Loss: 0.3596373200416565\n",
      "Epoch: 29, Loss: 0.5371829271316528\n",
      "Epoch: 29, Loss: 0.5508943796157837\n",
      "Epoch: 30, Loss: 0.4200546443462372\n",
      "Epoch: 30, Loss: 0.5867783427238464\n",
      "Epoch: 31, Loss: 0.28640246391296387\n",
      "Epoch: 31, Loss: 0.5049344301223755\n",
      "Epoch: 32, Loss: 0.22578085958957672\n",
      "Epoch: 32, Loss: 0.4973015785217285\n",
      "Epoch: 33, Loss: 0.4121329188346863\n",
      "Epoch: 33, Loss: 0.40491971373558044\n",
      "Epoch: 34, Loss: 0.5174183249473572\n",
      "Epoch: 34, Loss: 0.3213917911052704\n",
      "Epoch: 35, Loss: 0.44811639189720154\n",
      "Epoch: 35, Loss: 0.25476258993148804\n",
      "Epoch: 36, Loss: 0.4494064152240753\n",
      "Epoch: 36, Loss: 0.3959501385688782\n",
      "Epoch: 37, Loss: 0.4501345753669739\n",
      "Epoch: 37, Loss: 0.5203303098678589\n",
      "Epoch: 38, Loss: 0.473203182220459\n",
      "Epoch: 38, Loss: 0.39116811752319336\n",
      "Epoch: 39, Loss: 0.4156052768230438\n",
      "Epoch: 39, Loss: 0.38510948419570923\n",
      "Epoch: 40, Loss: 0.26825016736984253\n",
      "Epoch: 40, Loss: 0.22975623607635498\n",
      "Epoch: 41, Loss: 0.4510858952999115\n",
      "Epoch: 41, Loss: 0.33929091691970825\n",
      "Epoch: 42, Loss: 0.37441301345825195\n",
      "Epoch: 42, Loss: 0.4861288070678711\n",
      "Epoch: 43, Loss: 0.3778897821903229\n",
      "Epoch: 43, Loss: 0.5225332379341125\n",
      "Epoch: 44, Loss: 0.47758200764656067\n",
      "Epoch: 44, Loss: 0.45755109190940857\n",
      "Epoch: 45, Loss: 0.5501257181167603\n",
      "Epoch: 45, Loss: 0.4682190716266632\n",
      "Epoch: 46, Loss: 0.30974647402763367\n",
      "Epoch: 46, Loss: 0.357399046421051\n",
      "Epoch: 47, Loss: 0.3940337300300598\n",
      "Epoch: 47, Loss: 0.4299275279045105\n",
      "Epoch: 48, Loss: 0.5035363435745239\n",
      "Epoch: 48, Loss: 0.28835371136665344\n",
      "Epoch: 49, Loss: 0.4223708212375641\n",
      "Epoch: 49, Loss: 0.5264010429382324\n",
      "Epoch: 50, Loss: 0.7721766829490662\n",
      "Epoch: 50, Loss: 0.30416339635849\n",
      "Epoch: 51, Loss: 0.4626292586326599\n",
      "Epoch: 51, Loss: 0.5489354729652405\n",
      "Epoch: 52, Loss: 0.3211551606655121\n",
      "Epoch: 52, Loss: 0.38139835000038147\n",
      "Epoch: 53, Loss: 0.4718226492404938\n",
      "Epoch: 53, Loss: 0.34111085534095764\n",
      "Epoch: 54, Loss: 0.3917486369609833\n",
      "Epoch: 54, Loss: 0.4661984443664551\n",
      "Epoch: 55, Loss: 0.39527225494384766\n",
      "Epoch: 55, Loss: 0.46625128388404846\n",
      "Epoch: 56, Loss: 0.43105337023735046\n",
      "Epoch: 56, Loss: 0.44738179445266724\n",
      "Epoch: 57, Loss: 0.4664071500301361\n",
      "Epoch: 57, Loss: 0.3789157569408417\n",
      "Epoch: 58, Loss: 0.4365963637828827\n",
      "Epoch: 58, Loss: 0.40750691294670105\n",
      "Epoch: 59, Loss: 0.29632288217544556\n",
      "Epoch: 59, Loss: 0.5887393951416016\n",
      "Epoch: 60, Loss: 0.3766957223415375\n",
      "Epoch: 60, Loss: 0.5308806300163269\n",
      "Epoch: 61, Loss: 0.45795369148254395\n",
      "Epoch: 61, Loss: 0.3617229461669922\n",
      "Epoch: 62, Loss: 0.4890570640563965\n",
      "Epoch: 62, Loss: 0.3352416455745697\n",
      "Epoch: 63, Loss: 0.4866977035999298\n",
      "Epoch: 63, Loss: 0.4017256498336792\n",
      "Epoch: 64, Loss: 0.4268453121185303\n",
      "Epoch: 64, Loss: 0.5169077515602112\n",
      "Epoch: 65, Loss: 0.3732312023639679\n",
      "Epoch: 65, Loss: 0.4043256640434265\n",
      "Epoch: 66, Loss: 0.4210904538631439\n",
      "Epoch: 66, Loss: 0.49567511677742004\n",
      "Epoch: 67, Loss: 0.2965703308582306\n",
      "Epoch: 67, Loss: 0.31611213088035583\n",
      "Epoch: 68, Loss: 0.38393136858940125\n",
      "Epoch: 68, Loss: 0.3407934606075287\n",
      "Epoch: 69, Loss: 0.37595415115356445\n",
      "Epoch: 69, Loss: 0.4444061517715454\n",
      "Epoch: 70, Loss: 0.32626309990882874\n",
      "Epoch: 70, Loss: 0.42638644576072693\n",
      "Epoch: 71, Loss: 0.5874641537666321\n",
      "Epoch: 71, Loss: 0.3278552293777466\n",
      "Epoch: 72, Loss: 0.3911410868167877\n",
      "Epoch: 72, Loss: 0.43190616369247437\n",
      "Epoch: 73, Loss: 0.2695848047733307\n",
      "Epoch: 73, Loss: 0.3707225024700165\n",
      "Epoch: 74, Loss: 0.4497530162334442\n",
      "Epoch: 74, Loss: 0.38137781620025635\n",
      "Epoch: 75, Loss: 0.38749921321868896\n",
      "Epoch: 75, Loss: 0.37123337388038635\n",
      "Epoch: 76, Loss: 0.34754547476768494\n",
      "Epoch: 76, Loss: 0.38346803188323975\n",
      "Epoch: 77, Loss: 0.38947224617004395\n",
      "Epoch: 77, Loss: 0.3606478273868561\n",
      "Epoch: 78, Loss: 0.43639934062957764\n",
      "Epoch: 78, Loss: 0.4228914678096771\n",
      "Epoch: 79, Loss: 0.36181217432022095\n",
      "Epoch: 79, Loss: 0.36876538395881653\n",
      "Epoch: 80, Loss: 0.6921520233154297\n",
      "Epoch: 80, Loss: 0.41682344675064087\n",
      "Epoch: 81, Loss: 0.4054732322692871\n",
      "Epoch: 81, Loss: 0.4956994950771332\n",
      "Epoch: 82, Loss: 0.5496710538864136\n",
      "Epoch: 82, Loss: 0.522282063961029\n",
      "Epoch: 83, Loss: 0.30815020203590393\n",
      "Epoch: 83, Loss: 0.479462206363678\n",
      "Epoch: 84, Loss: 0.49770036339759827\n",
      "Epoch: 84, Loss: 0.38350147008895874\n",
      "Epoch: 85, Loss: 0.4529925584793091\n",
      "Epoch: 85, Loss: 0.6107838153839111\n",
      "Epoch: 86, Loss: 0.49221181869506836\n",
      "Epoch: 86, Loss: 0.35430821776390076\n",
      "Epoch: 87, Loss: 0.441718727350235\n",
      "Epoch: 87, Loss: 0.33903172612190247\n",
      "Epoch: 88, Loss: 0.4462287425994873\n",
      "Epoch: 88, Loss: 0.41477158665657043\n",
      "Epoch: 89, Loss: 0.5507601499557495\n",
      "Epoch: 89, Loss: 0.3694612979888916\n",
      "Epoch: 90, Loss: 0.5327604413032532\n",
      "Epoch: 90, Loss: 0.32304614782333374\n",
      "Epoch: 91, Loss: 0.3733730912208557\n",
      "Epoch: 91, Loss: 0.2709572911262512\n",
      "Epoch: 92, Loss: 0.4133093059062958\n",
      "Epoch: 92, Loss: 0.27066031098365784\n",
      "Epoch: 93, Loss: 0.36098477244377136\n",
      "Epoch: 93, Loss: 0.45451053977012634\n",
      "Epoch: 94, Loss: 0.4933937191963196\n",
      "Epoch: 94, Loss: 0.3867899477481842\n",
      "Epoch: 95, Loss: 0.393479585647583\n",
      "Epoch: 95, Loss: 0.33015185594558716\n",
      "Epoch: 96, Loss: 0.4111848771572113\n",
      "Epoch: 96, Loss: 0.4000051021575928\n",
      "Epoch: 97, Loss: 0.3927118182182312\n",
      "Epoch: 97, Loss: 0.32854822278022766\n",
      "Epoch: 98, Loss: 0.41985371708869934\n",
      "Epoch: 98, Loss: 0.4608149230480194\n",
      "Epoch: 99, Loss: 0.411998450756073\n",
      "Epoch: 99, Loss: 0.6122023463249207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:19:45,299] Trial 16 finished with value: 0.42277834058872293 and parameters: {'hidden_size': 197, 'dropout': 0.2784502409362497, 'lr': 1.5513357751988314e-05}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6792207360267639\n",
      "Epoch: 0, Loss: 0.6011373400688171\n",
      "Epoch: 1, Loss: 0.4136902987957001\n",
      "Epoch: 1, Loss: 0.5689564943313599\n",
      "Epoch: 2, Loss: 0.18878744542598724\n",
      "Epoch: 2, Loss: 0.40084517002105713\n",
      "Epoch: 3, Loss: 0.3485562801361084\n",
      "Epoch: 3, Loss: 0.3081146478652954\n",
      "Epoch: 4, Loss: 0.49107611179351807\n",
      "Epoch: 4, Loss: 0.4516052007675171\n",
      "Epoch: 5, Loss: 0.48697879910469055\n",
      "Epoch: 5, Loss: 0.4095098376274109\n",
      "Epoch: 6, Loss: 0.5391970872879028\n",
      "Epoch: 6, Loss: 0.5200082063674927\n",
      "Epoch: 7, Loss: 0.4110456705093384\n",
      "Epoch: 7, Loss: 0.4194624722003937\n",
      "Epoch: 8, Loss: 0.43597620725631714\n",
      "Epoch: 8, Loss: 0.4118558168411255\n",
      "Epoch: 9, Loss: 0.4409986734390259\n",
      "Epoch: 9, Loss: 0.554490327835083\n",
      "Epoch: 10, Loss: 0.2841767966747284\n",
      "Epoch: 10, Loss: 0.3356788456439972\n",
      "Epoch: 11, Loss: 0.3717643618583679\n",
      "Epoch: 11, Loss: 0.45510366559028625\n",
      "Epoch: 12, Loss: 0.36554190516471863\n",
      "Epoch: 12, Loss: 0.3702445924282074\n",
      "Epoch: 13, Loss: 0.49730223417282104\n",
      "Epoch: 13, Loss: 0.5303084254264832\n",
      "Epoch: 14, Loss: 0.422012060880661\n",
      "Epoch: 14, Loss: 0.28147149085998535\n",
      "Epoch: 15, Loss: 0.3756338357925415\n",
      "Epoch: 15, Loss: 0.5435037016868591\n",
      "Epoch: 16, Loss: 0.28810063004493713\n",
      "Epoch: 16, Loss: 0.4683058559894562\n",
      "Epoch: 17, Loss: 0.44366520643234253\n",
      "Epoch: 17, Loss: 0.4683053493499756\n",
      "Epoch: 18, Loss: 0.45222654938697815\n",
      "Epoch: 18, Loss: 0.45494434237480164\n",
      "Epoch: 19, Loss: 0.3709025979042053\n",
      "Epoch: 19, Loss: 0.3730817139148712\n",
      "Epoch: 20, Loss: 0.39826369285583496\n",
      "Epoch: 20, Loss: 0.29249560832977295\n",
      "Epoch: 21, Loss: 0.4482152462005615\n",
      "Epoch: 21, Loss: 0.42333605885505676\n",
      "Epoch: 22, Loss: 0.4432566463947296\n",
      "Epoch: 22, Loss: 0.44515615701675415\n",
      "Epoch: 23, Loss: 0.3458276391029358\n",
      "Epoch: 23, Loss: 0.3248026967048645\n",
      "Epoch: 24, Loss: 0.3231146037578583\n",
      "Epoch: 24, Loss: 0.4197784960269928\n",
      "Epoch: 25, Loss: 0.31227776408195496\n",
      "Epoch: 25, Loss: 0.3888946771621704\n",
      "Epoch: 26, Loss: 0.32415515184402466\n",
      "Epoch: 26, Loss: 0.3624480068683624\n",
      "Epoch: 27, Loss: 0.39815405011177063\n",
      "Epoch: 27, Loss: 0.5496448874473572\n",
      "Epoch: 28, Loss: 0.3933138847351074\n",
      "Epoch: 28, Loss: 0.40425732731819153\n",
      "Epoch: 29, Loss: 0.4013414978981018\n",
      "Epoch: 29, Loss: 0.34130406379699707\n",
      "Epoch: 30, Loss: 0.34083467721939087\n",
      "Epoch: 30, Loss: 0.31964385509490967\n",
      "Epoch: 31, Loss: 0.41861340403556824\n",
      "Epoch: 31, Loss: 0.35101988911628723\n",
      "Epoch: 32, Loss: 0.35785695910453796\n",
      "Epoch: 32, Loss: 0.37584155797958374\n",
      "Epoch: 33, Loss: 0.5752567648887634\n",
      "Epoch: 33, Loss: 0.5482369065284729\n",
      "Epoch: 34, Loss: 0.4551554024219513\n",
      "Epoch: 34, Loss: 0.5780609250068665\n",
      "Epoch: 35, Loss: 0.29275187849998474\n",
      "Epoch: 35, Loss: 0.45231911540031433\n",
      "Epoch: 36, Loss: 0.40144091844558716\n",
      "Epoch: 36, Loss: 0.31894123554229736\n",
      "Epoch: 37, Loss: 0.36760663986206055\n",
      "Epoch: 37, Loss: 0.43355780839920044\n",
      "Epoch: 38, Loss: 0.44152820110321045\n",
      "Epoch: 38, Loss: 0.4827268123626709\n",
      "Epoch: 39, Loss: 0.4504689574241638\n",
      "Epoch: 39, Loss: 0.48580747842788696\n",
      "Epoch: 40, Loss: 0.40983858704566956\n",
      "Epoch: 40, Loss: 0.45595866441726685\n",
      "Epoch: 41, Loss: 0.3411908745765686\n",
      "Epoch: 41, Loss: 0.47215062379837036\n",
      "Epoch: 42, Loss: 0.393211305141449\n",
      "Epoch: 42, Loss: 0.24994949996471405\n",
      "Epoch: 43, Loss: 0.4702679216861725\n",
      "Epoch: 43, Loss: 0.34442582726478577\n",
      "Epoch: 44, Loss: 0.4227362871170044\n",
      "Epoch: 44, Loss: 0.2828903794288635\n",
      "Epoch: 45, Loss: 0.3190370202064514\n",
      "Epoch: 45, Loss: 0.49718692898750305\n",
      "Epoch: 46, Loss: 0.48809635639190674\n",
      "Epoch: 46, Loss: 0.327456533908844\n",
      "Epoch: 47, Loss: 0.4052419662475586\n",
      "Epoch: 47, Loss: 0.48835423588752747\n",
      "Epoch: 48, Loss: 0.6314891576766968\n",
      "Epoch: 48, Loss: 0.4086335301399231\n",
      "Epoch: 49, Loss: 0.3194082975387573\n",
      "Epoch: 49, Loss: 0.3697253465652466\n",
      "Epoch: 50, Loss: 0.48488476872444153\n",
      "Epoch: 50, Loss: 0.3783046007156372\n",
      "Epoch: 51, Loss: 0.44546830654144287\n",
      "Epoch: 51, Loss: 0.5187790393829346\n",
      "Epoch: 52, Loss: 0.31545448303222656\n",
      "Epoch: 52, Loss: 0.329924613237381\n",
      "Epoch: 53, Loss: 0.545177698135376\n",
      "Epoch: 53, Loss: 0.32339712977409363\n",
      "Epoch: 54, Loss: 0.293988436460495\n",
      "Epoch: 54, Loss: 0.39609843492507935\n",
      "Epoch: 55, Loss: 0.5264551043510437\n",
      "Epoch: 55, Loss: 0.44328203797340393\n",
      "Epoch: 56, Loss: 0.46123167872428894\n",
      "Epoch: 56, Loss: 0.38944530487060547\n",
      "Epoch: 57, Loss: 0.38233810663223267\n",
      "Epoch: 57, Loss: 0.47776126861572266\n",
      "Epoch: 58, Loss: 0.47415775060653687\n",
      "Epoch: 58, Loss: 0.470551073551178\n",
      "Epoch: 59, Loss: 0.42531901597976685\n",
      "Epoch: 59, Loss: 0.5381724238395691\n",
      "Epoch: 60, Loss: 0.5115734338760376\n",
      "Epoch: 60, Loss: 0.5363852977752686\n",
      "Epoch: 61, Loss: 0.4270428717136383\n",
      "Epoch: 61, Loss: 0.5258020162582397\n",
      "Epoch: 62, Loss: 0.35495033860206604\n",
      "Epoch: 62, Loss: 0.35289981961250305\n",
      "Epoch: 63, Loss: 0.402466356754303\n",
      "Epoch: 63, Loss: 0.3320331871509552\n",
      "Epoch: 64, Loss: 0.294467031955719\n",
      "Epoch: 64, Loss: 0.3078955113887787\n",
      "Epoch: 65, Loss: 0.5042178630828857\n",
      "Epoch: 65, Loss: 0.5312877893447876\n",
      "Epoch: 66, Loss: 0.4919314384460449\n",
      "Epoch: 66, Loss: 0.27434250712394714\n",
      "Epoch: 67, Loss: 0.36350199580192566\n",
      "Epoch: 67, Loss: 0.3429070711135864\n",
      "Epoch: 68, Loss: 0.3856108486652374\n",
      "Epoch: 68, Loss: 0.6059064865112305\n",
      "Epoch: 69, Loss: 0.3204353153705597\n",
      "Epoch: 69, Loss: 0.37722644209861755\n",
      "Epoch: 70, Loss: 0.43855223059654236\n",
      "Epoch: 70, Loss: 0.3494393229484558\n",
      "Epoch: 71, Loss: 0.37480366230010986\n",
      "Epoch: 71, Loss: 0.4152056872844696\n",
      "Epoch: 72, Loss: 0.4534716010093689\n",
      "Epoch: 72, Loss: 0.39966171979904175\n",
      "Epoch: 73, Loss: 0.31725794076919556\n",
      "Epoch: 73, Loss: 0.32087773084640503\n",
      "Epoch: 74, Loss: 0.45080986618995667\n",
      "Epoch: 74, Loss: 0.4044305980205536\n",
      "Epoch: 75, Loss: 0.35134774446487427\n",
      "Epoch: 75, Loss: 0.5565243363380432\n",
      "Epoch: 76, Loss: 0.44578060507774353\n",
      "Epoch: 76, Loss: 0.3382176160812378\n",
      "Epoch: 77, Loss: 0.3873586356639862\n",
      "Epoch: 77, Loss: 0.4088074564933777\n",
      "Epoch: 78, Loss: 0.5083479285240173\n",
      "Epoch: 78, Loss: 0.535481333732605\n",
      "Epoch: 79, Loss: 0.4678947627544403\n",
      "Epoch: 79, Loss: 0.2886190712451935\n",
      "Epoch: 80, Loss: 0.36911678314208984\n",
      "Epoch: 80, Loss: 0.4028764069080353\n",
      "Epoch: 81, Loss: 0.30131399631500244\n",
      "Epoch: 81, Loss: 0.39933907985687256\n",
      "Epoch: 82, Loss: 0.46620872616767883\n",
      "Epoch: 82, Loss: 0.49823272228240967\n",
      "Epoch: 83, Loss: 0.6243444681167603\n",
      "Epoch: 83, Loss: 0.3668621778488159\n",
      "Epoch: 84, Loss: 0.3467177450656891\n",
      "Epoch: 84, Loss: 0.37783852219581604\n",
      "Epoch: 85, Loss: 0.34008321166038513\n",
      "Epoch: 85, Loss: 0.45831143856048584\n",
      "Epoch: 86, Loss: 0.2997090220451355\n",
      "Epoch: 86, Loss: 0.430896520614624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:20:09,833] Trial 17 finished with value: 0.41850195018724995 and parameters: {'hidden_size': 255, 'dropout': 0.35601213054255554, 'lr': 3.9754743505003223e-05}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 1.0394350290298462\n",
      "Epoch: 0, Loss: 0.4604789614677429\n",
      "Epoch: 1, Loss: 0.5222411155700684\n",
      "Epoch: 1, Loss: 0.5063695907592773\n",
      "Epoch: 2, Loss: 0.33699920773506165\n",
      "Epoch: 2, Loss: 0.3402629494667053\n",
      "Epoch: 3, Loss: 0.44299671053886414\n",
      "Epoch: 3, Loss: 0.4745674729347229\n",
      "Epoch: 4, Loss: 0.5447362661361694\n",
      "Epoch: 4, Loss: 0.39515700936317444\n",
      "Epoch: 5, Loss: 0.4409855306148529\n",
      "Epoch: 5, Loss: 0.3875155746936798\n",
      "Epoch: 6, Loss: 0.5399338006973267\n",
      "Epoch: 6, Loss: 0.3617687523365021\n",
      "Epoch: 7, Loss: 0.4323369264602661\n",
      "Epoch: 7, Loss: 0.38015416264533997\n",
      "Epoch: 8, Loss: 0.4102749526500702\n",
      "Epoch: 8, Loss: 0.412166565656662\n",
      "Epoch: 9, Loss: 0.45723938941955566\n",
      "Epoch: 9, Loss: 0.4470269978046417\n",
      "Epoch: 10, Loss: 0.4350980818271637\n",
      "Epoch: 10, Loss: 0.3483828902244568\n",
      "Epoch: 11, Loss: 0.5232032537460327\n",
      "Epoch: 11, Loss: 0.43492838740348816\n",
      "Epoch: 12, Loss: 0.5161138772964478\n",
      "Epoch: 12, Loss: 0.3723021447658539\n",
      "Epoch: 13, Loss: 0.44534051418304443\n",
      "Epoch: 13, Loss: 0.38655176758766174\n",
      "Epoch: 14, Loss: 0.5702619552612305\n",
      "Epoch: 14, Loss: 0.3891432583332062\n",
      "Epoch: 15, Loss: 0.4640185236930847\n",
      "Epoch: 15, Loss: 0.3297147750854492\n",
      "Epoch: 16, Loss: 0.45275989174842834\n",
      "Epoch: 16, Loss: 0.7621666789054871\n",
      "Epoch: 17, Loss: 0.4035261273384094\n",
      "Epoch: 17, Loss: 0.3155011832714081\n",
      "Epoch: 18, Loss: 0.4674592614173889\n",
      "Epoch: 18, Loss: 0.42418402433395386\n",
      "Epoch: 19, Loss: 0.38943496346473694\n",
      "Epoch: 19, Loss: 0.3934360444545746\n",
      "Epoch: 20, Loss: 0.4444766938686371\n",
      "Epoch: 20, Loss: 0.4753715693950653\n",
      "Epoch: 21, Loss: 0.5304625034332275\n",
      "Epoch: 21, Loss: 0.5486412644386292\n",
      "Epoch: 22, Loss: 0.3769208788871765\n",
      "Epoch: 22, Loss: 0.5271660685539246\n",
      "Epoch: 23, Loss: 0.296068012714386\n",
      "Epoch: 23, Loss: 0.2388681024312973\n",
      "Epoch: 24, Loss: 0.5275973677635193\n",
      "Epoch: 24, Loss: 0.4377855062484741\n",
      "Epoch: 25, Loss: 0.475018709897995\n",
      "Epoch: 25, Loss: 0.5100237131118774\n",
      "Epoch: 26, Loss: 0.49028438329696655\n",
      "Epoch: 26, Loss: 0.39985769987106323\n",
      "Epoch: 27, Loss: 0.451815664768219\n",
      "Epoch: 27, Loss: 0.5758600831031799\n",
      "Epoch: 28, Loss: 0.48160767555236816\n",
      "Epoch: 28, Loss: 0.6235727071762085\n",
      "Epoch: 29, Loss: 0.27578768134117126\n",
      "Epoch: 29, Loss: 0.4992971122264862\n",
      "Epoch: 30, Loss: 0.47184696793556213\n",
      "Epoch: 30, Loss: 0.35509994626045227\n",
      "Epoch: 31, Loss: 0.6130505800247192\n",
      "Epoch: 31, Loss: 0.44235745072364807\n",
      "Epoch: 32, Loss: 0.4337805509567261\n",
      "Epoch: 32, Loss: 0.3872917592525482\n",
      "Epoch: 33, Loss: 0.3664010763168335\n",
      "Epoch: 33, Loss: 0.49893221259117126\n",
      "Epoch: 34, Loss: 0.35209524631500244\n",
      "Epoch: 34, Loss: 0.35407543182373047\n",
      "Epoch: 35, Loss: 0.5094670653343201\n",
      "Epoch: 35, Loss: 0.33037808537483215\n",
      "Epoch: 36, Loss: 0.33230578899383545\n",
      "Epoch: 36, Loss: 0.42422208189964294\n",
      "Epoch: 37, Loss: 0.2792308032512665\n",
      "Epoch: 37, Loss: 0.4916455149650574\n",
      "Epoch: 38, Loss: 0.49793365597724915\n",
      "Epoch: 38, Loss: 0.32408666610717773\n",
      "Epoch: 39, Loss: 0.5112985372543335\n",
      "Epoch: 39, Loss: 0.524515688419342\n",
      "Epoch: 40, Loss: 0.6161947846412659\n",
      "Epoch: 40, Loss: 0.2603435516357422\n",
      "Epoch: 41, Loss: 0.38619428873062134\n",
      "Epoch: 41, Loss: 0.3885028660297394\n",
      "Epoch: 42, Loss: 0.3700661063194275\n",
      "Epoch: 42, Loss: 0.7050611972808838\n",
      "Epoch: 43, Loss: 0.5109654664993286\n",
      "Epoch: 43, Loss: 0.3721523880958557\n",
      "Epoch: 44, Loss: 0.4785313010215759\n",
      "Epoch: 44, Loss: 0.3697235882282257\n",
      "Epoch: 45, Loss: 0.4765579104423523\n",
      "Epoch: 45, Loss: 0.43216899037361145\n",
      "Epoch: 46, Loss: 0.3975686728954315\n",
      "Epoch: 46, Loss: 0.43331876397132874\n",
      "Epoch: 47, Loss: 0.35413858294487\n",
      "Epoch: 47, Loss: 0.40709954500198364\n",
      "Epoch: 48, Loss: 0.4991093575954437\n",
      "Epoch: 48, Loss: 0.5199834704399109\n",
      "Epoch: 49, Loss: 0.5107594728469849\n",
      "Epoch: 49, Loss: 0.37137556076049805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:20:25,438] Trial 18 finished with value: 0.4196419779005929 and parameters: {'hidden_size': 130, 'dropout': 0.34410806697581475, 'lr': 0.00016962200115256908}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6768295168876648\n",
      "Epoch: 0, Loss: 0.508387565612793\n",
      "Epoch: 1, Loss: 0.43392670154571533\n",
      "Epoch: 1, Loss: 0.4583619236946106\n",
      "Epoch: 2, Loss: 0.3269774615764618\n",
      "Epoch: 2, Loss: 0.3946947753429413\n",
      "Epoch: 3, Loss: 0.4364117980003357\n",
      "Epoch: 3, Loss: 0.4797314405441284\n",
      "Epoch: 4, Loss: 0.4203738570213318\n",
      "Epoch: 4, Loss: 0.5323182344436646\n",
      "Epoch: 5, Loss: 0.5503084659576416\n",
      "Epoch: 5, Loss: 0.5226340889930725\n",
      "Epoch: 6, Loss: 0.5286792516708374\n",
      "Epoch: 6, Loss: 0.32459861040115356\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:20:27,608] Trial 19 finished with value: 0.43414893620849954 and parameters: {'hidden_size': 147, 'dropout': 0.2579004953934265, 'lr': 0.0009916611999456172}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8292909264564514\n",
      "Epoch: 0, Loss: 0.429922491312027\n",
      "Epoch: 1, Loss: 0.4339311420917511\n",
      "Epoch: 1, Loss: 0.32926514744758606\n",
      "Epoch: 2, Loss: 0.46540623903274536\n",
      "Epoch: 2, Loss: 0.45221492648124695\n",
      "Epoch: 3, Loss: 0.5414474606513977\n",
      "Epoch: 3, Loss: 0.41236671805381775\n",
      "Epoch: 4, Loss: 0.4271396994590759\n",
      "Epoch: 4, Loss: 0.32143741846084595\n",
      "Epoch: 5, Loss: 0.4310992956161499\n",
      "Epoch: 5, Loss: 0.4188913106918335\n",
      "Epoch: 6, Loss: 0.4112606942653656\n",
      "Epoch: 6, Loss: 0.43109646439552307\n",
      "Epoch: 7, Loss: 0.3224967122077942\n",
      "Epoch: 7, Loss: 0.3936411738395691\n",
      "Epoch: 8, Loss: 0.33388498425483704\n",
      "Epoch: 8, Loss: 0.49881288409233093\n",
      "Epoch: 9, Loss: 0.48101845383644104\n",
      "Epoch: 9, Loss: 0.5091035962104797\n",
      "Epoch: 10, Loss: 0.4087529182434082\n",
      "Epoch: 10, Loss: 0.3012672960758209\n",
      "Epoch: 11, Loss: 0.4051114022731781\n",
      "Epoch: 11, Loss: 0.3836413323879242\n",
      "Epoch: 12, Loss: 0.4463704526424408\n",
      "Epoch: 12, Loss: 0.43438881635665894\n",
      "Epoch: 13, Loss: 0.5776848196983337\n",
      "Epoch: 13, Loss: 0.36110347509384155\n",
      "Epoch: 14, Loss: 0.51837557554245\n",
      "Epoch: 14, Loss: 0.5495858788490295\n",
      "Epoch: 15, Loss: 0.3863983154296875\n",
      "Epoch: 15, Loss: 0.43633612990379333\n",
      "Epoch: 16, Loss: 0.4952399730682373\n",
      "Epoch: 16, Loss: 0.35396280884742737\n",
      "Epoch: 17, Loss: 0.3312681317329407\n",
      "Epoch: 17, Loss: 0.42590636014938354\n",
      "Epoch: 18, Loss: 0.431105375289917\n",
      "Epoch: 18, Loss: 0.5003335475921631\n",
      "Epoch: 19, Loss: 0.36507856845855713\n",
      "Epoch: 19, Loss: 0.36802011728286743\n",
      "Epoch: 20, Loss: 0.30724677443504333\n",
      "Epoch: 20, Loss: 0.3250095546245575\n",
      "Epoch: 21, Loss: 0.4254686236381531\n",
      "Epoch: 21, Loss: 0.40686801075935364\n",
      "Epoch: 22, Loss: 0.3238263428211212\n",
      "Epoch: 22, Loss: 0.4104726314544678\n",
      "Epoch: 23, Loss: 0.44696924090385437\n",
      "Epoch: 23, Loss: 0.4144337773323059\n",
      "Epoch: 24, Loss: 0.32729658484458923\n",
      "Epoch: 24, Loss: 0.5079634189605713\n",
      "Epoch: 25, Loss: 0.5180810689926147\n",
      "Epoch: 25, Loss: 0.3542991876602173\n",
      "Epoch: 26, Loss: 0.39000022411346436\n",
      "Epoch: 26, Loss: 0.618261992931366\n",
      "Epoch: 27, Loss: 0.3445252776145935\n",
      "Epoch: 27, Loss: 0.4308684468269348\n",
      "Epoch: 28, Loss: 0.3523387014865875\n",
      "Epoch: 28, Loss: 0.32566845417022705\n",
      "Epoch: 29, Loss: 0.5138874053955078\n",
      "Epoch: 29, Loss: 0.42814791202545166\n",
      "Epoch: 30, Loss: 0.3645646274089813\n",
      "Epoch: 30, Loss: 0.4657658040523529\n",
      "Epoch: 31, Loss: 0.37210941314697266\n",
      "Epoch: 31, Loss: 0.4063047766685486\n",
      "Epoch: 32, Loss: 0.41719257831573486\n",
      "Epoch: 32, Loss: 0.42405638098716736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:20:37,057] Trial 20 finished with value: 0.41826087930268663 and parameters: {'hidden_size': 299, 'dropout': 0.19360571433233306, 'lr': 0.00011199542250545026}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.8109049797058105\n",
      "Epoch: 0, Loss: 0.6168668866157532\n",
      "Epoch: 1, Loss: 0.3748748302459717\n",
      "Epoch: 1, Loss: 0.46074461936950684\n",
      "Epoch: 2, Loss: 0.44627949595451355\n",
      "Epoch: 2, Loss: 0.4653921127319336\n",
      "Epoch: 3, Loss: 0.3499094247817993\n",
      "Epoch: 3, Loss: 0.4329628348350525\n",
      "Epoch: 4, Loss: 0.4212019741535187\n",
      "Epoch: 4, Loss: 0.4537171721458435\n",
      "Epoch: 5, Loss: 0.4209514558315277\n",
      "Epoch: 5, Loss: 0.3739659786224365\n",
      "Epoch: 6, Loss: 0.2852447032928467\n",
      "Epoch: 6, Loss: 0.3295736014842987\n",
      "Epoch: 7, Loss: 0.4518469274044037\n",
      "Epoch: 7, Loss: 0.327513724565506\n",
      "Epoch: 8, Loss: 0.430951863527298\n",
      "Epoch: 8, Loss: 0.33729439973831177\n",
      "Epoch: 9, Loss: 0.2970103919506073\n",
      "Epoch: 9, Loss: 0.5061845779418945\n",
      "Epoch: 10, Loss: 0.41651979088783264\n",
      "Epoch: 10, Loss: 0.41207563877105713\n",
      "Epoch: 11, Loss: 0.39881864190101624\n",
      "Epoch: 11, Loss: 0.41287124156951904\n",
      "Epoch: 12, Loss: 0.4964655041694641\n",
      "Epoch: 12, Loss: 0.5202757120132446\n",
      "Epoch: 13, Loss: 0.413693368434906\n",
      "Epoch: 13, Loss: 0.43647459149360657\n",
      "Epoch: 14, Loss: 0.4413319528102875\n",
      "Epoch: 14, Loss: 0.28919142484664917\n",
      "Epoch: 15, Loss: 0.485664427280426\n",
      "Epoch: 15, Loss: 0.32292428612709045\n",
      "Epoch: 16, Loss: 0.453694224357605\n",
      "Epoch: 16, Loss: 0.47882309556007385\n",
      "Epoch: 17, Loss: 0.3718929588794708\n",
      "Epoch: 17, Loss: 0.40515387058258057\n",
      "Epoch: 18, Loss: 0.44598355889320374\n",
      "Epoch: 18, Loss: 0.49440932273864746\n",
      "Epoch: 19, Loss: 0.4764939546585083\n",
      "Epoch: 19, Loss: 0.4731009900569916\n",
      "Epoch: 20, Loss: 0.27078431844711304\n",
      "Epoch: 20, Loss: 0.283303439617157\n",
      "Epoch: 21, Loss: 0.3727215826511383\n",
      "Epoch: 21, Loss: 0.44790276885032654\n",
      "Epoch: 22, Loss: 0.5741292834281921\n",
      "Epoch: 22, Loss: 0.4228934049606323\n",
      "Epoch: 23, Loss: 0.3798292577266693\n",
      "Epoch: 23, Loss: 0.37596091628074646\n",
      "Epoch: 24, Loss: 0.3174639046192169\n",
      "Epoch: 24, Loss: 0.41578322649002075\n",
      "Epoch: 25, Loss: 0.4093126952648163\n",
      "Epoch: 25, Loss: 0.45199987292289734\n",
      "Epoch: 26, Loss: 0.4912364184856415\n",
      "Epoch: 26, Loss: 0.39032188057899475\n",
      "Epoch: 27, Loss: 0.3987203538417816\n",
      "Epoch: 27, Loss: 0.599933922290802\n",
      "Epoch: 28, Loss: 0.4315986931324005\n",
      "Epoch: 28, Loss: 0.5155351758003235\n",
      "Epoch: 29, Loss: 0.435655802488327\n",
      "Epoch: 29, Loss: 0.3757339417934418\n",
      "Epoch: 30, Loss: 0.43226858973503113\n",
      "Epoch: 30, Loss: 0.40305042266845703\n",
      "Epoch: 31, Loss: 0.3812536299228668\n",
      "Epoch: 31, Loss: 0.3653510808944702\n",
      "Epoch: 32, Loss: 0.3933766782283783\n",
      "Epoch: 32, Loss: 0.26119521260261536\n",
      "Epoch: 33, Loss: 0.4950229823589325\n",
      "Epoch: 33, Loss: 0.3556280732154846\n",
      "Epoch: 34, Loss: 0.46391379833221436\n",
      "Epoch: 34, Loss: 0.32104381918907166\n",
      "Epoch: 35, Loss: 0.39159318804740906\n",
      "Epoch: 35, Loss: 0.4589369595050812\n",
      "Epoch: 36, Loss: 0.3252033293247223\n",
      "Epoch: 36, Loss: 0.33610519766807556\n",
      "Epoch: 37, Loss: 0.610474705696106\n",
      "Epoch: 37, Loss: 0.43365925550460815\n",
      "Epoch: 38, Loss: 0.36219677329063416\n",
      "Epoch: 38, Loss: 0.3737215995788574\n",
      "Epoch: 39, Loss: 0.4275490343570709\n",
      "Epoch: 39, Loss: 0.454806923866272\n",
      "Epoch: 40, Loss: 0.4419974088668823\n",
      "Epoch: 40, Loss: 0.37511172890663147\n",
      "Epoch: 41, Loss: 0.4127957820892334\n",
      "Epoch: 41, Loss: 0.5081586241722107\n",
      "Epoch: 42, Loss: 0.43843668699264526\n",
      "Epoch: 42, Loss: 0.47008419036865234\n",
      "Epoch: 43, Loss: 0.40227845311164856\n",
      "Epoch: 43, Loss: 0.36189934611320496\n",
      "Epoch: 44, Loss: 0.4314827620983124\n",
      "Epoch: 44, Loss: 0.6076790690422058\n",
      "Epoch: 45, Loss: 0.3629566729068756\n",
      "Epoch: 45, Loss: 0.4415770173072815\n",
      "Epoch: 46, Loss: 0.6084739565849304\n",
      "Epoch: 46, Loss: 0.39498868584632874\n",
      "Epoch: 47, Loss: 0.409206360578537\n",
      "Epoch: 47, Loss: 0.5136905312538147\n",
      "Epoch: 48, Loss: 0.4751836359500885\n",
      "Epoch: 48, Loss: 0.5111856460571289\n",
      "Epoch: 49, Loss: 0.45870187878608704\n",
      "Epoch: 49, Loss: 0.26158493757247925\n",
      "Epoch: 50, Loss: 0.35806456208229065\n",
      "Epoch: 50, Loss: 0.4927319884300232\n",
      "Epoch: 51, Loss: 0.5045912861824036\n",
      "Epoch: 51, Loss: 0.4036893844604492\n",
      "Epoch: 52, Loss: 0.44568976759910583\n",
      "Epoch: 52, Loss: 0.5027729868888855\n",
      "Epoch: 53, Loss: 0.40722373127937317\n",
      "Epoch: 53, Loss: 0.6623432040214539\n",
      "Epoch: 54, Loss: 0.32605063915252686\n",
      "Epoch: 54, Loss: 0.6889926791191101\n",
      "Epoch: 55, Loss: 0.4878876209259033\n",
      "Epoch: 55, Loss: 0.47302064299583435\n",
      "Epoch: 56, Loss: 0.3517356514930725\n",
      "Epoch: 56, Loss: 0.6017052531242371\n",
      "Epoch: 57, Loss: 0.4364151060581207\n",
      "Epoch: 57, Loss: 0.5190812945365906\n",
      "Epoch: 58, Loss: 0.44444188475608826\n",
      "Epoch: 58, Loss: 0.40170758962631226\n",
      "Epoch: 59, Loss: 0.3933107852935791\n",
      "Epoch: 59, Loss: 0.41912031173706055\n",
      "Epoch: 60, Loss: 0.588586151599884\n",
      "Epoch: 60, Loss: 0.41100040078163147\n",
      "Epoch: 61, Loss: 0.37294405698776245\n",
      "Epoch: 61, Loss: 0.6077719330787659\n",
      "Epoch: 62, Loss: 0.38474416732788086\n",
      "Epoch: 62, Loss: 0.4246636629104614\n",
      "Epoch: 63, Loss: 0.39216840267181396\n",
      "Epoch: 63, Loss: 0.26321905851364136\n",
      "Epoch: 64, Loss: 0.4510524570941925\n",
      "Epoch: 64, Loss: 0.4644846022129059\n",
      "Epoch: 65, Loss: 0.556246280670166\n",
      "Epoch: 65, Loss: 0.37066176533699036\n",
      "Epoch: 66, Loss: 0.32235783338546753\n",
      "Epoch: 66, Loss: 0.35858267545700073\n",
      "Epoch: 67, Loss: 0.41706427931785583\n",
      "Epoch: 67, Loss: 0.3877542018890381\n",
      "Epoch: 68, Loss: 0.3509046137332916\n",
      "Epoch: 68, Loss: 0.3242177963256836\n",
      "Epoch: 69, Loss: 0.251242071390152\n",
      "Epoch: 69, Loss: 0.5357349514961243\n",
      "Epoch: 70, Loss: 0.4731934368610382\n",
      "Epoch: 70, Loss: 0.4607207179069519\n",
      "Epoch: 71, Loss: 0.49288973212242126\n",
      "Epoch: 71, Loss: 0.3631364405155182\n",
      "Epoch: 72, Loss: 0.3111175000667572\n",
      "Epoch: 72, Loss: 0.5486868023872375\n",
      "Epoch: 73, Loss: 0.3368196189403534\n",
      "Epoch: 73, Loss: 0.438169926404953\n",
      "Epoch: 74, Loss: 0.25829020142555237\n",
      "Epoch: 74, Loss: 0.45080259442329407\n",
      "Epoch: 75, Loss: 0.2696612775325775\n",
      "Epoch: 75, Loss: 0.5364997386932373\n",
      "Epoch: 76, Loss: 0.4921130836009979\n",
      "Epoch: 76, Loss: 0.31101152300834656\n",
      "Epoch: 77, Loss: 0.29700562357902527\n",
      "Epoch: 77, Loss: 0.4654378294944763\n",
      "Epoch: 78, Loss: 0.4690288305282593\n",
      "Epoch: 78, Loss: 0.32662856578826904\n",
      "Epoch: 79, Loss: 0.5346790552139282\n",
      "Epoch: 79, Loss: 0.3924935758113861\n",
      "Epoch: 80, Loss: 0.4929102957248688\n",
      "Epoch: 80, Loss: 0.3866656720638275\n",
      "Epoch: 81, Loss: 0.439387708902359\n",
      "Epoch: 81, Loss: 0.5132809281349182\n",
      "Epoch: 82, Loss: 0.4078078866004944\n",
      "Epoch: 82, Loss: 0.332093745470047\n",
      "Epoch: 83, Loss: 0.5287958383560181\n",
      "Epoch: 83, Loss: 0.4266894459724426\n",
      "Epoch: 84, Loss: 0.4579518735408783\n",
      "Epoch: 84, Loss: 0.41372373700141907\n",
      "Epoch: 85, Loss: 0.39307522773742676\n",
      "Epoch: 85, Loss: 0.3514597713947296\n",
      "Epoch: 86, Loss: 0.52382892370224\n",
      "Epoch: 86, Loss: 0.5651423931121826\n",
      "Epoch: 87, Loss: 0.41030892729759216\n",
      "Epoch: 87, Loss: 0.33692458271980286\n",
      "Epoch: 88, Loss: 0.25940772891044617\n",
      "Epoch: 88, Loss: 0.4188983142375946\n",
      "Epoch: 89, Loss: 0.31754830479621887\n",
      "Epoch: 89, Loss: 0.45556727051734924\n",
      "Epoch: 90, Loss: 0.3751806914806366\n",
      "Epoch: 90, Loss: 0.4458830952644348\n",
      "Epoch: 91, Loss: 0.39557304978370667\n",
      "Epoch: 91, Loss: 0.5144971609115601\n",
      "Epoch: 92, Loss: 0.3470185101032257\n",
      "Epoch: 92, Loss: 0.48175036907196045\n",
      "Epoch: 93, Loss: 0.5286888480186462\n",
      "Epoch: 93, Loss: 0.3963998854160309\n",
      "Epoch: 94, Loss: 0.4992205798625946\n",
      "Epoch: 94, Loss: 0.3249245882034302\n",
      "Epoch: 95, Loss: 0.4415939450263977\n",
      "Epoch: 95, Loss: 0.338304728269577\n",
      "Epoch: 96, Loss: 0.38116782903671265\n",
      "Epoch: 96, Loss: 0.4970720410346985\n",
      "Epoch: 97, Loss: 0.5795065760612488\n",
      "Epoch: 97, Loss: 0.5641611814498901\n",
      "Epoch: 98, Loss: 0.3819121718406677\n",
      "Epoch: 98, Loss: 0.4445101022720337\n",
      "Epoch: 99, Loss: 0.4936240017414093\n",
      "Epoch: 99, Loss: 0.7543344497680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:21:04,124] Trial 21 finished with value: 0.41575329899440827 and parameters: {'hidden_size': 198, 'dropout': 0.39267410933713087, 'lr': 6.374683825134805e-05}. Best is trial 10 with value: 0.4133858236452414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8032262325286865\n",
      "Epoch: 0, Loss: 0.3141425848007202\n",
      "Epoch: 1, Loss: 0.37729305028915405\n",
      "Epoch: 1, Loss: 0.39470282196998596\n",
      "Epoch: 2, Loss: 0.34186044335365295\n",
      "Epoch: 2, Loss: 0.4157853126525879\n",
      "Epoch: 3, Loss: 0.3610039949417114\n",
      "Epoch: 3, Loss: 0.6648315787315369\n",
      "Epoch: 4, Loss: 0.40842992067337036\n",
      "Epoch: 4, Loss: 0.5006450414657593\n",
      "Epoch: 5, Loss: 0.5058852434158325\n",
      "Epoch: 5, Loss: 0.3078000247478485\n",
      "Epoch: 6, Loss: 0.42534199357032776\n",
      "Epoch: 6, Loss: 0.4482544958591461\n",
      "Epoch: 7, Loss: 0.24620138108730316\n",
      "Epoch: 7, Loss: 0.4118584394454956\n",
      "Epoch: 8, Loss: 0.4184364676475525\n",
      "Epoch: 8, Loss: 0.2751278579235077\n",
      "Epoch: 9, Loss: 0.7334849238395691\n",
      "Epoch: 9, Loss: 0.3377397954463959\n",
      "Epoch: 10, Loss: 0.48360028862953186\n",
      "Epoch: 10, Loss: 0.6481369733810425\n",
      "Epoch: 11, Loss: 0.415185809135437\n",
      "Epoch: 11, Loss: 0.37854310870170593\n",
      "Epoch: 12, Loss: 0.4779294431209564\n",
      "Epoch: 12, Loss: 0.3714950382709503\n",
      "Epoch: 13, Loss: 0.4046710729598999\n",
      "Epoch: 13, Loss: 0.4457545578479767\n",
      "Epoch: 14, Loss: 0.2605724036693573\n",
      "Epoch: 14, Loss: 0.4401203393936157\n",
      "Epoch: 15, Loss: 0.5982561111450195\n",
      "Epoch: 15, Loss: 0.2347688227891922\n",
      "Epoch: 16, Loss: 0.5184906721115112\n",
      "Epoch: 16, Loss: 0.3824717402458191\n",
      "Epoch: 17, Loss: 0.2451051026582718\n",
      "Epoch: 17, Loss: 0.43460753560066223\n",
      "Epoch: 18, Loss: 0.43663927912712097\n",
      "Epoch: 18, Loss: 0.3736013174057007\n",
      "Epoch: 19, Loss: 0.4441419541835785\n",
      "Epoch: 19, Loss: 0.3648884892463684\n",
      "Epoch: 20, Loss: 0.3612912893295288\n",
      "Epoch: 20, Loss: 0.3530101776123047\n",
      "Epoch: 21, Loss: 0.3273182511329651\n",
      "Epoch: 21, Loss: 0.5090202689170837\n",
      "Epoch: 22, Loss: 0.36255916953086853\n",
      "Epoch: 22, Loss: 0.4117518365383148\n",
      "Epoch: 23, Loss: 0.43765872716903687\n",
      "Epoch: 23, Loss: 0.4541674852371216\n",
      "Epoch: 24, Loss: 0.570128858089447\n",
      "Epoch: 24, Loss: 0.400479257106781\n",
      "Epoch: 25, Loss: 0.3467920422554016\n",
      "Epoch: 25, Loss: 0.44546157121658325\n",
      "Epoch: 26, Loss: 0.42926591634750366\n",
      "Epoch: 26, Loss: 0.42839866876602173\n",
      "Epoch: 27, Loss: 0.4459317624568939\n",
      "Epoch: 27, Loss: 0.383230596780777\n",
      "Epoch: 28, Loss: 0.47992217540740967\n",
      "Epoch: 28, Loss: 0.4527811110019684\n",
      "Epoch: 29, Loss: 0.304887980222702\n",
      "Epoch: 29, Loss: 0.5011597275733948\n",
      "Epoch: 30, Loss: 0.4004998803138733\n",
      "Epoch: 30, Loss: 0.24633674323558807\n",
      "Epoch: 31, Loss: 0.3987087905406952\n",
      "Epoch: 31, Loss: 0.3770199120044708\n",
      "Epoch: 32, Loss: 0.5581161975860596\n",
      "Epoch: 32, Loss: 0.4163869023323059\n",
      "Epoch: 33, Loss: 0.44499120116233826\n",
      "Epoch: 33, Loss: 0.46324536204338074\n",
      "Epoch: 34, Loss: 0.45187947154045105\n",
      "Epoch: 34, Loss: 0.3321462869644165\n",
      "Epoch: 35, Loss: 0.4934942126274109\n",
      "Epoch: 35, Loss: 0.4288598299026489\n",
      "Epoch: 36, Loss: 0.46596524119377136\n",
      "Epoch: 36, Loss: 0.4158329367637634\n",
      "Epoch: 37, Loss: 0.3955487906932831\n",
      "Epoch: 37, Loss: 0.2941301167011261\n",
      "Epoch: 38, Loss: 0.5089644193649292\n",
      "Epoch: 38, Loss: 0.4715832471847534\n",
      "Epoch: 39, Loss: 0.38787028193473816\n",
      "Epoch: 39, Loss: 0.32840073108673096\n",
      "Epoch: 40, Loss: 0.42179033160209656\n",
      "Epoch: 40, Loss: 0.48509353399276733\n",
      "Epoch: 41, Loss: 0.3338402807712555\n",
      "Epoch: 41, Loss: 0.5224042534828186\n",
      "Epoch: 42, Loss: 0.4018278121948242\n",
      "Epoch: 42, Loss: 0.5468553900718689\n",
      "Epoch: 43, Loss: 0.5125784873962402\n",
      "Epoch: 43, Loss: 0.45811715722084045\n",
      "Epoch: 44, Loss: 0.3985975682735443\n",
      "Epoch: 44, Loss: 0.48184531927108765\n",
      "Epoch: 45, Loss: 0.34004002809524536\n",
      "Epoch: 45, Loss: 0.3581065237522125\n",
      "Epoch: 46, Loss: 0.4242075979709625\n",
      "Epoch: 46, Loss: 0.25031667947769165\n",
      "Epoch: 47, Loss: 0.2526394724845886\n",
      "Epoch: 47, Loss: 0.42512717843055725\n",
      "Epoch: 48, Loss: 0.38930878043174744\n",
      "Epoch: 48, Loss: 0.32645297050476074\n",
      "Epoch: 49, Loss: 0.5523338913917542\n",
      "Epoch: 49, Loss: 0.4622907340526581\n",
      "Epoch: 50, Loss: 0.3513137698173523\n",
      "Epoch: 50, Loss: 0.5978190302848816\n",
      "Epoch: 51, Loss: 0.2873062193393707\n",
      "Epoch: 51, Loss: 0.44967347383499146\n",
      "Epoch: 52, Loss: 0.3645462393760681\n",
      "Epoch: 52, Loss: 0.4555036425590515\n",
      "Epoch: 53, Loss: 0.4458767771720886\n",
      "Epoch: 53, Loss: 0.4239121079444885\n",
      "Epoch: 54, Loss: 0.4443519711494446\n",
      "Epoch: 54, Loss: 0.5860803127288818\n",
      "Epoch: 55, Loss: 0.453662633895874\n",
      "Epoch: 55, Loss: 0.3900704085826874\n",
      "Epoch: 56, Loss: 0.3822775185108185\n",
      "Epoch: 56, Loss: 0.38187375664711\n",
      "Epoch: 57, Loss: 0.39016246795654297\n",
      "Epoch: 57, Loss: 0.3573661744594574\n",
      "Epoch: 58, Loss: 0.6444157958030701\n",
      "Epoch: 58, Loss: 0.3408641815185547\n",
      "Epoch: 59, Loss: 0.4962456226348877\n",
      "Epoch: 59, Loss: 0.32422807812690735\n",
      "Epoch: 60, Loss: 0.3231959342956543\n",
      "Epoch: 60, Loss: 0.41073083877563477\n",
      "Epoch: 61, Loss: 0.3842107355594635\n",
      "Epoch: 61, Loss: 0.326094388961792\n",
      "Epoch: 62, Loss: 0.36012518405914307\n",
      "Epoch: 62, Loss: 0.4054986238479614\n",
      "Epoch: 63, Loss: 0.344728946685791\n",
      "Epoch: 63, Loss: 0.49987587332725525\n",
      "Epoch: 64, Loss: 0.48367607593536377\n",
      "Epoch: 64, Loss: 0.31244584918022156\n",
      "Epoch: 65, Loss: 0.5146968960762024\n",
      "Epoch: 65, Loss: 0.4269220530986786\n",
      "Epoch: 66, Loss: 0.32650676369667053\n",
      "Epoch: 66, Loss: 0.5862868428230286\n",
      "Epoch: 67, Loss: 0.4226236641407013\n",
      "Epoch: 67, Loss: 0.6557665467262268\n",
      "Epoch: 68, Loss: 0.3809802532196045\n",
      "Epoch: 68, Loss: 0.3478628098964691\n",
      "Epoch: 69, Loss: 0.46316227316856384\n",
      "Epoch: 69, Loss: 0.41406872868537903\n",
      "Epoch: 70, Loss: 0.2585044801235199\n",
      "Epoch: 70, Loss: 0.4449236989021301\n",
      "Epoch: 71, Loss: 0.31443822383880615\n",
      "Epoch: 71, Loss: 0.5382961630821228\n",
      "Epoch: 72, Loss: 0.33064600825309753\n",
      "Epoch: 72, Loss: 0.32238322496414185\n",
      "Epoch: 73, Loss: 0.3842266798019409\n",
      "Epoch: 73, Loss: 0.4443259835243225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:21:24,049] Trial 22 finished with value: 0.41142106168145626 and parameters: {'hidden_size': 185, 'dropout': 0.3267140666269757, 'lr': 0.00025937129423859296}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6184343099594116\n",
      "Epoch: 0, Loss: 0.43344083428382874\n",
      "Epoch: 1, Loss: 0.3750566840171814\n",
      "Epoch: 1, Loss: 0.41125452518463135\n",
      "Epoch: 2, Loss: 0.4656153619289398\n",
      "Epoch: 2, Loss: 0.4806395173072815\n",
      "Epoch: 3, Loss: 0.402471661567688\n",
      "Epoch: 3, Loss: 0.29434001445770264\n",
      "Epoch: 4, Loss: 0.532741904258728\n",
      "Epoch: 4, Loss: 0.3713465631008148\n",
      "Epoch: 5, Loss: 0.3573569059371948\n",
      "Epoch: 5, Loss: 0.4249607026576996\n",
      "Epoch: 6, Loss: 0.35871145129203796\n",
      "Epoch: 6, Loss: 0.40845078229904175\n",
      "Epoch: 7, Loss: 0.28790926933288574\n",
      "Epoch: 7, Loss: 0.514184832572937\n",
      "Epoch: 8, Loss: 0.4576171636581421\n",
      "Epoch: 8, Loss: 0.4103585183620453\n",
      "Epoch: 9, Loss: 0.5668452978134155\n",
      "Epoch: 9, Loss: 0.344501793384552\n",
      "Epoch: 10, Loss: 0.46131715178489685\n",
      "Epoch: 10, Loss: 0.49834614992141724\n",
      "Epoch: 11, Loss: 0.3748658299446106\n",
      "Epoch: 11, Loss: 0.38405895233154297\n",
      "Epoch: 12, Loss: 0.4230014681816101\n",
      "Epoch: 12, Loss: 0.39076921343803406\n",
      "Epoch: 13, Loss: 0.4086211323738098\n",
      "Epoch: 13, Loss: 0.3594479560852051\n",
      "Epoch: 14, Loss: 0.4420519769191742\n",
      "Epoch: 14, Loss: 0.2777915596961975\n",
      "Epoch: 15, Loss: 0.5085801482200623\n",
      "Epoch: 15, Loss: 0.33005547523498535\n",
      "Epoch: 16, Loss: 0.3459504544734955\n",
      "Epoch: 16, Loss: 0.475867360830307\n",
      "Epoch: 17, Loss: 0.4551248550415039\n",
      "Epoch: 17, Loss: 0.342287540435791\n",
      "Epoch: 18, Loss: 0.4447272717952728\n",
      "Epoch: 18, Loss: 0.5329779982566833\n",
      "Epoch: 19, Loss: 0.38308611512184143\n",
      "Epoch: 19, Loss: 0.5614569783210754\n",
      "Epoch: 20, Loss: 0.45351120829582214\n",
      "Epoch: 20, Loss: 0.3221650719642639\n",
      "Epoch: 21, Loss: 0.4164758324623108\n",
      "Epoch: 21, Loss: 0.5764495134353638\n",
      "Epoch: 22, Loss: 0.5165359973907471\n",
      "Epoch: 22, Loss: 0.4901072680950165\n",
      "Epoch: 23, Loss: 0.44133174419403076\n",
      "Epoch: 23, Loss: 0.4241332411766052\n",
      "Epoch: 24, Loss: 0.4043538272380829\n",
      "Epoch: 24, Loss: 0.38387051224708557\n",
      "Epoch: 25, Loss: 0.3939628601074219\n",
      "Epoch: 25, Loss: 0.39094316959381104\n",
      "Epoch: 26, Loss: 0.5573429465293884\n",
      "Epoch: 26, Loss: 0.4771106243133545\n",
      "Epoch: 27, Loss: 0.32694658637046814\n",
      "Epoch: 27, Loss: 0.4017240107059479\n",
      "Epoch: 28, Loss: 0.4427591562271118\n",
      "Epoch: 28, Loss: 0.39718255400657654\n",
      "Epoch: 29, Loss: 0.39765119552612305\n",
      "Epoch: 29, Loss: 0.4363625943660736\n",
      "Epoch: 30, Loss: 0.6052854061126709\n",
      "Epoch: 30, Loss: 0.38033631443977356\n",
      "Epoch: 31, Loss: 0.4574185013771057\n",
      "Epoch: 31, Loss: 0.5061653852462769\n",
      "Epoch: 32, Loss: 0.3631764054298401\n",
      "Epoch: 32, Loss: 0.5043150186538696\n",
      "Epoch: 33, Loss: 0.5061721801757812\n",
      "Epoch: 33, Loss: 0.29444554448127747\n",
      "Epoch: 34, Loss: 0.5279203653335571\n",
      "Epoch: 34, Loss: 0.4511210024356842\n",
      "Epoch: 35, Loss: 0.3065560460090637\n",
      "Epoch: 35, Loss: 0.4384443163871765\n",
      "Epoch: 36, Loss: 0.3258020579814911\n",
      "Epoch: 36, Loss: 0.5523241758346558\n",
      "Epoch: 37, Loss: 0.31769663095474243\n",
      "Epoch: 37, Loss: 0.3366522192955017\n",
      "Epoch: 38, Loss: 0.41702261567115784\n",
      "Epoch: 38, Loss: 0.41011252999305725\n",
      "Epoch: 39, Loss: 0.34766533970832825\n",
      "Epoch: 39, Loss: 0.4820460379123688\n",
      "Epoch: 40, Loss: 0.41048863530158997\n",
      "Epoch: 40, Loss: 0.38102102279663086\n",
      "Epoch: 41, Loss: 0.4548789858818054\n",
      "Epoch: 41, Loss: 0.5154067277908325\n",
      "Epoch: 42, Loss: 0.3822890520095825\n",
      "Epoch: 42, Loss: 0.360697865486145\n",
      "Epoch: 43, Loss: 0.27939414978027344\n",
      "Epoch: 43, Loss: 0.4466765522956848\n",
      "Epoch: 44, Loss: 0.33661675453186035\n",
      "Epoch: 44, Loss: 0.47117316722869873\n",
      "Epoch: 45, Loss: 0.5470662713050842\n",
      "Epoch: 45, Loss: 0.4918941557407379\n",
      "Epoch: 46, Loss: 0.4913139343261719\n",
      "Epoch: 46, Loss: 0.499994695186615\n",
      "Epoch: 47, Loss: 0.34931379556655884\n",
      "Epoch: 47, Loss: 0.4183170199394226\n",
      "Epoch: 48, Loss: 0.6145350337028503\n",
      "Epoch: 48, Loss: 0.32826727628707886\n",
      "Epoch: 49, Loss: 0.43334725499153137\n",
      "Epoch: 49, Loss: 0.36284226179122925\n",
      "Epoch: 50, Loss: 0.4615156650543213\n",
      "Epoch: 50, Loss: 0.4368847906589508\n",
      "Epoch: 51, Loss: 0.310891330242157\n",
      "Epoch: 51, Loss: 0.356925368309021\n",
      "Epoch: 52, Loss: 0.3601369261741638\n",
      "Epoch: 52, Loss: 0.3738422095775604\n",
      "Epoch: 53, Loss: 0.5564501285552979\n",
      "Epoch: 53, Loss: 0.42770683765411377\n",
      "Epoch: 54, Loss: 0.2955463230609894\n",
      "Epoch: 54, Loss: 0.5915651917457581\n",
      "Epoch: 55, Loss: 0.4213581383228302\n",
      "Epoch: 55, Loss: 0.41709545254707336\n",
      "Epoch: 56, Loss: 0.42264553904533386\n",
      "Epoch: 56, Loss: 0.3016115725040436\n",
      "Epoch: 57, Loss: 0.39199498295783997\n",
      "Epoch: 57, Loss: 0.33174458146095276\n",
      "Epoch: 58, Loss: 0.4153521656990051\n",
      "Epoch: 58, Loss: 0.32728520035743713\n",
      "Epoch: 59, Loss: 0.3305782675743103\n",
      "Epoch: 59, Loss: 0.45569634437561035\n",
      "Epoch: 60, Loss: 0.5449048280715942\n",
      "Epoch: 60, Loss: 0.3754867613315582\n",
      "Epoch: 61, Loss: 0.3670770227909088\n",
      "Epoch: 61, Loss: 0.48076122999191284\n",
      "Epoch: 62, Loss: 0.3114534616470337\n",
      "Epoch: 62, Loss: 0.4903634190559387\n",
      "Epoch: 63, Loss: 0.5968227982521057\n",
      "Epoch: 63, Loss: 0.324968159198761\n",
      "Epoch: 64, Loss: 0.3986354470252991\n",
      "Epoch: 64, Loss: 0.37620964646339417\n",
      "Epoch: 65, Loss: 0.4943525195121765\n",
      "Epoch: 65, Loss: 0.43212637305259705\n",
      "Epoch: 66, Loss: 0.46453556418418884\n",
      "Epoch: 66, Loss: 0.43654918670654297\n",
      "Epoch: 67, Loss: 0.38377878069877625\n",
      "Epoch: 67, Loss: 0.2826589345932007\n",
      "Epoch: 68, Loss: 0.384890615940094\n",
      "Epoch: 68, Loss: 0.4819476008415222\n",
      "Epoch: 69, Loss: 0.44738781452178955\n",
      "Epoch: 69, Loss: 0.34270572662353516\n",
      "Epoch: 70, Loss: 0.36065366864204407\n",
      "Epoch: 70, Loss: 0.2881997525691986\n",
      "Epoch: 71, Loss: 0.4805651903152466\n",
      "Epoch: 71, Loss: 0.46301791071891785\n",
      "Epoch: 72, Loss: 0.5316299796104431\n",
      "Epoch: 72, Loss: 0.5568098425865173\n",
      "Epoch: 73, Loss: 0.3934674561023712\n",
      "Epoch: 73, Loss: 0.46091365814208984\n",
      "Epoch: 74, Loss: 0.6042625308036804\n",
      "Epoch: 74, Loss: 0.24217452108860016\n",
      "Epoch: 75, Loss: 0.4237127900123596\n",
      "Epoch: 75, Loss: 0.36355072259902954\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:21:46,249] Trial 23 finished with value: 0.417357018402552 and parameters: {'hidden_size': 96, 'dropout': 0.3254157799250297, 'lr': 0.0003086937136482746}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8564077019691467\n",
      "Epoch: 0, Loss: 0.323379248380661\n",
      "Epoch: 1, Loss: 0.5548016428947449\n",
      "Epoch: 1, Loss: 0.5820204019546509\n",
      "Epoch: 2, Loss: 0.549239456653595\n",
      "Epoch: 2, Loss: 0.5641719102859497\n",
      "Epoch: 3, Loss: 0.5791448950767517\n",
      "Epoch: 3, Loss: 0.5066545009613037\n",
      "Epoch: 4, Loss: 0.4450621008872986\n",
      "Epoch: 4, Loss: 0.2919240891933441\n",
      "Epoch: 5, Loss: 0.36084654927253723\n",
      "Epoch: 5, Loss: 0.4436222314834595\n",
      "Epoch: 6, Loss: 0.476350337266922\n",
      "Epoch: 6, Loss: 0.48477649688720703\n",
      "Epoch: 7, Loss: 0.19874954223632812\n",
      "Epoch: 7, Loss: 0.41563859581947327\n",
      "Epoch: 8, Loss: 0.2670953869819641\n",
      "Epoch: 8, Loss: 0.5392099618911743\n",
      "Epoch: 9, Loss: 0.311025470495224\n",
      "Epoch: 9, Loss: 0.343886137008667\n",
      "Epoch: 10, Loss: 0.36433443427085876\n",
      "Epoch: 10, Loss: 0.45127129554748535\n",
      "Epoch: 11, Loss: 0.3764741122722626\n",
      "Epoch: 11, Loss: 0.511155366897583\n",
      "Epoch: 12, Loss: 0.4609072506427765\n",
      "Epoch: 12, Loss: 0.39142903685569763\n",
      "Epoch: 13, Loss: 0.568294107913971\n",
      "Epoch: 13, Loss: 0.49619144201278687\n",
      "Epoch: 14, Loss: 0.5591863393783569\n",
      "Epoch: 14, Loss: 0.36574727296829224\n",
      "Epoch: 15, Loss: 0.5657740831375122\n",
      "Epoch: 15, Loss: 0.5437840223312378\n",
      "Epoch: 16, Loss: 0.5506100058555603\n",
      "Epoch: 16, Loss: 0.49157536029815674\n",
      "Epoch: 17, Loss: 0.48536524176597595\n",
      "Epoch: 17, Loss: 0.4403398633003235\n",
      "Epoch: 18, Loss: 0.3957597613334656\n",
      "Epoch: 18, Loss: 0.39113739132881165\n",
      "Epoch: 19, Loss: 0.37110185623168945\n",
      "Epoch: 19, Loss: 0.2933180332183838\n",
      "Epoch: 20, Loss: 0.41351011395454407\n",
      "Epoch: 20, Loss: 0.38612958788871765\n",
      "Epoch: 21, Loss: 0.3788672387599945\n",
      "Epoch: 21, Loss: 0.4502819776535034\n",
      "Epoch: 22, Loss: 0.5363528728485107\n",
      "Epoch: 22, Loss: 0.30686479806900024\n",
      "Epoch: 23, Loss: 0.4749220907688141\n",
      "Epoch: 23, Loss: 0.4080014228820801\n",
      "Epoch: 24, Loss: 0.2029857337474823\n",
      "Epoch: 24, Loss: 0.4285438358783722\n",
      "Epoch: 25, Loss: 0.3123954236507416\n",
      "Epoch: 25, Loss: 0.38966137170791626\n",
      "Epoch: 26, Loss: 0.3636398911476135\n",
      "Epoch: 26, Loss: 0.3901289999485016\n",
      "Epoch: 27, Loss: 0.4282416105270386\n",
      "Epoch: 27, Loss: 0.3784823715686798\n",
      "Epoch: 28, Loss: 0.33458828926086426\n",
      "Epoch: 28, Loss: 0.38267016410827637\n",
      "Epoch: 29, Loss: 0.40601474046707153\n",
      "Epoch: 29, Loss: 0.32172074913978577\n",
      "Epoch: 30, Loss: 0.5992447137832642\n",
      "Epoch: 30, Loss: 0.47765570878982544\n",
      "Epoch: 31, Loss: 0.5242003202438354\n",
      "Epoch: 31, Loss: 0.5176855325698853\n",
      "Epoch: 32, Loss: 0.41384759545326233\n",
      "Epoch: 32, Loss: 0.39434677362442017\n",
      "Epoch: 33, Loss: 0.44811275601387024\n",
      "Epoch: 33, Loss: 0.44654563069343567\n",
      "Epoch: 34, Loss: 0.35033777356147766\n",
      "Epoch: 34, Loss: 0.5089021325111389\n",
      "Epoch: 35, Loss: 0.37071114778518677\n",
      "Epoch: 35, Loss: 0.3688681423664093\n",
      "Epoch: 36, Loss: 0.5741562247276306\n",
      "Epoch: 36, Loss: 0.298782080411911\n",
      "Epoch: 37, Loss: 0.5147934556007385\n",
      "Epoch: 37, Loss: 0.49632528424263\n",
      "Epoch: 38, Loss: 0.4532366693019867\n",
      "Epoch: 38, Loss: 0.4320809543132782\n",
      "Epoch: 39, Loss: 0.3761827349662781\n",
      "Epoch: 39, Loss: 0.4630553126335144\n",
      "Epoch: 40, Loss: 0.3457346558570862\n",
      "Epoch: 40, Loss: 0.36519771814346313\n",
      "Epoch: 41, Loss: 0.4482976794242859\n",
      "Epoch: 41, Loss: 0.33086609840393066\n",
      "Epoch: 42, Loss: 0.32777923345565796\n",
      "Epoch: 42, Loss: 0.5293728113174438\n",
      "Epoch: 43, Loss: 0.5426687598228455\n",
      "Epoch: 43, Loss: 0.4630766212940216\n",
      "Epoch: 44, Loss: 0.4102078676223755\n",
      "Epoch: 44, Loss: 0.2957876920700073\n",
      "Epoch: 45, Loss: 0.3847978711128235\n",
      "Epoch: 45, Loss: 0.35390952229499817\n",
      "Epoch: 46, Loss: 0.3731054961681366\n",
      "Epoch: 46, Loss: 0.31646299362182617\n",
      "Epoch: 47, Loss: 0.3625420331954956\n",
      "Epoch: 47, Loss: 0.6276566386222839\n",
      "Epoch: 48, Loss: 0.39720579981803894\n",
      "Epoch: 48, Loss: 0.364126592874527\n",
      "Epoch: 49, Loss: 0.346395879983902\n",
      "Epoch: 49, Loss: 0.5222730040550232\n",
      "Epoch: 50, Loss: 0.4201675057411194\n",
      "Epoch: 50, Loss: 0.39549821615219116\n",
      "Epoch: 51, Loss: 0.299852192401886\n",
      "Epoch: 51, Loss: 0.38836920261383057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:02,907] Trial 24 finished with value: 0.41294921435754145 and parameters: {'hidden_size': 277, 'dropout': 0.27697295830138785, 'lr': 0.000207125195692809}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7757750749588013\n",
      "Epoch: 0, Loss: 0.38768941164016724\n",
      "Epoch: 1, Loss: 0.4543401896953583\n",
      "Epoch: 1, Loss: 0.31761810183525085\n",
      "Epoch: 2, Loss: 0.6771405935287476\n",
      "Epoch: 2, Loss: 0.4091896712779999\n",
      "Epoch: 3, Loss: 0.43493571877479553\n",
      "Epoch: 3, Loss: 0.3314876854419708\n",
      "Epoch: 4, Loss: 0.44715195894241333\n",
      "Epoch: 4, Loss: 0.3209998607635498\n",
      "Epoch: 5, Loss: 0.4653337597846985\n",
      "Epoch: 5, Loss: 0.4066220819950104\n",
      "Epoch: 6, Loss: 0.3484858274459839\n",
      "Epoch: 6, Loss: 0.38941508531570435\n",
      "Epoch: 7, Loss: 0.48485222458839417\n",
      "Epoch: 7, Loss: 0.45695385336875916\n",
      "Epoch: 8, Loss: 0.27122926712036133\n",
      "Epoch: 8, Loss: 0.5170010924339294\n",
      "Epoch: 9, Loss: 0.31885790824890137\n",
      "Epoch: 9, Loss: 0.41508740186691284\n",
      "Epoch: 10, Loss: 0.5154159069061279\n",
      "Epoch: 10, Loss: 0.4006732702255249\n",
      "Epoch: 11, Loss: 0.44473037123680115\n",
      "Epoch: 11, Loss: 0.3807721734046936\n",
      "Epoch: 12, Loss: 0.3713129460811615\n",
      "Epoch: 12, Loss: 0.38012877106666565\n",
      "Epoch: 13, Loss: 0.361144095659256\n",
      "Epoch: 13, Loss: 0.4052940309047699\n",
      "Epoch: 14, Loss: 0.40880507230758667\n",
      "Epoch: 14, Loss: 0.38456282019615173\n",
      "Epoch: 15, Loss: 0.3687044382095337\n",
      "Epoch: 15, Loss: 0.35207444429397583\n",
      "Epoch: 16, Loss: 0.3439162075519562\n",
      "Epoch: 16, Loss: 0.4913676977157593\n",
      "Epoch: 17, Loss: 0.5534034967422485\n",
      "Epoch: 17, Loss: 0.4733050763607025\n",
      "Epoch: 18, Loss: 0.43350163102149963\n",
      "Epoch: 18, Loss: 0.3188515603542328\n",
      "Epoch: 19, Loss: 0.46689531207084656\n",
      "Epoch: 19, Loss: 0.6832990646362305\n",
      "Epoch: 20, Loss: 0.3722345232963562\n",
      "Epoch: 20, Loss: 0.4876909852027893\n",
      "Epoch: 21, Loss: 0.48016029596328735\n",
      "Epoch: 21, Loss: 0.4783959686756134\n",
      "Epoch: 22, Loss: 0.47576838731765747\n",
      "Epoch: 22, Loss: 0.4667130410671234\n",
      "Epoch: 23, Loss: 0.5224491953849792\n",
      "Epoch: 23, Loss: 0.4271829426288605\n",
      "Epoch: 24, Loss: 0.4916558563709259\n",
      "Epoch: 24, Loss: 0.35653677582740784\n",
      "Epoch: 25, Loss: 0.48007920384407043\n",
      "Epoch: 25, Loss: 0.49666669964790344\n",
      "Epoch: 26, Loss: 0.40793389081954956\n",
      "Epoch: 26, Loss: 0.40928253531455994\n",
      "Epoch: 27, Loss: 0.4645692706108093\n",
      "Epoch: 27, Loss: 0.4702104926109314\n",
      "Epoch: 28, Loss: 0.5630025267601013\n",
      "Epoch: 28, Loss: 0.4193963408470154\n",
      "Epoch: 29, Loss: 0.4216189682483673\n",
      "Epoch: 29, Loss: 0.41486459970474243\n",
      "Epoch: 30, Loss: 0.4654816687107086\n",
      "Epoch: 30, Loss: 0.49161621928215027\n",
      "Epoch: 31, Loss: 0.3095617890357971\n",
      "Epoch: 31, Loss: 0.37889185547828674\n",
      "Epoch: 32, Loss: 0.41067302227020264\n",
      "Epoch: 32, Loss: 0.5676649808883667\n",
      "Epoch: 33, Loss: 0.3777361214160919\n",
      "Epoch: 33, Loss: 0.4003135859966278\n",
      "Epoch: 34, Loss: 0.4266805946826935\n",
      "Epoch: 34, Loss: 0.505598783493042\n",
      "Epoch: 35, Loss: 0.18080714344978333\n",
      "Epoch: 35, Loss: 0.6442893147468567\n",
      "Epoch: 36, Loss: 0.49248144030570984\n",
      "Epoch: 36, Loss: 0.4536743760108948\n",
      "Epoch: 37, Loss: 0.3654822111129761\n",
      "Epoch: 37, Loss: 0.4610021710395813\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:14,274] Trial 25 finished with value: 0.41580875423268315 and parameters: {'hidden_size': 242, 'dropout': 0.27019300482923075, 'lr': 0.0003288789376053345}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7891616225242615\n",
      "Epoch: 0, Loss: 0.4553638696670532\n",
      "Epoch: 1, Loss: 0.5105327367782593\n",
      "Epoch: 1, Loss: 0.4248303174972534\n",
      "Epoch: 2, Loss: 0.4320656657218933\n",
      "Epoch: 2, Loss: 0.42627236247062683\n",
      "Epoch: 3, Loss: 0.39269211888313293\n",
      "Epoch: 3, Loss: 0.37754884362220764\n",
      "Epoch: 4, Loss: 0.4886242151260376\n",
      "Epoch: 4, Loss: 0.3955436050891876\n",
      "Epoch: 5, Loss: 0.46965351700782776\n",
      "Epoch: 5, Loss: 0.5213238000869751\n",
      "Epoch: 6, Loss: 0.6011773943901062\n",
      "Epoch: 6, Loss: 0.5202828049659729\n",
      "Epoch: 7, Loss: 0.7041504383087158\n",
      "Epoch: 7, Loss: 0.46777766942977905\n",
      "Epoch: 8, Loss: 0.46657028794288635\n",
      "Epoch: 8, Loss: 0.41758033633232117\n",
      "Epoch: 9, Loss: 0.3835514783859253\n",
      "Epoch: 9, Loss: 0.3528725802898407\n",
      "Epoch: 10, Loss: 0.47046980261802673\n",
      "Epoch: 10, Loss: 0.4200840890407562\n",
      "Epoch: 11, Loss: 0.314089834690094\n",
      "Epoch: 11, Loss: 0.41895362734794617\n",
      "Epoch: 12, Loss: 0.4863075315952301\n",
      "Epoch: 12, Loss: 0.43841153383255005\n",
      "Epoch: 13, Loss: 0.29905855655670166\n",
      "Epoch: 13, Loss: 0.4761866629123688\n",
      "Epoch: 14, Loss: 0.4615679681301117\n",
      "Epoch: 14, Loss: 0.3561933636665344\n",
      "Epoch: 15, Loss: 0.2930000424385071\n",
      "Epoch: 15, Loss: 0.43164604902267456\n",
      "Epoch: 16, Loss: 0.5425309538841248\n",
      "Epoch: 16, Loss: 0.34896478056907654\n",
      "Epoch: 17, Loss: 0.51962810754776\n",
      "Epoch: 17, Loss: 0.5043749213218689\n",
      "Epoch: 18, Loss: 0.38302525877952576\n",
      "Epoch: 18, Loss: 0.4715918302536011\n",
      "Epoch: 19, Loss: 0.26938340067863464\n",
      "Epoch: 19, Loss: 0.41115841269493103\n",
      "Epoch: 20, Loss: 0.3490283787250519\n",
      "Epoch: 20, Loss: 0.36875981092453003\n",
      "Epoch: 21, Loss: 0.4173891246318817\n",
      "Epoch: 21, Loss: 0.35803595185279846\n",
      "Epoch: 22, Loss: 0.39750489592552185\n",
      "Epoch: 22, Loss: 0.41130152344703674\n",
      "Epoch: 23, Loss: 0.450603187084198\n",
      "Epoch: 23, Loss: 0.4448853135108948\n",
      "Epoch: 24, Loss: 0.44230177998542786\n",
      "Epoch: 24, Loss: 0.29688289761543274\n",
      "Epoch: 25, Loss: 0.36960068345069885\n",
      "Epoch: 25, Loss: 0.41909849643707275\n",
      "Epoch: 26, Loss: 0.5132212042808533\n",
      "Epoch: 26, Loss: 0.4729863703250885\n",
      "Epoch: 27, Loss: 0.45705750584602356\n",
      "Epoch: 27, Loss: 0.42215853929519653\n",
      "Epoch: 28, Loss: 0.2705579996109009\n",
      "Epoch: 28, Loss: 0.39576902985572815\n",
      "Epoch: 29, Loss: 0.3147650361061096\n",
      "Epoch: 29, Loss: 0.4445376694202423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:23,758] Trial 26 finished with value: 0.421739813808051 and parameters: {'hidden_size': 207, 'dropout': 0.18240190684785895, 'lr': 0.0005799965274538032}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7094021439552307\n",
      "Epoch: 0, Loss: 0.39647161960601807\n",
      "Epoch: 1, Loss: 0.33249014616012573\n",
      "Epoch: 1, Loss: 0.5133110284805298\n",
      "Epoch: 2, Loss: 0.31269174814224243\n",
      "Epoch: 2, Loss: 0.5483527183532715\n",
      "Epoch: 3, Loss: 0.39785823225975037\n",
      "Epoch: 3, Loss: 0.44867047667503357\n",
      "Epoch: 4, Loss: 0.3556528091430664\n",
      "Epoch: 4, Loss: 0.4407114088535309\n",
      "Epoch: 5, Loss: 0.42343437671661377\n",
      "Epoch: 5, Loss: 0.5328407287597656\n",
      "Epoch: 6, Loss: 0.4526453912258148\n",
      "Epoch: 6, Loss: 0.5399359464645386\n",
      "Epoch: 7, Loss: 0.33388209342956543\n",
      "Epoch: 7, Loss: 0.29166823625564575\n",
      "Epoch: 8, Loss: 0.4685247838497162\n",
      "Epoch: 8, Loss: 0.48673006892204285\n",
      "Epoch: 9, Loss: 0.36749327182769775\n",
      "Epoch: 9, Loss: 0.32909780740737915\n",
      "Epoch: 10, Loss: 0.37360304594039917\n",
      "Epoch: 10, Loss: 0.393698513507843\n",
      "Epoch: 11, Loss: 0.515442967414856\n",
      "Epoch: 11, Loss: 0.6043148040771484\n",
      "Epoch: 12, Loss: 0.347233384847641\n",
      "Epoch: 12, Loss: 0.40766268968582153\n",
      "Epoch: 13, Loss: 0.2703312337398529\n",
      "Epoch: 13, Loss: 0.4232979118824005\n",
      "Epoch: 14, Loss: 0.34816375374794006\n",
      "Epoch: 14, Loss: 0.4273717403411865\n",
      "Epoch: 15, Loss: 0.5000661015510559\n",
      "Epoch: 15, Loss: 0.29446300864219666\n",
      "Epoch: 16, Loss: 0.45620596408843994\n",
      "Epoch: 16, Loss: 0.296342670917511\n",
      "Epoch: 17, Loss: 0.4158448576927185\n",
      "Epoch: 17, Loss: 0.4434664249420166\n",
      "Epoch: 18, Loss: 0.34435805678367615\n",
      "Epoch: 18, Loss: 0.39812996983528137\n",
      "Epoch: 19, Loss: 0.512258768081665\n",
      "Epoch: 19, Loss: 0.4595882296562195\n",
      "Epoch: 20, Loss: 0.47619327902793884\n",
      "Epoch: 20, Loss: 0.38054922223091125\n",
      "Epoch: 21, Loss: 0.284827321767807\n",
      "Epoch: 21, Loss: 0.5403003692626953\n",
      "Epoch: 22, Loss: 0.34186291694641113\n",
      "Epoch: 22, Loss: 0.28430354595184326\n",
      "Epoch: 23, Loss: 0.4298957288265228\n",
      "Epoch: 23, Loss: 0.34027212858200073\n",
      "Epoch: 24, Loss: 0.4164433479309082\n",
      "Epoch: 24, Loss: 0.275509238243103\n",
      "Epoch: 25, Loss: 0.44245728850364685\n",
      "Epoch: 25, Loss: 0.4042879343032837\n",
      "Epoch: 26, Loss: 0.3821982145309448\n",
      "Epoch: 26, Loss: 0.4176494777202606\n",
      "Epoch: 27, Loss: 0.3612058460712433\n",
      "Epoch: 27, Loss: 0.21013374626636505\n",
      "Epoch: 28, Loss: 0.3997877836227417\n",
      "Epoch: 28, Loss: 0.3985598385334015\n",
      "Epoch: 29, Loss: 0.355083703994751\n",
      "Epoch: 29, Loss: 0.4343040883541107\n",
      "Epoch: 30, Loss: 0.3532291650772095\n",
      "Epoch: 30, Loss: 0.38846147060394287\n",
      "Epoch: 31, Loss: 0.4487028121948242\n",
      "Epoch: 31, Loss: 0.4081168472766876\n",
      "Epoch: 32, Loss: 0.6122492551803589\n",
      "Epoch: 32, Loss: 0.3587748110294342\n",
      "Epoch: 33, Loss: 0.40821462869644165\n",
      "Epoch: 33, Loss: 0.29161566495895386\n",
      "Epoch: 34, Loss: 0.2915932238101959\n",
      "Epoch: 34, Loss: 0.3284323215484619\n",
      "Epoch: 35, Loss: 0.3873295187950134\n",
      "Epoch: 35, Loss: 0.3563522398471832\n",
      "Epoch: 36, Loss: 0.5345243811607361\n",
      "Epoch: 36, Loss: 0.4305611848831177\n",
      "Epoch: 37, Loss: 0.3016291856765747\n",
      "Epoch: 37, Loss: 0.50444096326828\n",
      "Epoch: 38, Loss: 0.3745790421962738\n",
      "Epoch: 38, Loss: 0.5130086541175842\n",
      "Epoch: 39, Loss: 0.4068225622177124\n",
      "Epoch: 39, Loss: 0.44279664754867554\n",
      "Epoch: 40, Loss: 0.47951459884643555\n",
      "Epoch: 40, Loss: 0.34274688363075256\n",
      "Epoch: 41, Loss: 0.5374196767807007\n",
      "Epoch: 41, Loss: 0.36889374256134033\n",
      "Epoch: 42, Loss: 0.2871783971786499\n",
      "Epoch: 42, Loss: 0.45812490582466125\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:37,549] Trial 27 finished with value: 0.4153703557072867 and parameters: {'hidden_size': 255, 'dropout': 0.258371978715597, 'lr': 0.00024652876440864487}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5908716917037964\n",
      "Epoch: 0, Loss: 0.4792781472206116\n",
      "Epoch: 1, Loss: 0.4519689977169037\n",
      "Epoch: 1, Loss: 0.5169774889945984\n",
      "Epoch: 2, Loss: 0.31644517183303833\n",
      "Epoch: 2, Loss: 0.3673352301120758\n",
      "Epoch: 3, Loss: 0.4875529110431671\n",
      "Epoch: 3, Loss: 0.38875797390937805\n",
      "Epoch: 4, Loss: 0.4324813783168793\n",
      "Epoch: 4, Loss: 0.25127995014190674\n",
      "Epoch: 5, Loss: 0.4014703333377838\n",
      "Epoch: 5, Loss: 0.3681872487068176\n",
      "Epoch: 6, Loss: 0.4506581425666809\n",
      "Epoch: 6, Loss: 0.22295495867729187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:39,858] Trial 28 finished with value: 0.4274504472275976 and parameters: {'hidden_size': 176, 'dropout': 0.36262648074631254, 'lr': 0.0009989258161975552}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7760007381439209\n",
      "Epoch: 0, Loss: 0.31794360280036926\n",
      "Epoch: 1, Loss: 0.3847731351852417\n",
      "Epoch: 1, Loss: 0.45324623584747314\n",
      "Epoch: 2, Loss: 0.46276602149009705\n",
      "Epoch: 2, Loss: 0.6870188117027283\n",
      "Epoch: 3, Loss: 0.3894668221473694\n",
      "Epoch: 3, Loss: 0.5218105912208557\n",
      "Epoch: 4, Loss: 0.3336523473262787\n",
      "Epoch: 4, Loss: 0.3483591675758362\n",
      "Epoch: 5, Loss: 0.42292311787605286\n",
      "Epoch: 5, Loss: 0.39175042510032654\n",
      "Epoch: 6, Loss: 0.3942975103855133\n",
      "Epoch: 6, Loss: 0.41417884826660156\n",
      "Epoch: 7, Loss: 0.5461790561676025\n",
      "Epoch: 7, Loss: 0.45670977234840393\n",
      "Epoch: 8, Loss: 0.353970468044281\n",
      "Epoch: 8, Loss: 0.3724155128002167\n",
      "Epoch: 9, Loss: 0.334440678358078\n",
      "Epoch: 9, Loss: 0.4336477220058441\n",
      "Epoch: 10, Loss: 0.47341489791870117\n",
      "Epoch: 10, Loss: 0.3267141282558441\n",
      "Epoch: 11, Loss: 0.49451881647109985\n",
      "Epoch: 11, Loss: 0.42180153727531433\n",
      "Epoch: 12, Loss: 0.4231661260128021\n",
      "Epoch: 12, Loss: 0.40945619344711304\n",
      "Epoch: 13, Loss: 0.3496643900871277\n",
      "Epoch: 13, Loss: 0.5880663394927979\n",
      "Epoch: 14, Loss: 0.5750921964645386\n",
      "Epoch: 14, Loss: 0.5320711135864258\n",
      "Epoch: 15, Loss: 0.4120291769504547\n",
      "Epoch: 15, Loss: 0.33347269892692566\n",
      "Epoch: 16, Loss: 0.44406044483184814\n",
      "Epoch: 16, Loss: 0.3993055522441864\n",
      "Epoch: 17, Loss: 0.34467053413391113\n",
      "Epoch: 17, Loss: 0.44692811369895935\n",
      "Epoch: 18, Loss: 0.517692506313324\n",
      "Epoch: 18, Loss: 0.45794516801834106\n",
      "Epoch: 19, Loss: 0.47327232360839844\n",
      "Epoch: 19, Loss: 0.3835156559944153\n",
      "Epoch: 20, Loss: 0.6267119646072388\n",
      "Epoch: 20, Loss: 0.36229315400123596\n",
      "Epoch: 21, Loss: 0.3545324504375458\n",
      "Epoch: 21, Loss: 0.40552762150764465\n",
      "Epoch: 22, Loss: 0.3125242590904236\n",
      "Epoch: 22, Loss: 0.608198344707489\n",
      "Epoch: 23, Loss: 0.5015076398849487\n",
      "Epoch: 23, Loss: 0.4822252094745636\n",
      "Epoch: 24, Loss: 0.4650428593158722\n",
      "Epoch: 24, Loss: 0.37974169850349426\n",
      "Epoch: 25, Loss: 0.5161755084991455\n",
      "Epoch: 25, Loss: 0.3926065266132355\n",
      "Epoch: 26, Loss: 0.4057689607143402\n",
      "Epoch: 26, Loss: 0.36226585507392883\n",
      "Epoch: 27, Loss: 0.46855980157852173\n",
      "Epoch: 27, Loss: 0.40133312344551086\n",
      "Epoch: 28, Loss: 0.4472721815109253\n",
      "Epoch: 28, Loss: 0.3723642826080322\n",
      "Epoch: 29, Loss: 0.3426423668861389\n",
      "Epoch: 29, Loss: 0.36927181482315063\n",
      "Epoch: 30, Loss: 0.356067031621933\n",
      "Epoch: 30, Loss: 0.6590341329574585\n",
      "Epoch: 31, Loss: 0.3167223334312439\n",
      "Epoch: 31, Loss: 0.47522464394569397\n",
      "Epoch: 32, Loss: 0.316862553358078\n",
      "Epoch: 32, Loss: 0.419864684343338\n",
      "Epoch: 33, Loss: 0.27928200364112854\n",
      "Epoch: 33, Loss: 0.35277166962623596\n",
      "Epoch: 34, Loss: 0.5600870847702026\n",
      "Epoch: 34, Loss: 0.5390074849128723\n",
      "Epoch: 35, Loss: 0.5733572840690613\n",
      "Epoch: 35, Loss: 0.4025149941444397\n",
      "Epoch: 36, Loss: 0.44842854142189026\n",
      "Epoch: 36, Loss: 0.38671237230300903\n",
      "Epoch: 37, Loss: 0.25375792384147644\n",
      "Epoch: 37, Loss: 0.28332558274269104\n",
      "Epoch: 38, Loss: 0.4929828643798828\n",
      "Epoch: 38, Loss: 0.43480342626571655\n",
      "Epoch: 39, Loss: 0.48256707191467285\n",
      "Epoch: 39, Loss: 0.4315130412578583\n",
      "Epoch: 40, Loss: 0.3774603605270386\n",
      "Epoch: 40, Loss: 0.5798342823982239\n",
      "Epoch: 41, Loss: 0.43848299980163574\n",
      "Epoch: 41, Loss: 0.5093833804130554\n",
      "Epoch: 42, Loss: 0.35357654094696045\n",
      "Epoch: 42, Loss: 0.5805535316467285\n",
      "Epoch: 43, Loss: 0.43643155694007874\n",
      "Epoch: 43, Loss: 0.37469276785850525\n",
      "Epoch: 44, Loss: 0.4387206733226776\n",
      "Epoch: 44, Loss: 0.38123857975006104\n",
      "Epoch: 45, Loss: 0.2982129752635956\n",
      "Epoch: 45, Loss: 0.4494187831878662\n",
      "Epoch: 46, Loss: 0.27900511026382446\n",
      "Epoch: 46, Loss: 0.372236967086792\n",
      "Epoch: 47, Loss: 0.2708204686641693\n",
      "Epoch: 47, Loss: 0.46389269828796387\n",
      "Epoch: 48, Loss: 0.4182873070240021\n",
      "Epoch: 48, Loss: 0.3986591696739197\n",
      "Epoch: 49, Loss: 0.444760799407959\n",
      "Epoch: 49, Loss: 0.3738674521446228\n",
      "Epoch: 50, Loss: 0.3046010136604309\n",
      "Epoch: 50, Loss: 0.3898051381111145\n",
      "Epoch: 51, Loss: 0.4379635155200958\n",
      "Epoch: 51, Loss: 0.35799679160118103\n",
      "Epoch: 52, Loss: 0.3812863826751709\n",
      "Epoch: 52, Loss: 0.4255003333091736\n",
      "Epoch: 53, Loss: 0.49743562936782837\n",
      "Epoch: 53, Loss: 0.49217793345451355\n",
      "Epoch: 54, Loss: 0.5166089534759521\n",
      "Epoch: 54, Loss: 0.44288718700408936\n",
      "Epoch: 55, Loss: 0.3685859143733978\n",
      "Epoch: 55, Loss: 0.39727193117141724\n",
      "Epoch: 56, Loss: 0.421608567237854\n",
      "Epoch: 56, Loss: 0.45227259397506714\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:22:57,191] Trial 29 finished with value: 0.41320806200853233 and parameters: {'hidden_size': 279, 'dropout': 0.3018807167173934, 'lr': 0.0001463742902104402}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8779733777046204\n",
      "Epoch: 0, Loss: 0.6515771746635437\n",
      "Epoch: 1, Loss: 0.5502623319625854\n",
      "Epoch: 1, Loss: 0.4888053238391876\n",
      "Epoch: 2, Loss: 0.5410681962966919\n",
      "Epoch: 2, Loss: 0.4580555856227875\n",
      "Epoch: 3, Loss: 0.28020793199539185\n",
      "Epoch: 3, Loss: 0.429210364818573\n",
      "Epoch: 4, Loss: 0.36923739314079285\n",
      "Epoch: 4, Loss: 0.4518321454524994\n",
      "Epoch: 5, Loss: 0.38265758752822876\n",
      "Epoch: 5, Loss: 0.42077627778053284\n",
      "Epoch: 6, Loss: 0.3825010061264038\n",
      "Epoch: 6, Loss: 0.3646394908428192\n",
      "Epoch: 7, Loss: 0.5837793946266174\n",
      "Epoch: 7, Loss: 0.47966885566711426\n",
      "Epoch: 8, Loss: 0.43935251235961914\n",
      "Epoch: 8, Loss: 0.25368523597717285\n",
      "Epoch: 9, Loss: 0.35094740986824036\n",
      "Epoch: 9, Loss: 0.5224201083183289\n",
      "Epoch: 10, Loss: 0.3251098096370697\n",
      "Epoch: 10, Loss: 0.39729970693588257\n",
      "Epoch: 11, Loss: 0.4033587574958801\n",
      "Epoch: 11, Loss: 0.3535175323486328\n",
      "Epoch: 12, Loss: 0.2819468080997467\n",
      "Epoch: 12, Loss: 0.4045778512954712\n",
      "Epoch: 13, Loss: 0.42831724882125854\n",
      "Epoch: 13, Loss: 0.5653668642044067\n",
      "Epoch: 14, Loss: 0.3632390797138214\n",
      "Epoch: 14, Loss: 0.5132208466529846\n",
      "Epoch: 15, Loss: 0.49535149335861206\n",
      "Epoch: 15, Loss: 0.29085466265678406\n",
      "Epoch: 16, Loss: 0.36805933713912964\n",
      "Epoch: 16, Loss: 0.5703099370002747\n",
      "Epoch: 17, Loss: 0.4727385640144348\n",
      "Epoch: 17, Loss: 0.4821489155292511\n",
      "Epoch: 18, Loss: 0.5872341990470886\n",
      "Epoch: 18, Loss: 0.35088270902633667\n",
      "Epoch: 19, Loss: 0.47125938534736633\n",
      "Epoch: 19, Loss: 0.3521044850349426\n",
      "Epoch: 20, Loss: 0.567961573600769\n",
      "Epoch: 20, Loss: 0.4463493227958679\n",
      "Epoch: 21, Loss: 0.34590330719947815\n",
      "Epoch: 21, Loss: 0.4580914080142975\n",
      "Epoch: 22, Loss: 0.28570106625556946\n",
      "Epoch: 22, Loss: 0.4292171001434326\n",
      "Epoch: 23, Loss: 0.32573431730270386\n",
      "Epoch: 23, Loss: 0.44219478964805603\n",
      "Epoch: 24, Loss: 0.3473353683948517\n",
      "Epoch: 24, Loss: 0.45976752042770386\n",
      "Epoch: 25, Loss: 0.3803759813308716\n",
      "Epoch: 25, Loss: 0.48330453038215637\n",
      "Epoch: 26, Loss: 0.48882773518562317\n",
      "Epoch: 26, Loss: 0.4788443446159363\n",
      "Epoch: 27, Loss: 0.3670637905597687\n",
      "Epoch: 27, Loss: 0.37406522035598755\n",
      "Epoch: 28, Loss: 0.34116044640541077\n",
      "Epoch: 28, Loss: 0.44534164667129517\n",
      "Epoch: 29, Loss: 0.413696825504303\n",
      "Epoch: 29, Loss: 0.4990915358066559\n",
      "Epoch: 30, Loss: 0.453713059425354\n",
      "Epoch: 30, Loss: 0.48360878229141235\n",
      "Epoch: 31, Loss: 0.36908480525016785\n",
      "Epoch: 31, Loss: 0.2543451189994812\n",
      "Epoch: 32, Loss: 0.37936922907829285\n",
      "Epoch: 32, Loss: 0.515210747718811\n",
      "Epoch: 33, Loss: 0.43996837735176086\n",
      "Epoch: 33, Loss: 0.2689896821975708\n",
      "Epoch: 34, Loss: 0.3848405182361603\n",
      "Epoch: 34, Loss: 0.3939112424850464\n",
      "Epoch: 35, Loss: 0.3973323404788971\n",
      "Epoch: 35, Loss: 0.30620354413986206\n",
      "Epoch: 36, Loss: 0.45425140857696533\n",
      "Epoch: 36, Loss: 0.44606518745422363\n",
      "Epoch: 37, Loss: 0.436443954706192\n",
      "Epoch: 37, Loss: 0.37540456652641296\n",
      "Epoch: 38, Loss: 0.49087995290756226\n",
      "Epoch: 38, Loss: 0.43382158875465393\n",
      "Epoch: 39, Loss: 0.5159947872161865\n",
      "Epoch: 39, Loss: 0.33128154277801514\n",
      "Epoch: 40, Loss: 0.3967064619064331\n",
      "Epoch: 40, Loss: 0.47537678480148315\n",
      "Epoch: 41, Loss: 0.42448416352272034\n",
      "Epoch: 41, Loss: 0.31327229738235474\n",
      "Epoch: 42, Loss: 0.3728882670402527\n",
      "Epoch: 42, Loss: 0.5151277184486389\n",
      "Epoch: 43, Loss: 0.43373826146125793\n",
      "Epoch: 43, Loss: 0.5092855095863342\n",
      "Epoch: 44, Loss: 0.3699125647544861\n",
      "Epoch: 44, Loss: 0.5865938067436218\n",
      "Epoch: 45, Loss: 0.37253084778785706\n",
      "Epoch: 45, Loss: 0.5326344966888428\n",
      "Epoch: 46, Loss: 0.45115578174591064\n",
      "Epoch: 46, Loss: 0.37561699748039246\n",
      "Epoch: 47, Loss: 0.3847878873348236\n",
      "Epoch: 47, Loss: 0.3419925272464752\n",
      "Epoch: 48, Loss: 0.418846070766449\n",
      "Epoch: 48, Loss: 0.4257200360298157\n",
      "Epoch: 49, Loss: 0.4255293309688568\n",
      "Epoch: 49, Loss: 0.5188314914703369\n",
      "Epoch: 50, Loss: 0.5592547059059143\n",
      "Epoch: 50, Loss: 0.29412606358528137\n",
      "Epoch: 51, Loss: 0.4421027898788452\n",
      "Epoch: 51, Loss: 0.3760504126548767\n",
      "Epoch: 52, Loss: 0.38968440890312195\n",
      "Epoch: 52, Loss: 0.4734784662723541\n",
      "Epoch: 53, Loss: 0.40766024589538574\n",
      "Epoch: 53, Loss: 0.36934977769851685\n",
      "Epoch: 54, Loss: 0.438180536031723\n",
      "Epoch: 54, Loss: 0.3508659601211548\n",
      "Epoch: 55, Loss: 0.31630033254623413\n",
      "Epoch: 55, Loss: 0.405093789100647\n",
      "Epoch: 56, Loss: 0.503994345664978\n",
      "Epoch: 56, Loss: 0.45623087882995605\n",
      "Epoch: 57, Loss: 0.6149595975875854\n",
      "Epoch: 57, Loss: 0.34257516264915466\n",
      "Epoch: 58, Loss: 0.5224778056144714\n",
      "Epoch: 58, Loss: 0.3157777786254883\n",
      "Epoch: 59, Loss: 0.29629671573638916\n",
      "Epoch: 59, Loss: 0.4785797595977783\n",
      "Epoch: 60, Loss: 0.5035849809646606\n",
      "Epoch: 60, Loss: 0.45293834805488586\n",
      "Epoch: 61, Loss: 0.4487406313419342\n",
      "Epoch: 61, Loss: 0.24559248983860016\n",
      "Epoch: 62, Loss: 0.4019771218299866\n",
      "Epoch: 62, Loss: 0.46103644371032715\n",
      "Epoch: 63, Loss: 0.30068325996398926\n",
      "Epoch: 63, Loss: 0.4745347797870636\n",
      "Epoch: 64, Loss: 0.391723096370697\n",
      "Epoch: 64, Loss: 0.3704764246940613\n",
      "Epoch: 65, Loss: 0.5071013569831848\n",
      "Epoch: 65, Loss: 0.544775664806366\n",
      "Epoch: 66, Loss: 0.6058418154716492\n",
      "Epoch: 66, Loss: 0.579086422920227\n",
      "Epoch: 67, Loss: 0.4260255694389343\n",
      "Epoch: 67, Loss: 0.4210021197795868\n",
      "Epoch: 68, Loss: 0.465701162815094\n",
      "Epoch: 68, Loss: 0.4341980814933777\n",
      "Epoch: 69, Loss: 0.2384355217218399\n",
      "Epoch: 69, Loss: 0.548347532749176\n",
      "Epoch: 70, Loss: 0.45097506046295166\n",
      "Epoch: 70, Loss: 0.32108616828918457\n",
      "Epoch: 71, Loss: 0.46741265058517456\n",
      "Epoch: 71, Loss: 0.4106796979904175\n",
      "Epoch: 72, Loss: 0.45907464623451233\n",
      "Epoch: 72, Loss: 0.54654461145401\n",
      "Epoch: 73, Loss: 0.449847936630249\n",
      "Epoch: 73, Loss: 0.3931284248828888\n",
      "Epoch: 74, Loss: 0.3518076539039612\n",
      "Epoch: 74, Loss: 0.3521658778190613\n",
      "Epoch: 75, Loss: 0.4582340121269226\n",
      "Epoch: 75, Loss: 0.3773570954799652\n",
      "Epoch: 76, Loss: 0.4640445113182068\n",
      "Epoch: 76, Loss: 0.4547763764858246\n",
      "Epoch: 77, Loss: 0.4317569434642792\n",
      "Epoch: 77, Loss: 0.4816492199897766\n",
      "Epoch: 78, Loss: 0.34153833985328674\n",
      "Epoch: 78, Loss: 0.5415211319923401\n",
      "Epoch: 79, Loss: 0.3052442669868469\n",
      "Epoch: 79, Loss: 0.41397127509117126\n",
      "Epoch: 80, Loss: 0.33636292815208435\n",
      "Epoch: 80, Loss: 0.38591963052749634\n",
      "Epoch: 81, Loss: 0.39043673872947693\n",
      "Epoch: 81, Loss: 0.4358139932155609\n",
      "Epoch: 82, Loss: 0.2907677888870239\n",
      "Epoch: 82, Loss: 0.5062402486801147\n",
      "Epoch: 83, Loss: 0.301310658454895\n",
      "Epoch: 83, Loss: 0.3276561498641968\n",
      "Epoch: 84, Loss: 0.37571045756340027\n",
      "Epoch: 84, Loss: 0.36649370193481445\n",
      "Epoch: 85, Loss: 0.5017305612564087\n",
      "Epoch: 85, Loss: 0.31289172172546387\n",
      "Epoch: 86, Loss: 0.4052251875400543\n",
      "Epoch: 86, Loss: 0.3846955895423889\n",
      "Epoch: 87, Loss: 0.46389397978782654\n",
      "Epoch: 87, Loss: 0.4127979576587677\n",
      "Epoch: 88, Loss: 0.38734138011932373\n",
      "Epoch: 88, Loss: 0.3884326219558716\n",
      "Epoch: 89, Loss: 0.4172467291355133\n",
      "Epoch: 89, Loss: 0.3837299942970276\n",
      "Epoch: 90, Loss: 0.4417702555656433\n",
      "Epoch: 90, Loss: 0.3325858414173126\n",
      "Epoch: 91, Loss: 0.4500020444393158\n",
      "Epoch: 91, Loss: 0.33476951718330383\n",
      "Epoch: 92, Loss: 0.4796842634677887\n",
      "Epoch: 92, Loss: 0.4437941014766693\n",
      "Epoch: 93, Loss: 0.3416556417942047\n",
      "Epoch: 93, Loss: 0.392958402633667\n",
      "Epoch: 94, Loss: 0.28350943326950073\n",
      "Epoch: 94, Loss: 0.3989381492137909\n",
      "Epoch: 95, Loss: 0.3884425759315491\n",
      "Epoch: 95, Loss: 0.4574997127056122\n",
      "Epoch: 96, Loss: 0.4300769567489624\n",
      "Epoch: 96, Loss: 0.5124790072441101\n",
      "Epoch: 97, Loss: 0.44880709052085876\n",
      "Epoch: 97, Loss: 0.605659008026123\n",
      "Epoch: 98, Loss: 0.4123970866203308\n",
      "Epoch: 98, Loss: 0.3080958127975464\n",
      "Epoch: 99, Loss: 0.43386057019233704\n",
      "Epoch: 99, Loss: 0.5412296056747437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:23:27,741] Trial 30 finished with value: 0.41751373934657865 and parameters: {'hidden_size': 279, 'dropout': 0.31980017265344407, 'lr': 2.680671941084907e-05}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5824875831604004\n",
      "Epoch: 0, Loss: 0.41156506538391113\n",
      "Epoch: 1, Loss: 0.4322410225868225\n",
      "Epoch: 1, Loss: 0.43664219975471497\n",
      "Epoch: 2, Loss: 0.4502539038658142\n",
      "Epoch: 2, Loss: 0.3507157564163208\n",
      "Epoch: 3, Loss: 0.416007399559021\n",
      "Epoch: 3, Loss: 0.6163187623023987\n",
      "Epoch: 4, Loss: 0.34860655665397644\n",
      "Epoch: 4, Loss: 0.4426819682121277\n",
      "Epoch: 5, Loss: 0.42771241068840027\n",
      "Epoch: 5, Loss: 0.5884087681770325\n",
      "Epoch: 6, Loss: 0.5320281982421875\n",
      "Epoch: 6, Loss: 0.4020153284072876\n",
      "Epoch: 7, Loss: 0.36096709966659546\n",
      "Epoch: 7, Loss: 0.41225171089172363\n",
      "Epoch: 8, Loss: 0.4125945568084717\n",
      "Epoch: 8, Loss: 0.2859940826892853\n",
      "Epoch: 9, Loss: 0.6063390374183655\n",
      "Epoch: 9, Loss: 0.43960970640182495\n",
      "Epoch: 10, Loss: 0.4149962365627289\n",
      "Epoch: 10, Loss: 0.3449767231941223\n",
      "Epoch: 11, Loss: 0.4064171314239502\n",
      "Epoch: 11, Loss: 0.23045411705970764\n",
      "Epoch: 12, Loss: 0.4783358871936798\n",
      "Epoch: 12, Loss: 0.5267965793609619\n",
      "Epoch: 13, Loss: 0.4172229468822479\n",
      "Epoch: 13, Loss: 0.4915138781070709\n",
      "Epoch: 14, Loss: 0.4200321435928345\n",
      "Epoch: 14, Loss: 0.26250171661376953\n",
      "Epoch: 15, Loss: 0.3228878974914551\n",
      "Epoch: 15, Loss: 0.49765342473983765\n",
      "Epoch: 16, Loss: 0.549765944480896\n",
      "Epoch: 16, Loss: 0.411186158657074\n",
      "Epoch: 17, Loss: 0.4003582000732422\n",
      "Epoch: 17, Loss: 0.52181077003479\n",
      "Epoch: 18, Loss: 0.24752625823020935\n",
      "Epoch: 18, Loss: 0.4175122082233429\n",
      "Epoch: 19, Loss: 0.40354862809181213\n",
      "Epoch: 19, Loss: 0.37036916613578796\n",
      "Epoch: 20, Loss: 0.5729714632034302\n",
      "Epoch: 20, Loss: 0.5199635624885559\n",
      "Epoch: 21, Loss: 0.34511807560920715\n",
      "Epoch: 21, Loss: 0.4350440502166748\n",
      "Epoch: 22, Loss: 0.5942296385765076\n",
      "Epoch: 22, Loss: 0.40024030208587646\n",
      "Epoch: 23, Loss: 0.5050950050354004\n",
      "Epoch: 23, Loss: 0.36338570713996887\n",
      "Epoch: 24, Loss: 0.37557196617126465\n",
      "Epoch: 24, Loss: 0.32763463258743286\n",
      "Epoch: 25, Loss: 0.39279305934906006\n",
      "Epoch: 25, Loss: 0.3767336308956146\n",
      "Epoch: 26, Loss: 0.41857877373695374\n",
      "Epoch: 26, Loss: 0.3655124306678772\n",
      "Epoch: 27, Loss: 0.4317438006401062\n",
      "Epoch: 27, Loss: 0.3956630229949951\n",
      "Epoch: 28, Loss: 0.38163119554519653\n",
      "Epoch: 28, Loss: 0.35224342346191406\n",
      "Epoch: 29, Loss: 0.3928029537200928\n",
      "Epoch: 29, Loss: 0.47228285670280457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:23:36,574] Trial 31 finished with value: 0.42498758603003084 and parameters: {'hidden_size': 283, 'dropout': 0.2912908708883592, 'lr': 0.00015306469593488707}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.9704750776290894\n",
      "Epoch: 0, Loss: 0.5289576649665833\n",
      "Epoch: 1, Loss: 0.3843547999858856\n",
      "Epoch: 1, Loss: 0.33924174308776855\n",
      "Epoch: 2, Loss: 0.4723791182041168\n",
      "Epoch: 2, Loss: 0.35394614934921265\n",
      "Epoch: 3, Loss: 0.4151081144809723\n",
      "Epoch: 3, Loss: 0.34049099683761597\n",
      "Epoch: 4, Loss: 0.4180057942867279\n",
      "Epoch: 4, Loss: 0.3731224834918976\n",
      "Epoch: 5, Loss: 0.3662867844104767\n",
      "Epoch: 5, Loss: 0.428676962852478\n",
      "Epoch: 6, Loss: 0.43910831212997437\n",
      "Epoch: 6, Loss: 0.4086076617240906\n",
      "Epoch: 7, Loss: 0.387251615524292\n",
      "Epoch: 7, Loss: 0.39606666564941406\n",
      "Epoch: 8, Loss: 0.5474719405174255\n",
      "Epoch: 8, Loss: 0.4544270634651184\n",
      "Epoch: 9, Loss: 0.32489195466041565\n",
      "Epoch: 9, Loss: 0.43787339329719543\n",
      "Epoch: 10, Loss: 0.6383377313613892\n",
      "Epoch: 10, Loss: 0.5123156905174255\n",
      "Epoch: 11, Loss: 0.4597218632698059\n",
      "Epoch: 11, Loss: 0.34896552562713623\n",
      "Epoch: 12, Loss: 0.565575361251831\n",
      "Epoch: 12, Loss: 0.3821967840194702\n",
      "Epoch: 13, Loss: 0.4240351617336273\n",
      "Epoch: 13, Loss: 0.5190171599388123\n",
      "Epoch: 14, Loss: 0.31715327501296997\n",
      "Epoch: 14, Loss: 0.44371044635772705\n",
      "Epoch: 15, Loss: 0.4579108655452728\n",
      "Epoch: 15, Loss: 0.33654189109802246\n",
      "Epoch: 16, Loss: 0.5947006940841675\n",
      "Epoch: 16, Loss: 0.5347375869750977\n",
      "Epoch: 17, Loss: 0.4033055007457733\n",
      "Epoch: 17, Loss: 0.36755940318107605\n",
      "Epoch: 18, Loss: 0.4197203516960144\n",
      "Epoch: 18, Loss: 0.5031533241271973\n",
      "Epoch: 19, Loss: 0.4847642183303833\n",
      "Epoch: 19, Loss: 0.3857371509075165\n",
      "Epoch: 20, Loss: 0.4102678596973419\n",
      "Epoch: 20, Loss: 0.4818932116031647\n",
      "Epoch: 21, Loss: 0.4497096538543701\n",
      "Epoch: 21, Loss: 0.381001353263855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:23:43,251] Trial 32 finished with value: 0.42142934612467053 and parameters: {'hidden_size': 268, 'dropout': 0.33448522931561553, 'lr': 8.3340163850344e-05}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6068981289863586\n",
      "Epoch: 0, Loss: 0.6029860973358154\n",
      "Epoch: 1, Loss: 0.4222269654273987\n",
      "Epoch: 1, Loss: 0.3478381931781769\n",
      "Epoch: 2, Loss: 0.36102795600891113\n",
      "Epoch: 2, Loss: 0.398763507604599\n",
      "Epoch: 3, Loss: 0.44378089904785156\n",
      "Epoch: 3, Loss: 0.6858465075492859\n",
      "Epoch: 4, Loss: 0.5188701748847961\n",
      "Epoch: 4, Loss: 0.43932420015335083\n",
      "Epoch: 5, Loss: 0.31981998682022095\n",
      "Epoch: 5, Loss: 0.560104489326477\n",
      "Epoch: 6, Loss: 0.52043616771698\n",
      "Epoch: 6, Loss: 0.534106969833374\n",
      "Epoch: 7, Loss: 0.27597472071647644\n",
      "Epoch: 7, Loss: 0.5308474898338318\n",
      "Epoch: 8, Loss: 0.3546425998210907\n",
      "Epoch: 8, Loss: 0.3650049567222595\n",
      "Epoch: 9, Loss: 0.43364569544792175\n",
      "Epoch: 9, Loss: 0.5851512551307678\n",
      "Epoch: 10, Loss: 0.4368531107902527\n",
      "Epoch: 10, Loss: 0.45079469680786133\n",
      "Epoch: 11, Loss: 0.42657554149627686\n",
      "Epoch: 11, Loss: 0.3143395483493805\n",
      "Epoch: 12, Loss: 0.4450947940349579\n",
      "Epoch: 12, Loss: 0.5078508853912354\n",
      "Epoch: 13, Loss: 0.3993369936943054\n",
      "Epoch: 13, Loss: 0.32772690057754517\n",
      "Epoch: 14, Loss: 0.3750103712081909\n",
      "Epoch: 14, Loss: 0.4548626244068146\n",
      "Epoch: 15, Loss: 0.3495609760284424\n",
      "Epoch: 15, Loss: 0.42081210017204285\n",
      "Epoch: 16, Loss: 0.4783514440059662\n",
      "Epoch: 16, Loss: 0.40803539752960205\n",
      "Epoch: 17, Loss: 0.42405736446380615\n",
      "Epoch: 17, Loss: 0.39484965801239014\n",
      "Epoch: 18, Loss: 0.3810267448425293\n",
      "Epoch: 18, Loss: 0.42480704188346863\n",
      "Epoch: 19, Loss: 0.3305221498012543\n",
      "Epoch: 19, Loss: 0.3679659068584442\n",
      "Epoch: 20, Loss: 0.33004674315452576\n",
      "Epoch: 20, Loss: 0.41180577874183655\n",
      "Epoch: 21, Loss: 0.6797923445701599\n",
      "Epoch: 21, Loss: 0.2874738574028015\n",
      "Epoch: 22, Loss: 0.4108947813510895\n",
      "Epoch: 22, Loss: 0.5935647487640381\n",
      "Epoch: 23, Loss: 0.3258594572544098\n",
      "Epoch: 23, Loss: 0.5421415567398071\n",
      "Epoch: 24, Loss: 0.4489913284778595\n",
      "Epoch: 24, Loss: 0.3926108181476593\n",
      "Epoch: 25, Loss: 0.5041643977165222\n",
      "Epoch: 25, Loss: 0.4517131447792053\n",
      "Epoch: 26, Loss: 0.526680052280426\n",
      "Epoch: 26, Loss: 0.3753769099712372\n",
      "Epoch: 27, Loss: 0.36784881353378296\n",
      "Epoch: 27, Loss: 0.35886350274086\n",
      "Epoch: 28, Loss: 0.508296012878418\n",
      "Epoch: 28, Loss: 0.3630143404006958\n",
      "Epoch: 29, Loss: 0.4511221945285797\n",
      "Epoch: 29, Loss: 0.34531211853027344\n",
      "Epoch: 30, Loss: 0.4548681676387787\n",
      "Epoch: 30, Loss: 0.3875422775745392\n",
      "Epoch: 31, Loss: 0.45716792345046997\n",
      "Epoch: 31, Loss: 0.3577227294445038\n",
      "Epoch: 32, Loss: 0.4874870777130127\n",
      "Epoch: 32, Loss: 0.3856094777584076\n",
      "Epoch: 33, Loss: 0.37743327021598816\n",
      "Epoch: 33, Loss: 0.4086085557937622\n",
      "Epoch: 34, Loss: 0.49876102805137634\n",
      "Epoch: 34, Loss: 0.2975296080112457\n",
      "Epoch: 35, Loss: 0.5516672134399414\n",
      "Epoch: 35, Loss: 0.40702903270721436\n",
      "Epoch: 36, Loss: 0.34320250153541565\n",
      "Epoch: 36, Loss: 0.432547390460968\n",
      "Epoch: 37, Loss: 0.33452308177948\n",
      "Epoch: 37, Loss: 0.5263580083847046\n",
      "Epoch: 38, Loss: 0.4165208339691162\n",
      "Epoch: 38, Loss: 0.39269059896469116\n",
      "Epoch: 39, Loss: 0.47510209679603577\n",
      "Epoch: 39, Loss: 0.44198834896087646\n",
      "Epoch: 40, Loss: 0.36597150564193726\n",
      "Epoch: 40, Loss: 0.4342298209667206\n",
      "Epoch: 41, Loss: 0.4103557765483856\n",
      "Epoch: 41, Loss: 0.4633779525756836\n",
      "Epoch: 42, Loss: 0.4268713593482971\n",
      "Epoch: 42, Loss: 0.4193517863750458\n",
      "Epoch: 43, Loss: 0.36310142278671265\n",
      "Epoch: 43, Loss: 0.7239510416984558\n",
      "Epoch: 44, Loss: 0.3147598206996918\n",
      "Epoch: 44, Loss: 0.47979915142059326\n",
      "Epoch: 45, Loss: 0.404572457075119\n",
      "Epoch: 45, Loss: 0.49015122652053833\n",
      "Epoch: 46, Loss: 0.37644022703170776\n",
      "Epoch: 46, Loss: 0.363353967666626\n",
      "Epoch: 47, Loss: 0.3193965256214142\n",
      "Epoch: 47, Loss: 0.311545729637146\n",
      "Epoch: 48, Loss: 0.41841351985931396\n",
      "Epoch: 48, Loss: 0.4551393687725067\n",
      "Epoch: 49, Loss: 0.6280466318130493\n",
      "Epoch: 49, Loss: 0.37903541326522827\n",
      "Epoch: 50, Loss: 0.3941258192062378\n",
      "Epoch: 50, Loss: 0.35730692744255066\n",
      "Epoch: 51, Loss: 0.5132293105125427\n",
      "Epoch: 51, Loss: 0.34165817499160767\n",
      "Epoch: 52, Loss: 0.37915927171707153\n",
      "Epoch: 52, Loss: 0.30858391523361206\n",
      "Epoch: 53, Loss: 0.33614230155944824\n",
      "Epoch: 53, Loss: 0.2819688618183136\n",
      "Epoch: 54, Loss: 0.5683205127716064\n",
      "Epoch: 54, Loss: 0.40471118688583374\n",
      "Epoch: 55, Loss: 0.525418758392334\n",
      "Epoch: 55, Loss: 0.37392839789390564\n",
      "Epoch: 56, Loss: 0.4736003577709198\n",
      "Epoch: 56, Loss: 0.42851123213768005\n",
      "Epoch: 57, Loss: 0.47443342208862305\n",
      "Epoch: 57, Loss: 0.4426785409450531\n",
      "Epoch: 58, Loss: 0.37061846256256104\n",
      "Epoch: 58, Loss: 0.3530837297439575\n",
      "Epoch: 59, Loss: 0.41944748163223267\n",
      "Epoch: 59, Loss: 0.38905856013298035\n",
      "Epoch: 60, Loss: 0.3400537073612213\n",
      "Epoch: 60, Loss: 0.5233059525489807\n",
      "Epoch: 61, Loss: 0.43204575777053833\n",
      "Epoch: 61, Loss: 0.3436450958251953\n",
      "Epoch: 62, Loss: 0.34956982731819153\n",
      "Epoch: 62, Loss: 0.2863776385784149\n",
      "Epoch: 63, Loss: 0.4637341797351837\n",
      "Epoch: 63, Loss: 0.44291672110557556\n",
      "Epoch: 64, Loss: 0.34860485792160034\n",
      "Epoch: 64, Loss: 0.3090749979019165\n",
      "Epoch: 65, Loss: 0.40522414445877075\n",
      "Epoch: 65, Loss: 0.33654430508613586\n",
      "Epoch: 66, Loss: 0.47127044200897217\n",
      "Epoch: 66, Loss: 0.45283666253089905\n",
      "Epoch: 67, Loss: 0.35249409079551697\n",
      "Epoch: 67, Loss: 0.49417850375175476\n",
      "Epoch: 68, Loss: 0.4107293486595154\n",
      "Epoch: 68, Loss: 0.5824084281921387\n",
      "Epoch: 69, Loss: 0.44680625200271606\n",
      "Epoch: 69, Loss: 0.46363410353660583\n",
      "Epoch: 70, Loss: 0.6125906705856323\n",
      "Epoch: 70, Loss: 0.33959051966667175\n",
      "Epoch: 71, Loss: 0.3330877125263214\n",
      "Epoch: 71, Loss: 0.4075051248073578\n",
      "Epoch: 72, Loss: 0.47150900959968567\n",
      "Epoch: 72, Loss: 0.417785108089447\n",
      "Epoch: 73, Loss: 0.41639113426208496\n",
      "Epoch: 73, Loss: 0.4196408689022064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:05,376] Trial 33 finished with value: 0.4214380207060842 and parameters: {'hidden_size': 253, 'dropout': 0.3011139534284636, 'lr': 4.1334260078029867e-05}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.8888320326805115\n",
      "Epoch: 0, Loss: 0.3671099543571472\n",
      "Epoch: 1, Loss: 0.5335007905960083\n",
      "Epoch: 1, Loss: 0.4650208353996277\n",
      "Epoch: 2, Loss: 0.4089147448539734\n",
      "Epoch: 2, Loss: 0.3894609212875366\n",
      "Epoch: 3, Loss: 0.3609093427658081\n",
      "Epoch: 3, Loss: 0.493629515171051\n",
      "Epoch: 4, Loss: 0.3795165419578552\n",
      "Epoch: 4, Loss: 0.5599185824394226\n",
      "Epoch: 5, Loss: 0.6296118497848511\n",
      "Epoch: 5, Loss: 0.3552209436893463\n",
      "Epoch: 6, Loss: 0.360736221075058\n",
      "Epoch: 6, Loss: 0.5067839026451111\n",
      "Epoch: 7, Loss: 0.5319949388504028\n",
      "Epoch: 7, Loss: 0.5913386940956116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:08,426] Trial 34 finished with value: 0.4284508244961685 and parameters: {'hidden_size': 285, 'dropout': 0.3655856909907644, 'lr': 0.00043715265980480693}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.657315731048584\n",
      "Epoch: 0, Loss: 0.4059622585773468\n",
      "Epoch: 1, Loss: 0.5505164265632629\n",
      "Epoch: 1, Loss: 0.41224274039268494\n",
      "Epoch: 2, Loss: 0.398725301027298\n",
      "Epoch: 2, Loss: 0.468932181596756\n",
      "Epoch: 3, Loss: 0.40197649598121643\n",
      "Epoch: 3, Loss: 0.32573068141937256\n",
      "Epoch: 4, Loss: 0.325283020734787\n",
      "Epoch: 4, Loss: 0.4901019334793091\n",
      "Epoch: 5, Loss: 0.26720112562179565\n",
      "Epoch: 5, Loss: 0.2802318334579468\n",
      "Epoch: 6, Loss: 0.26619741320610046\n",
      "Epoch: 6, Loss: 0.2961514890193939\n",
      "Epoch: 7, Loss: 0.44596704840660095\n",
      "Epoch: 7, Loss: 0.4719405770301819\n",
      "Epoch: 8, Loss: 0.4361151158809662\n",
      "Epoch: 8, Loss: 0.5468736886978149\n",
      "Epoch: 9, Loss: 0.46109336614608765\n",
      "Epoch: 9, Loss: 0.5028877854347229\n",
      "Epoch: 10, Loss: 0.411708265542984\n",
      "Epoch: 10, Loss: 0.45795634388923645\n",
      "Epoch: 11, Loss: 0.509690523147583\n",
      "Epoch: 11, Loss: 0.4167811870574951\n",
      "Epoch: 12, Loss: 0.8455369472503662\n",
      "Epoch: 12, Loss: 0.3209781050682068\n",
      "Epoch: 13, Loss: 0.46091803908348083\n",
      "Epoch: 13, Loss: 0.39404892921447754\n",
      "Epoch: 14, Loss: 0.6079816818237305\n",
      "Epoch: 14, Loss: 0.4692767560482025\n",
      "Epoch: 15, Loss: 0.4148509204387665\n",
      "Epoch: 15, Loss: 0.5634719729423523\n",
      "Epoch: 16, Loss: 0.5856362581253052\n",
      "Epoch: 16, Loss: 0.5085210204124451\n",
      "Epoch: 17, Loss: 0.5466566681861877\n",
      "Epoch: 17, Loss: 0.4964480400085449\n",
      "Epoch: 18, Loss: 0.3582612872123718\n",
      "Epoch: 18, Loss: 0.3931574523448944\n",
      "Epoch: 19, Loss: 0.3864653706550598\n",
      "Epoch: 19, Loss: 0.23562036454677582\n",
      "Epoch: 20, Loss: 0.4197944402694702\n",
      "Epoch: 20, Loss: 0.39889752864837646\n",
      "Epoch: 21, Loss: 0.5571867227554321\n",
      "Epoch: 21, Loss: 0.3179362714290619\n",
      "Epoch: 22, Loss: 0.40138256549835205\n",
      "Epoch: 22, Loss: 0.39227479696273804\n",
      "Epoch: 23, Loss: 0.4162442088127136\n",
      "Epoch: 23, Loss: 0.3731166422367096\n",
      "Epoch: 24, Loss: 0.4025368094444275\n",
      "Epoch: 24, Loss: 0.5365383625030518\n",
      "Epoch: 25, Loss: 0.3419162929058075\n",
      "Epoch: 25, Loss: 0.6252642869949341\n",
      "Epoch: 26, Loss: 0.39508187770843506\n",
      "Epoch: 26, Loss: 0.41419047117233276\n",
      "Epoch: 27, Loss: 0.4257756471633911\n",
      "Epoch: 27, Loss: 0.26060348749160767\n",
      "Epoch: 28, Loss: 0.44166654348373413\n",
      "Epoch: 28, Loss: 0.3707418143749237\n",
      "Epoch: 29, Loss: 0.32855790853500366\n",
      "Epoch: 29, Loss: 0.40042993426322937\n",
      "Epoch: 30, Loss: 0.36091476678848267\n",
      "Epoch: 30, Loss: 0.35007503628730774\n",
      "Epoch: 31, Loss: 0.43490880727767944\n",
      "Epoch: 31, Loss: 0.35586249828338623\n",
      "Epoch: 32, Loss: 0.35413822531700134\n",
      "Epoch: 32, Loss: 0.5257958173751831\n",
      "Epoch: 33, Loss: 0.4593202769756317\n",
      "Epoch: 33, Loss: 0.29253414273262024\n",
      "Epoch: 34, Loss: 0.5474675893783569\n",
      "Epoch: 34, Loss: 0.35242003202438354\n",
      "Epoch: 35, Loss: 0.3924878239631653\n",
      "Epoch: 35, Loss: 0.3383558988571167\n",
      "Epoch: 36, Loss: 0.36630532145500183\n",
      "Epoch: 36, Loss: 0.36278611421585083\n",
      "Epoch: 37, Loss: 0.43414071202278137\n",
      "Epoch: 37, Loss: 0.4202311038970947\n",
      "Epoch: 38, Loss: 0.3771288990974426\n",
      "Epoch: 38, Loss: 0.34521400928497314\n",
      "Epoch: 39, Loss: 0.5601314902305603\n",
      "Epoch: 39, Loss: 0.3178436756134033\n",
      "Epoch: 40, Loss: 0.4464241862297058\n",
      "Epoch: 40, Loss: 0.45261529088020325\n",
      "Epoch: 41, Loss: 0.48073089122772217\n",
      "Epoch: 41, Loss: 0.4137420654296875\n",
      "Epoch: 42, Loss: 0.38776135444641113\n",
      "Epoch: 42, Loss: 0.4961096942424774\n",
      "Epoch: 43, Loss: 0.39684465527534485\n",
      "Epoch: 43, Loss: 0.26507940888404846\n",
      "Epoch: 44, Loss: 0.5450456142425537\n",
      "Epoch: 44, Loss: 0.34058961272239685\n",
      "Epoch: 45, Loss: 0.26808226108551025\n",
      "Epoch: 45, Loss: 0.4706079661846161\n",
      "Epoch: 46, Loss: 0.39542174339294434\n",
      "Epoch: 46, Loss: 0.4470292627811432\n",
      "Epoch: 47, Loss: 0.4940134286880493\n",
      "Epoch: 47, Loss: 0.46302929520606995\n",
      "Epoch: 48, Loss: 0.4558367133140564\n",
      "Epoch: 48, Loss: 0.40016430616378784\n",
      "Epoch: 49, Loss: 0.38348013162612915\n",
      "Epoch: 49, Loss: 0.4428229033946991\n",
      "Epoch: 50, Loss: 0.4216037690639496\n",
      "Epoch: 50, Loss: 0.5626053214073181\n",
      "Epoch: 51, Loss: 0.37292394042015076\n",
      "Epoch: 51, Loss: 0.4213704764842987\n",
      "Epoch: 52, Loss: 0.3592824935913086\n",
      "Epoch: 52, Loss: 0.44025927782058716\n",
      "Epoch: 53, Loss: 0.43552860617637634\n",
      "Epoch: 53, Loss: 0.540512204170227\n",
      "Epoch: 54, Loss: 0.5923984050750732\n",
      "Epoch: 54, Loss: 0.5128811597824097\n",
      "Epoch: 55, Loss: 0.38142871856689453\n",
      "Epoch: 55, Loss: 0.35540613532066345\n",
      "Epoch: 56, Loss: 0.39358747005462646\n",
      "Epoch: 56, Loss: 0.44220882654190063\n",
      "Epoch: 57, Loss: 0.36073583364486694\n",
      "Epoch: 57, Loss: 0.488149493932724\n",
      "Epoch: 58, Loss: 0.4180190861225128\n",
      "Epoch: 58, Loss: 0.41582122445106506\n",
      "Epoch: 59, Loss: 0.35762450098991394\n",
      "Epoch: 59, Loss: 0.3613675832748413\n",
      "Epoch: 60, Loss: 0.40749797224998474\n",
      "Epoch: 60, Loss: 0.3798663020133972\n",
      "Epoch: 61, Loss: 0.4909415543079376\n",
      "Epoch: 61, Loss: 0.40089917182922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:27,624] Trial 35 finished with value: 0.4123738813286312 and parameters: {'hidden_size': 229, 'dropout': 0.24820005644198162, 'lr': 0.00019308355255944528}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6271116137504578\n",
      "Epoch: 0, Loss: 0.6409357786178589\n",
      "Epoch: 1, Loss: 0.3928734064102173\n",
      "Epoch: 1, Loss: 0.3768504559993744\n",
      "Epoch: 2, Loss: 0.45734965801239014\n",
      "Epoch: 2, Loss: 0.5158106088638306\n",
      "Epoch: 3, Loss: 0.5074344873428345\n",
      "Epoch: 3, Loss: 0.37841182947158813\n",
      "Epoch: 4, Loss: 0.43740901350975037\n",
      "Epoch: 4, Loss: 0.3810364902019501\n",
      "Epoch: 5, Loss: 0.41631898283958435\n",
      "Epoch: 5, Loss: 0.420406311750412\n",
      "Epoch: 6, Loss: 0.34629857540130615\n",
      "Epoch: 6, Loss: 0.4390786290168762\n",
      "Epoch: 7, Loss: 0.49049562215805054\n",
      "Epoch: 7, Loss: 0.33984649181365967\n",
      "Epoch: 8, Loss: 0.39226770401000977\n",
      "Epoch: 8, Loss: 0.5250398516654968\n",
      "Epoch: 9, Loss: 0.3678412437438965\n",
      "Epoch: 9, Loss: 0.40770915150642395\n",
      "Epoch: 10, Loss: 0.4908808767795563\n",
      "Epoch: 10, Loss: 0.35359299182891846\n",
      "Epoch: 11, Loss: 0.4591810405254364\n",
      "Epoch: 11, Loss: 0.38074418902397156\n",
      "Epoch: 12, Loss: 0.4670509099960327\n",
      "Epoch: 12, Loss: 0.46184828877449036\n",
      "Epoch: 13, Loss: 0.45918259024620056\n",
      "Epoch: 13, Loss: 0.4112856090068817\n",
      "Epoch: 14, Loss: 0.4998040795326233\n",
      "Epoch: 14, Loss: 0.4977288544178009\n",
      "Epoch: 15, Loss: 0.6111800670623779\n",
      "Epoch: 15, Loss: 0.5094923973083496\n",
      "Epoch: 16, Loss: 0.48215413093566895\n",
      "Epoch: 16, Loss: 0.46050432324409485\n",
      "Epoch: 17, Loss: 0.3991691470146179\n",
      "Epoch: 17, Loss: 0.47548767924308777\n",
      "Epoch: 18, Loss: 0.5784335732460022\n",
      "Epoch: 18, Loss: 0.34814420342445374\n",
      "Epoch: 19, Loss: 0.36802783608436584\n",
      "Epoch: 19, Loss: 0.48304882645606995\n",
      "Epoch: 20, Loss: 0.5406520366668701\n",
      "Epoch: 20, Loss: 0.3371521234512329\n",
      "Epoch: 21, Loss: 0.36420994997024536\n",
      "Epoch: 21, Loss: 0.3376801311969757\n",
      "Epoch: 22, Loss: 0.35611194372177124\n",
      "Epoch: 22, Loss: 0.42571619153022766\n",
      "Epoch: 23, Loss: 0.4712497293949127\n",
      "Epoch: 23, Loss: 0.5679504871368408\n",
      "Epoch: 24, Loss: 0.3911544680595398\n",
      "Epoch: 24, Loss: 0.4929536283016205\n",
      "Epoch: 25, Loss: 0.4000156819820404\n",
      "Epoch: 25, Loss: 0.5187482833862305\n",
      "Epoch: 26, Loss: 0.4002211093902588\n",
      "Epoch: 26, Loss: 0.42694228887557983\n",
      "Epoch: 27, Loss: 0.3100166320800781\n",
      "Epoch: 27, Loss: 0.375222384929657\n",
      "Epoch: 28, Loss: 0.4831068217754364\n",
      "Epoch: 28, Loss: 0.4075595438480377\n",
      "Epoch: 29, Loss: 0.43831318616867065\n",
      "Epoch: 29, Loss: 0.5601819157600403\n",
      "Epoch: 30, Loss: 0.320530503988266\n",
      "Epoch: 30, Loss: 0.36966922879219055\n",
      "Epoch: 31, Loss: 0.46295803785324097\n",
      "Epoch: 31, Loss: 0.5611463189125061\n",
      "Epoch: 32, Loss: 0.549770712852478\n",
      "Epoch: 32, Loss: 0.4363239109516144\n",
      "Epoch: 33, Loss: 0.3836739659309387\n",
      "Epoch: 33, Loss: 0.3601364195346832\n",
      "Epoch: 34, Loss: 0.488055944442749\n",
      "Epoch: 34, Loss: 0.4697737395763397\n",
      "Epoch: 35, Loss: 0.4866527318954468\n",
      "Epoch: 35, Loss: 0.33213213086128235\n",
      "Epoch: 36, Loss: 0.4933420419692993\n",
      "Epoch: 36, Loss: 0.46529731154441833\n",
      "Epoch: 37, Loss: 0.42522478103637695\n",
      "Epoch: 37, Loss: 0.49230095744132996\n",
      "Epoch: 38, Loss: 0.4177422821521759\n",
      "Epoch: 38, Loss: 0.4102335572242737\n",
      "Epoch: 39, Loss: 0.47390761971473694\n",
      "Epoch: 39, Loss: 0.32347235083580017\n",
      "Epoch: 40, Loss: 0.32579687237739563\n",
      "Epoch: 40, Loss: 0.7254685759544373\n",
      "Epoch: 41, Loss: 0.510941207408905\n",
      "Epoch: 41, Loss: 0.4729872941970825\n",
      "Epoch: 42, Loss: 0.4079044461250305\n",
      "Epoch: 42, Loss: 0.41163697838783264\n",
      "Epoch: 43, Loss: 0.3580484092235565\n",
      "Epoch: 43, Loss: 0.5218529105186462\n",
      "Epoch: 44, Loss: 0.35655319690704346\n",
      "Epoch: 44, Loss: 0.4578305184841156\n",
      "Epoch: 45, Loss: 0.4005713164806366\n",
      "Epoch: 45, Loss: 0.39213618636131287\n",
      "Epoch: 46, Loss: 0.42358309030532837\n",
      "Epoch: 46, Loss: 0.5679659247398376\n",
      "Epoch: 47, Loss: 0.4033137857913971\n",
      "Epoch: 47, Loss: 0.4191286265850067\n",
      "Epoch: 48, Loss: 0.3474017381668091\n",
      "Epoch: 48, Loss: 0.47117775678634644\n",
      "Epoch: 49, Loss: 0.39907926321029663\n",
      "Epoch: 49, Loss: 0.39981958270072937\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:41,866] Trial 36 finished with value: 0.4154822923682676 and parameters: {'hidden_size': 218, 'dropout': 0.21754167860426038, 'lr': 0.0002628190116349351}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8276278376579285\n",
      "Epoch: 0, Loss: 0.36665114760398865\n",
      "Epoch: 1, Loss: 0.5138198733329773\n",
      "Epoch: 1, Loss: 0.39679861068725586\n",
      "Epoch: 2, Loss: 0.3111347556114197\n",
      "Epoch: 2, Loss: 0.4654383659362793\n",
      "Epoch: 3, Loss: 0.36017370223999023\n",
      "Epoch: 3, Loss: 0.3048732876777649\n",
      "Epoch: 4, Loss: 0.5395935773849487\n",
      "Epoch: 4, Loss: 0.411927729845047\n",
      "Epoch: 5, Loss: 0.42585161328315735\n",
      "Epoch: 5, Loss: 0.3824366331100464\n",
      "Epoch: 6, Loss: 0.47433987259864807\n",
      "Epoch: 6, Loss: 0.32592490315437317\n",
      "Epoch: 7, Loss: 0.306641161441803\n",
      "Epoch: 7, Loss: 0.5907979011535645\n",
      "Epoch: 8, Loss: 0.30703821778297424\n",
      "Epoch: 8, Loss: 0.3750962018966675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:44,466] Trial 37 finished with value: 0.42800659611416253 and parameters: {'hidden_size': 236, 'dropout': 0.23610586201351305, 'lr': 0.00021275519219199397}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7141053676605225\n",
      "Epoch: 0, Loss: 0.4957612156867981\n",
      "Epoch: 1, Loss: 0.4314635992050171\n",
      "Epoch: 1, Loss: 0.5551658868789673\n",
      "Epoch: 2, Loss: 0.6270381212234497\n",
      "Epoch: 2, Loss: 0.44469523429870605\n",
      "Epoch: 3, Loss: 0.43629902601242065\n",
      "Epoch: 3, Loss: 0.39032265543937683\n",
      "Epoch: 4, Loss: 0.4595312476158142\n",
      "Epoch: 4, Loss: 0.2855978012084961\n",
      "Epoch: 5, Loss: 0.4704415202140808\n",
      "Epoch: 5, Loss: 0.4212471544742584\n",
      "Epoch: 6, Loss: 0.3737679719924927\n",
      "Epoch: 6, Loss: 0.5362594723701477\n",
      "Epoch: 7, Loss: 0.4220590889453888\n",
      "Epoch: 7, Loss: 0.3754488229751587\n",
      "Epoch: 8, Loss: 0.32281750440597534\n",
      "Epoch: 8, Loss: 0.46989357471466064\n",
      "Epoch: 9, Loss: 0.39740967750549316\n",
      "Epoch: 9, Loss: 0.5063669681549072\n",
      "Epoch: 10, Loss: 0.4090690314769745\n",
      "Epoch: 10, Loss: 0.36381348967552185\n",
      "Epoch: 11, Loss: 0.42503082752227783\n",
      "Epoch: 11, Loss: 0.4793691337108612\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:47,934] Trial 38 finished with value: 0.42841428608535237 and parameters: {'hidden_size': 163, 'dropout': 0.24911859524221588, 'lr': 0.005730889320311669}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7427371144294739\n",
      "Epoch: 0, Loss: 0.4601782560348511\n",
      "Epoch: 1, Loss: 0.5404643416404724\n",
      "Epoch: 1, Loss: 0.35595518350601196\n",
      "Epoch: 2, Loss: 0.360617995262146\n",
      "Epoch: 2, Loss: 0.5195513963699341\n",
      "Epoch: 3, Loss: 0.314423531293869\n",
      "Epoch: 3, Loss: 0.4744381010532379\n",
      "Epoch: 4, Loss: 0.3766460418701172\n",
      "Epoch: 4, Loss: 0.3674538731575012\n",
      "Epoch: 5, Loss: 0.3848278522491455\n",
      "Epoch: 5, Loss: 0.4173975884914398\n",
      "Epoch: 6, Loss: 0.39916157722473145\n",
      "Epoch: 6, Loss: 0.5139745473861694\n",
      "Epoch: 7, Loss: 0.40171173214912415\n",
      "Epoch: 7, Loss: 0.5207732915878296\n",
      "Epoch: 8, Loss: 0.34524980187416077\n",
      "Epoch: 8, Loss: 0.5365312695503235\n",
      "Epoch: 9, Loss: 0.5657285451889038\n",
      "Epoch: 9, Loss: 0.4551917612552643\n",
      "Epoch: 10, Loss: 0.44549137353897095\n",
      "Epoch: 10, Loss: 0.33632177114486694\n",
      "Epoch: 11, Loss: 0.5059067606925964\n",
      "Epoch: 11, Loss: 0.5733465552330017\n",
      "Epoch: 12, Loss: 0.4928952157497406\n",
      "Epoch: 12, Loss: 0.38123294711112976\n",
      "Epoch: 13, Loss: 0.31228360533714294\n",
      "Epoch: 13, Loss: 0.3317677676677704\n",
      "Epoch: 14, Loss: 0.38832297921180725\n",
      "Epoch: 14, Loss: 0.4425554573535919\n",
      "Epoch: 15, Loss: 0.45247894525527954\n",
      "Epoch: 15, Loss: 0.543170690536499\n",
      "Epoch: 16, Loss: 0.4235134422779083\n",
      "Epoch: 16, Loss: 0.5411813259124756\n",
      "Epoch: 17, Loss: 0.30612802505493164\n",
      "Epoch: 17, Loss: 0.5607930421829224\n",
      "Epoch: 18, Loss: 0.43918588757514954\n",
      "Epoch: 18, Loss: 0.4613921046257019\n",
      "Epoch: 19, Loss: 0.4005860686302185\n",
      "Epoch: 19, Loss: 0.6045228838920593\n",
      "Epoch: 20, Loss: 0.4518742263317108\n",
      "Epoch: 20, Loss: 0.6382637023925781\n",
      "Epoch: 21, Loss: 0.2886374592781067\n",
      "Epoch: 21, Loss: 0.3773145079612732\n",
      "Epoch: 22, Loss: 0.3713645040988922\n",
      "Epoch: 22, Loss: 0.5197942852973938\n",
      "Epoch: 23, Loss: 0.3720620274543762\n",
      "Epoch: 23, Loss: 0.5374630689620972\n",
      "Epoch: 24, Loss: 0.34192562103271484\n",
      "Epoch: 24, Loss: 0.47460228204727173\n",
      "Epoch: 25, Loss: 0.27176111936569214\n",
      "Epoch: 25, Loss: 0.4679335951805115\n",
      "Epoch: 26, Loss: 0.43795499205589294\n",
      "Epoch: 26, Loss: 0.38148102164268494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:55,486] Trial 39 finished with value: 0.42549444641339484 and parameters: {'hidden_size': 192, 'dropout': 0.271295452352129, 'lr': 0.0004933016044528566}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.634751558303833\n",
      "Epoch: 0, Loss: 0.3227721154689789\n",
      "Epoch: 1, Loss: 0.4407879710197449\n",
      "Epoch: 1, Loss: 0.5172328948974609\n",
      "Epoch: 2, Loss: 0.32084736227989197\n",
      "Epoch: 2, Loss: 0.4188827574253082\n",
      "Epoch: 3, Loss: 0.3641853332519531\n",
      "Epoch: 3, Loss: 0.3641831874847412\n",
      "Epoch: 4, Loss: 0.4623240828514099\n",
      "Epoch: 4, Loss: 0.3600706160068512\n",
      "Epoch: 5, Loss: 0.46830326318740845\n",
      "Epoch: 5, Loss: 0.5238387584686279\n",
      "Epoch: 6, Loss: 0.4760659635066986\n",
      "Epoch: 6, Loss: 0.5075551271438599\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:24:57,524] Trial 40 finished with value: 0.42975827616564233 and parameters: {'hidden_size': 225, 'dropout': 0.11019959894648403, 'lr': 0.0015390722131768853}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6118379235267639\n",
      "Epoch: 0, Loss: 0.4216712713241577\n",
      "Epoch: 1, Loss: 0.4170805513858795\n",
      "Epoch: 1, Loss: 0.27953779697418213\n",
      "Epoch: 2, Loss: 0.6240262389183044\n",
      "Epoch: 2, Loss: 0.40868091583251953\n",
      "Epoch: 3, Loss: 0.38991931080818176\n",
      "Epoch: 3, Loss: 0.4931084215641022\n",
      "Epoch: 4, Loss: 0.4261399805545807\n",
      "Epoch: 4, Loss: 0.34847888350486755\n",
      "Epoch: 5, Loss: 0.6485743522644043\n",
      "Epoch: 5, Loss: 0.43297043442726135\n",
      "Epoch: 6, Loss: 0.6097636222839355\n",
      "Epoch: 6, Loss: 0.3415853679180145\n",
      "Epoch: 7, Loss: 0.6520900130271912\n",
      "Epoch: 7, Loss: 0.23939470946788788\n",
      "Epoch: 8, Loss: 0.37529098987579346\n",
      "Epoch: 8, Loss: 0.2842743694782257\n",
      "Epoch: 9, Loss: 0.4399546980857849\n",
      "Epoch: 9, Loss: 0.5561841726303101\n",
      "Epoch: 10, Loss: 0.4632228910923004\n",
      "Epoch: 10, Loss: 0.4233096241950989\n",
      "Epoch: 11, Loss: 0.5984839797019958\n",
      "Epoch: 11, Loss: 0.3824460208415985\n",
      "Epoch: 12, Loss: 0.4020361006259918\n",
      "Epoch: 12, Loss: 0.42441028356552124\n",
      "Epoch: 13, Loss: 0.45722249150276184\n",
      "Epoch: 13, Loss: 0.3763953745365143\n",
      "Epoch: 14, Loss: 0.5024711489677429\n",
      "Epoch: 14, Loss: 0.4728142023086548\n",
      "Epoch: 15, Loss: 0.4589815139770508\n",
      "Epoch: 15, Loss: 0.39439326524734497\n",
      "Epoch: 16, Loss: 0.5081427693367004\n",
      "Epoch: 16, Loss: 0.45454272627830505\n",
      "Epoch: 17, Loss: 0.3838343620300293\n",
      "Epoch: 17, Loss: 0.3796743154525757\n",
      "Epoch: 18, Loss: 0.328800231218338\n",
      "Epoch: 18, Loss: 0.38341382145881653\n",
      "Epoch: 19, Loss: 0.30621811747550964\n",
      "Epoch: 19, Loss: 0.33858832716941833\n",
      "Epoch: 20, Loss: 0.40168070793151855\n",
      "Epoch: 20, Loss: 0.3683894872665405\n",
      "Epoch: 21, Loss: 0.4711422324180603\n",
      "Epoch: 21, Loss: 0.3210701644420624\n",
      "Epoch: 22, Loss: 0.4421474039554596\n",
      "Epoch: 22, Loss: 0.38583600521087646\n",
      "Epoch: 23, Loss: 0.40042465925216675\n",
      "Epoch: 23, Loss: 0.5586636066436768\n",
      "Epoch: 24, Loss: 0.3673635423183441\n",
      "Epoch: 24, Loss: 0.5909770131111145\n",
      "Epoch: 25, Loss: 0.4145128130912781\n",
      "Epoch: 25, Loss: 0.37188228964805603\n",
      "Epoch: 26, Loss: 0.21920070052146912\n",
      "Epoch: 26, Loss: 0.2937788665294647\n",
      "Epoch: 27, Loss: 0.40072059631347656\n",
      "Epoch: 27, Loss: 0.35341352224349976\n",
      "Epoch: 28, Loss: 0.283848375082016\n",
      "Epoch: 28, Loss: 0.2946469187736511\n",
      "Epoch: 29, Loss: 0.43194150924682617\n",
      "Epoch: 29, Loss: 0.4071710407733917\n",
      "Epoch: 30, Loss: 0.36696386337280273\n",
      "Epoch: 30, Loss: 0.34309205412864685\n",
      "Epoch: 31, Loss: 0.3834517002105713\n",
      "Epoch: 31, Loss: 0.43756112456321716\n",
      "Epoch: 32, Loss: 0.45375004410743713\n",
      "Epoch: 32, Loss: 0.6659414768218994\n",
      "Epoch: 33, Loss: 0.43842390179634094\n",
      "Epoch: 33, Loss: 0.4541986882686615\n",
      "Epoch: 34, Loss: 0.35831424593925476\n",
      "Epoch: 34, Loss: 0.34004461765289307\n",
      "Epoch: 35, Loss: 0.40793585777282715\n",
      "Epoch: 35, Loss: 0.4000510275363922\n",
      "Epoch: 36, Loss: 0.4038991332054138\n",
      "Epoch: 36, Loss: 0.5659791231155396\n",
      "Epoch: 37, Loss: 0.35048672556877136\n",
      "Epoch: 37, Loss: 0.38493847846984863\n",
      "Epoch: 38, Loss: 0.4182755947113037\n",
      "Epoch: 38, Loss: 0.4138947129249573\n",
      "Epoch: 39, Loss: 0.4725401699542999\n",
      "Epoch: 39, Loss: 0.4071882963180542\n",
      "Epoch: 40, Loss: 0.41000497341156006\n",
      "Epoch: 40, Loss: 0.4534704387187958\n",
      "Epoch: 41, Loss: 0.4894832670688629\n",
      "Epoch: 41, Loss: 0.44495224952697754\n",
      "Epoch: 42, Loss: 0.4794469177722931\n",
      "Epoch: 42, Loss: 0.34337329864501953\n",
      "Epoch: 43, Loss: 0.3537553548812866\n",
      "Epoch: 43, Loss: 0.3833126425743103\n",
      "Epoch: 44, Loss: 0.443806529045105\n",
      "Epoch: 44, Loss: 0.4389176666736603\n",
      "Epoch: 45, Loss: 0.3961576521396637\n",
      "Epoch: 45, Loss: 0.4305187463760376\n",
      "Epoch: 46, Loss: 0.4658282995223999\n",
      "Epoch: 46, Loss: 0.4013974666595459\n",
      "Epoch: 47, Loss: 0.46548163890838623\n",
      "Epoch: 47, Loss: 0.42155981063842773\n",
      "Epoch: 48, Loss: 0.37174245715141296\n",
      "Epoch: 48, Loss: 0.3322797417640686\n",
      "Epoch: 49, Loss: 0.48634031414985657\n",
      "Epoch: 49, Loss: 0.48187747597694397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:25:11,192] Trial 41 finished with value: 0.41561344361673136 and parameters: {'hidden_size': 268, 'dropout': 0.30040903470044555, 'lr': 0.0001472421663238304}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.5109156370162964\n",
      "Epoch: 0, Loss: 0.5238698124885559\n",
      "Epoch: 1, Loss: 0.38962340354919434\n",
      "Epoch: 1, Loss: 0.33665260672569275\n",
      "Epoch: 2, Loss: 0.4903319180011749\n",
      "Epoch: 2, Loss: 0.6856141090393066\n",
      "Epoch: 3, Loss: 0.34391894936561584\n",
      "Epoch: 3, Loss: 0.5052558183670044\n",
      "Epoch: 4, Loss: 0.4181535542011261\n",
      "Epoch: 4, Loss: 0.3615402281284332\n",
      "Epoch: 5, Loss: 0.4864533841609955\n",
      "Epoch: 5, Loss: 0.3511711657047272\n",
      "Epoch: 6, Loss: 0.5145512819290161\n",
      "Epoch: 6, Loss: 0.3171069025993347\n",
      "Epoch: 7, Loss: 0.4176945090293884\n",
      "Epoch: 7, Loss: 0.33383142948150635\n",
      "Epoch: 8, Loss: 0.40452808141708374\n",
      "Epoch: 8, Loss: 0.5639935731887817\n",
      "Epoch: 9, Loss: 0.3505898416042328\n",
      "Epoch: 9, Loss: 0.36464324593544006\n",
      "Epoch: 10, Loss: 0.3470747172832489\n",
      "Epoch: 10, Loss: 0.30307310819625854\n",
      "Epoch: 11, Loss: 0.4355284869670868\n",
      "Epoch: 11, Loss: 0.3703177273273468\n",
      "Epoch: 12, Loss: 0.5315213203430176\n",
      "Epoch: 12, Loss: 0.4392206072807312\n",
      "Epoch: 13, Loss: 0.5120853185653687\n",
      "Epoch: 13, Loss: 0.3836974799633026\n",
      "Epoch: 14, Loss: 0.38763442635536194\n",
      "Epoch: 14, Loss: 0.3729774057865143\n",
      "Epoch: 15, Loss: 0.3818334937095642\n",
      "Epoch: 15, Loss: 0.40766772627830505\n",
      "Epoch: 16, Loss: 0.35495686531066895\n",
      "Epoch: 16, Loss: 0.42503345012664795\n",
      "Epoch: 17, Loss: 0.4585248529911041\n",
      "Epoch: 17, Loss: 0.30081596970558167\n",
      "Epoch: 18, Loss: 0.4684447646141052\n",
      "Epoch: 18, Loss: 0.3315756916999817\n",
      "Epoch: 19, Loss: 0.5102040767669678\n",
      "Epoch: 19, Loss: 0.43983525037765503\n",
      "Epoch: 20, Loss: 0.4494839012622833\n",
      "Epoch: 20, Loss: 0.4612407684326172\n",
      "Epoch: 21, Loss: 0.38140979409217834\n",
      "Epoch: 21, Loss: 0.38571181893348694\n",
      "Epoch: 22, Loss: 0.5132569670677185\n",
      "Epoch: 22, Loss: 0.3956611156463623\n",
      "Epoch: 23, Loss: 0.5492635369300842\n",
      "Epoch: 23, Loss: 0.3984208405017853\n",
      "Epoch: 24, Loss: 0.43324097990989685\n",
      "Epoch: 24, Loss: 0.7107635736465454\n",
      "Epoch: 25, Loss: 0.3960154950618744\n",
      "Epoch: 25, Loss: 0.4507865011692047\n",
      "Epoch: 26, Loss: 0.5401540994644165\n",
      "Epoch: 26, Loss: 0.5858458876609802\n",
      "Epoch: 27, Loss: 0.4499528706073761\n",
      "Epoch: 27, Loss: 0.4461849629878998\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:25:18,958] Trial 42 finished with value: 0.42567106522619724 and parameters: {'hidden_size': 288, 'dropout': 0.3125448536942983, 'lr': 0.00018977057605471984}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.697641909122467\n",
      "Epoch: 0, Loss: 0.33642393350601196\n",
      "Epoch: 1, Loss: 0.3781593441963196\n",
      "Epoch: 1, Loss: 0.5088818669319153\n",
      "Epoch: 2, Loss: 0.5438861846923828\n",
      "Epoch: 2, Loss: 0.43686643242836\n",
      "Epoch: 3, Loss: 0.5145899653434753\n",
      "Epoch: 3, Loss: 0.49094462394714355\n",
      "Epoch: 4, Loss: 0.4389941692352295\n",
      "Epoch: 4, Loss: 0.41192054748535156\n",
      "Epoch: 5, Loss: 0.34666872024536133\n",
      "Epoch: 5, Loss: 0.494228333234787\n",
      "Epoch: 6, Loss: 0.453975647687912\n",
      "Epoch: 6, Loss: 0.32378891110420227\n",
      "Epoch: 7, Loss: 0.43389034271240234\n",
      "Epoch: 7, Loss: 0.3954554498195648\n",
      "Epoch: 8, Loss: 0.48562756180763245\n",
      "Epoch: 8, Loss: 0.3917124271392822\n",
      "Epoch: 9, Loss: 0.3621903657913208\n",
      "Epoch: 9, Loss: 0.44615796208381653\n",
      "Epoch: 10, Loss: 0.4429987072944641\n",
      "Epoch: 10, Loss: 0.42968782782554626\n",
      "Epoch: 11, Loss: 0.5549191236495972\n",
      "Epoch: 11, Loss: 0.4128831923007965\n",
      "Epoch: 12, Loss: 0.4325103163719177\n",
      "Epoch: 12, Loss: 0.4415058195590973\n",
      "Epoch: 13, Loss: 0.4006352722644806\n",
      "Epoch: 13, Loss: 0.3918338716030121\n",
      "Epoch: 14, Loss: 0.3304668962955475\n",
      "Epoch: 14, Loss: 0.4639740586280823\n",
      "Epoch: 15, Loss: 0.36791518330574036\n",
      "Epoch: 15, Loss: 0.4444144666194916\n",
      "Epoch: 16, Loss: 0.3602117896080017\n",
      "Epoch: 16, Loss: 0.35134628415107727\n",
      "Epoch: 17, Loss: 0.4476259648799896\n",
      "Epoch: 17, Loss: 0.46379780769348145\n",
      "Epoch: 18, Loss: 0.3147085905075073\n",
      "Epoch: 18, Loss: 0.3275546133518219\n",
      "Epoch: 19, Loss: 0.37307804822921753\n",
      "Epoch: 19, Loss: 0.3952381908893585\n",
      "Epoch: 20, Loss: 0.49376100301742554\n",
      "Epoch: 20, Loss: 0.4420469403266907\n",
      "Epoch: 21, Loss: 0.3667733073234558\n",
      "Epoch: 21, Loss: 0.3964901566505432\n",
      "Epoch: 22, Loss: 0.3495575189590454\n",
      "Epoch: 22, Loss: 0.26042279601097107\n",
      "Epoch: 23, Loss: 0.4622393250465393\n",
      "Epoch: 23, Loss: 0.3694588840007782\n",
      "Epoch: 24, Loss: 0.43553727865219116\n",
      "Epoch: 24, Loss: 0.40545395016670227\n",
      "Epoch: 25, Loss: 0.5505822896957397\n",
      "Epoch: 25, Loss: 0.4252002239227295\n",
      "Epoch: 26, Loss: 0.4915318191051483\n",
      "Epoch: 26, Loss: 0.42806872725486755\n",
      "Epoch: 27, Loss: 0.3623233139514923\n",
      "Epoch: 27, Loss: 0.3078272342681885\n",
      "Epoch: 28, Loss: 0.2716839015483856\n",
      "Epoch: 28, Loss: 0.4898507595062256\n",
      "Epoch: 29, Loss: 0.42125922441482544\n",
      "Epoch: 29, Loss: 0.3930126130580902\n",
      "Epoch: 30, Loss: 0.4116249084472656\n",
      "Epoch: 30, Loss: 0.37954267859458923\n",
      "Epoch: 31, Loss: 0.27695173025131226\n",
      "Epoch: 31, Loss: 0.5248345732688904\n",
      "Epoch: 32, Loss: 0.6000254154205322\n",
      "Epoch: 32, Loss: 0.5009400844573975\n",
      "Epoch: 33, Loss: 0.489934504032135\n",
      "Epoch: 33, Loss: 0.25496309995651245\n",
      "Epoch: 34, Loss: 0.5211737155914307\n",
      "Epoch: 34, Loss: 0.3312719166278839\n",
      "Epoch: 35, Loss: 0.2961157560348511\n",
      "Epoch: 35, Loss: 0.33404427766799927\n",
      "Epoch: 36, Loss: 0.4097207188606262\n",
      "Epoch: 36, Loss: 0.4506961703300476\n",
      "Epoch: 37, Loss: 0.42240941524505615\n",
      "Epoch: 37, Loss: 0.35446786880493164\n",
      "Epoch: 38, Loss: 0.33964836597442627\n",
      "Epoch: 38, Loss: 0.30885887145996094\n",
      "Epoch: 39, Loss: 0.41482090950012207\n",
      "Epoch: 39, Loss: 0.416043758392334\n",
      "Epoch: 40, Loss: 0.3601265549659729\n",
      "Epoch: 40, Loss: 0.46766799688339233\n",
      "Epoch: 41, Loss: 0.41135746240615845\n",
      "Epoch: 41, Loss: 0.5108627080917358\n",
      "Epoch: 42, Loss: 0.474712997674942\n",
      "Epoch: 42, Loss: 0.3776884973049164\n",
      "Epoch: 43, Loss: 0.4217754006385803\n",
      "Epoch: 43, Loss: 0.42238378524780273\n",
      "Epoch: 44, Loss: 0.6052712798118591\n",
      "Epoch: 44, Loss: 0.32377493381500244\n",
      "Epoch: 45, Loss: 0.28473371267318726\n",
      "Epoch: 45, Loss: 0.369903028011322\n",
      "Epoch: 46, Loss: 0.3708759546279907\n",
      "Epoch: 46, Loss: 0.25865575671195984\n",
      "Epoch: 47, Loss: 0.4630725085735321\n",
      "Epoch: 47, Loss: 0.5212339758872986\n",
      "Epoch: 48, Loss: 0.43218398094177246\n",
      "Epoch: 48, Loss: 0.40163859724998474\n",
      "Epoch: 49, Loss: 0.33118876814842224\n",
      "Epoch: 49, Loss: 0.3160112500190735\n",
      "Epoch: 50, Loss: 0.4424085319042206\n",
      "Epoch: 50, Loss: 0.465801864862442\n",
      "Epoch: 51, Loss: 0.3707103133201599\n",
      "Epoch: 51, Loss: 0.34987446665763855\n",
      "Epoch: 52, Loss: 0.4780980348587036\n",
      "Epoch: 52, Loss: 0.32458800077438354\n",
      "Epoch: 53, Loss: 0.4411124587059021\n",
      "Epoch: 53, Loss: 0.4115274250507355\n",
      "Epoch: 54, Loss: 0.3839793801307678\n",
      "Epoch: 54, Loss: 0.543005645275116\n",
      "Epoch: 55, Loss: 0.5379880666732788\n",
      "Epoch: 55, Loss: 0.33609217405319214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:25:34,216] Trial 43 finished with value: 0.4143256602450187 and parameters: {'hidden_size': 249, 'dropout': 0.2876129763415412, 'lr': 0.0001008451139799531}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6892909407615662\n",
      "Epoch: 0, Loss: 0.3904135823249817\n",
      "Epoch: 1, Loss: 0.2867647111415863\n",
      "Epoch: 1, Loss: 0.4111950397491455\n",
      "Epoch: 2, Loss: 0.47075021266937256\n",
      "Epoch: 2, Loss: 0.533437192440033\n",
      "Epoch: 3, Loss: 0.5152962803840637\n",
      "Epoch: 3, Loss: 0.2706857919692993\n",
      "Epoch: 4, Loss: 0.3667164742946625\n",
      "Epoch: 4, Loss: 0.386931449174881\n",
      "Epoch: 5, Loss: 0.5360320210456848\n",
      "Epoch: 5, Loss: 0.4282771944999695\n",
      "Epoch: 6, Loss: 0.48412710428237915\n",
      "Epoch: 6, Loss: 0.4513421356678009\n",
      "Epoch: 7, Loss: 0.4568648934364319\n",
      "Epoch: 7, Loss: 0.4088757336139679\n",
      "Epoch: 8, Loss: 0.3626546263694763\n",
      "Epoch: 8, Loss: 0.4328317940235138\n",
      "Epoch: 9, Loss: 0.5055121779441833\n",
      "Epoch: 9, Loss: 0.37551558017730713\n",
      "Epoch: 10, Loss: 0.37418538331985474\n",
      "Epoch: 10, Loss: 0.30289387702941895\n",
      "Epoch: 11, Loss: 0.454725056886673\n",
      "Epoch: 11, Loss: 0.2578202486038208\n",
      "Epoch: 12, Loss: 0.44622382521629333\n",
      "Epoch: 12, Loss: 0.474077045917511\n",
      "Epoch: 13, Loss: 0.5689993500709534\n",
      "Epoch: 13, Loss: 0.3663308918476105\n",
      "Epoch: 14, Loss: 0.3239910900592804\n",
      "Epoch: 14, Loss: 0.439327210187912\n",
      "Epoch: 15, Loss: 0.42536836862564087\n",
      "Epoch: 15, Loss: 0.4696038067340851\n",
      "Epoch: 16, Loss: 0.3503533899784088\n",
      "Epoch: 16, Loss: 0.38759714365005493\n",
      "Epoch: 17, Loss: 0.5547321438789368\n",
      "Epoch: 17, Loss: 0.365552693605423\n",
      "Epoch: 18, Loss: 0.41839170455932617\n",
      "Epoch: 18, Loss: 0.3991512358188629\n",
      "Epoch: 19, Loss: 0.506344735622406\n",
      "Epoch: 19, Loss: 0.35820043087005615\n",
      "Epoch: 20, Loss: 0.3149779736995697\n",
      "Epoch: 20, Loss: 0.5569725036621094\n",
      "Epoch: 21, Loss: 0.32096627354621887\n",
      "Epoch: 21, Loss: 0.5652170181274414\n",
      "Epoch: 22, Loss: 0.30895787477493286\n",
      "Epoch: 22, Loss: 0.4581245481967926\n",
      "Epoch: 23, Loss: 0.3030052185058594\n",
      "Epoch: 23, Loss: 0.6513374447822571\n",
      "Epoch: 24, Loss: 0.4041718542575836\n",
      "Epoch: 24, Loss: 0.4608156383037567\n",
      "Epoch: 25, Loss: 0.279851496219635\n",
      "Epoch: 25, Loss: 0.3339233994483948\n",
      "Epoch: 26, Loss: 0.477486252784729\n",
      "Epoch: 26, Loss: 0.41508692502975464\n",
      "Epoch: 27, Loss: 0.438252717256546\n",
      "Epoch: 27, Loss: 0.4069688320159912\n",
      "Epoch: 28, Loss: 0.35994523763656616\n",
      "Epoch: 28, Loss: 0.3015396296977997\n",
      "Epoch: 29, Loss: 0.4878726303577423\n",
      "Epoch: 29, Loss: 0.49076443910598755\n",
      "Epoch: 30, Loss: 0.4165004789829254\n",
      "Epoch: 30, Loss: 0.4844546616077423\n",
      "Epoch: 31, Loss: 0.4381040632724762\n",
      "Epoch: 31, Loss: 0.45069798827171326\n",
      "Epoch: 32, Loss: 0.3135182559490204\n",
      "Epoch: 32, Loss: 0.3003285825252533\n",
      "Epoch: 33, Loss: 0.3879312574863434\n",
      "Epoch: 33, Loss: 0.3339637815952301\n",
      "Epoch: 34, Loss: 0.4435955584049225\n",
      "Epoch: 34, Loss: 0.5445085167884827\n",
      "Epoch: 35, Loss: 0.30505064129829407\n",
      "Epoch: 35, Loss: 0.4010431170463562\n",
      "Epoch: 36, Loss: 0.45417696237564087\n",
      "Epoch: 36, Loss: 0.41188105940818787\n",
      "Epoch: 37, Loss: 0.36299362778663635\n",
      "Epoch: 37, Loss: 0.4569457173347473\n",
      "Epoch: 38, Loss: 0.4953831434249878\n",
      "Epoch: 38, Loss: 0.5677765011787415\n",
      "Epoch: 39, Loss: 0.3577982187271118\n",
      "Epoch: 39, Loss: 0.4073520004749298\n",
      "Epoch: 40, Loss: 0.41988739371299744\n",
      "Epoch: 40, Loss: 0.46372437477111816\n",
      "Epoch: 41, Loss: 0.27516883611679077\n",
      "Epoch: 41, Loss: 0.46764305233955383\n",
      "Epoch: 42, Loss: 0.5034016370773315\n",
      "Epoch: 42, Loss: 0.5484446883201599\n",
      "Epoch: 43, Loss: 0.5125104784965515\n",
      "Epoch: 43, Loss: 0.5002529621124268\n",
      "Epoch: 44, Loss: 0.4305242598056793\n",
      "Epoch: 44, Loss: 0.33649155497550964\n",
      "Epoch: 45, Loss: 0.5798951387405396\n",
      "Epoch: 45, Loss: 0.33036601543426514\n",
      "Epoch: 46, Loss: 0.47497543692588806\n",
      "Epoch: 46, Loss: 0.40120643377304077\n",
      "Epoch: 47, Loss: 0.366055965423584\n",
      "Epoch: 47, Loss: 0.5937002897262573\n",
      "Epoch: 48, Loss: 0.37482279539108276\n",
      "Epoch: 48, Loss: 0.3467651903629303\n",
      "Epoch: 49, Loss: 0.3630343973636627\n",
      "Epoch: 49, Loss: 0.384950190782547\n",
      "Epoch: 50, Loss: 0.32675498723983765\n",
      "Epoch: 50, Loss: 0.5919525027275085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:25:48,983] Trial 44 finished with value: 0.41780284623852615 and parameters: {'hidden_size': 272, 'dropout': 0.24611251583625868, 'lr': 0.0001208461346633042}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.6959660053253174\n",
      "Epoch: 0, Loss: 0.4023086130619049\n",
      "Epoch: 1, Loss: 0.48259320855140686\n",
      "Epoch: 1, Loss: 0.43012863397598267\n",
      "Epoch: 2, Loss: 0.3677346408367157\n",
      "Epoch: 2, Loss: 0.34111490845680237\n",
      "Epoch: 3, Loss: 0.40128156542778015\n",
      "Epoch: 3, Loss: 0.46011969447135925\n",
      "Epoch: 4, Loss: 0.39526164531707764\n",
      "Epoch: 4, Loss: 0.5537604689598083\n",
      "Epoch: 5, Loss: 0.40444427728652954\n",
      "Epoch: 5, Loss: 0.4126347005367279\n",
      "Epoch: 6, Loss: 0.37426435947418213\n",
      "Epoch: 6, Loss: 0.3811018466949463\n",
      "Epoch: 7, Loss: 0.44038066267967224\n",
      "Epoch: 7, Loss: 0.4390643239021301\n",
      "Epoch: 8, Loss: 0.5319239497184753\n",
      "Epoch: 8, Loss: 0.25974905490875244\n",
      "Epoch: 9, Loss: 0.30812016129493713\n",
      "Epoch: 9, Loss: 0.35903337597846985\n",
      "Epoch: 10, Loss: 0.48262226581573486\n",
      "Epoch: 10, Loss: 0.3586776554584503\n",
      "Epoch: 11, Loss: 0.32945647835731506\n",
      "Epoch: 11, Loss: 0.4662114083766937\n",
      "Epoch: 12, Loss: 0.4899764060974121\n",
      "Epoch: 12, Loss: 0.1870235651731491\n",
      "Epoch: 13, Loss: 0.45667311549186707\n",
      "Epoch: 13, Loss: 0.37879109382629395\n",
      "Epoch: 14, Loss: 0.455320805311203\n",
      "Epoch: 14, Loss: 0.36259013414382935\n",
      "Epoch: 15, Loss: 0.38869956135749817\n",
      "Epoch: 15, Loss: 0.2734488844871521\n",
      "Epoch: 16, Loss: 0.5165361762046814\n",
      "Epoch: 16, Loss: 0.46252354979515076\n",
      "Epoch: 17, Loss: 0.37188103795051575\n",
      "Epoch: 17, Loss: 0.5207777619361877\n",
      "Epoch: 18, Loss: 0.42563238739967346\n",
      "Epoch: 18, Loss: 0.41281741857528687\n",
      "Epoch: 19, Loss: 0.4587358832359314\n",
      "Epoch: 19, Loss: 0.5947887301445007\n",
      "Epoch: 20, Loss: 0.32431790232658386\n",
      "Epoch: 20, Loss: 0.39930328726768494\n",
      "Epoch: 21, Loss: 0.3011285364627838\n",
      "Epoch: 21, Loss: 0.4547594487667084\n",
      "Epoch: 22, Loss: 0.3937760591506958\n",
      "Epoch: 22, Loss: 0.3627563416957855\n",
      "Epoch: 23, Loss: 0.40433356165885925\n",
      "Epoch: 23, Loss: 0.4210374653339386\n",
      "Epoch: 24, Loss: 0.47548937797546387\n",
      "Epoch: 24, Loss: 0.39440029859542847\n",
      "Epoch: 25, Loss: 0.37025710940361023\n",
      "Epoch: 25, Loss: 0.39476317167282104\n",
      "Epoch: 26, Loss: 0.4370064437389374\n",
      "Epoch: 26, Loss: 0.40026840567588806\n",
      "Epoch: 27, Loss: 0.3192808926105499\n",
      "Epoch: 27, Loss: 0.4227944314479828\n",
      "Epoch: 28, Loss: 0.39162206649780273\n",
      "Epoch: 28, Loss: 0.4609021246433258\n",
      "Epoch: 29, Loss: 0.4199000298976898\n",
      "Epoch: 29, Loss: 0.4968563914299011\n",
      "Epoch: 30, Loss: 0.3510495722293854\n",
      "Epoch: 30, Loss: 0.4313623607158661\n",
      "Epoch: 31, Loss: 0.46320757269859314\n",
      "Epoch: 31, Loss: 0.421272873878479\n",
      "Epoch: 32, Loss: 0.5928018093109131\n",
      "Epoch: 32, Loss: 0.4266858398914337\n",
      "Epoch: 33, Loss: 0.3737555742263794\n",
      "Epoch: 33, Loss: 0.42132049798965454\n",
      "Epoch: 34, Loss: 0.25016579031944275\n",
      "Epoch: 34, Loss: 0.48324528336524963\n",
      "Epoch: 35, Loss: 0.5687212944030762\n",
      "Epoch: 35, Loss: 0.43140745162963867\n",
      "Epoch: 36, Loss: 0.3558783233165741\n",
      "Epoch: 36, Loss: 0.4902101159095764\n",
      "Epoch: 37, Loss: 0.4581303894519806\n",
      "Epoch: 37, Loss: 0.37675613164901733\n",
      "Epoch: 38, Loss: 0.361423522233963\n",
      "Epoch: 38, Loss: 0.5165040493011475\n",
      "Epoch: 39, Loss: 0.5263193845748901\n",
      "Epoch: 39, Loss: 0.49520453810691833\n",
      "Epoch: 40, Loss: 0.41628018021583557\n",
      "Epoch: 40, Loss: 0.3835975229740143\n",
      "Epoch: 41, Loss: 0.49942001700401306\n",
      "Epoch: 41, Loss: 0.3591268062591553\n",
      "Epoch: 42, Loss: 0.3386368155479431\n",
      "Epoch: 42, Loss: 0.36692044138908386\n",
      "Epoch: 43, Loss: 0.4619307816028595\n",
      "Epoch: 43, Loss: 0.44374117255210876\n",
      "Epoch: 44, Loss: 0.26622509956359863\n",
      "Epoch: 44, Loss: 0.2714873254299164\n",
      "Epoch: 45, Loss: 0.5303834676742554\n",
      "Epoch: 45, Loss: 0.4238569736480713\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:26:02,409] Trial 45 finished with value: 0.42199275536196573 and parameters: {'hidden_size': 210, 'dropout': 0.3766255441081515, 'lr': 0.00037894369027433993}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6253012418746948\n",
      "Epoch: 0, Loss: 0.5313881635665894\n",
      "Epoch: 1, Loss: 0.4615197777748108\n",
      "Epoch: 1, Loss: 0.4166426658630371\n",
      "Epoch: 2, Loss: 0.42827826738357544\n",
      "Epoch: 2, Loss: 0.43044331669807434\n",
      "Epoch: 3, Loss: 0.4847976863384247\n",
      "Epoch: 3, Loss: 0.36723148822784424\n",
      "Epoch: 4, Loss: 0.48902952671051025\n",
      "Epoch: 4, Loss: 0.47706758975982666\n",
      "Epoch: 5, Loss: 0.38766390085220337\n",
      "Epoch: 5, Loss: 0.34464263916015625\n",
      "Epoch: 6, Loss: 0.48567965626716614\n",
      "Epoch: 6, Loss: 0.43597909808158875\n",
      "Epoch: 7, Loss: 0.32210659980773926\n",
      "Epoch: 7, Loss: 0.37382662296295166\n",
      "Epoch: 8, Loss: 0.40376579761505127\n",
      "Epoch: 8, Loss: 0.4271926283836365\n",
      "Epoch: 9, Loss: 0.3593493103981018\n",
      "Epoch: 9, Loss: 0.348323255777359\n",
      "Epoch: 10, Loss: 0.40576696395874023\n",
      "Epoch: 10, Loss: 0.40598544478416443\n",
      "Epoch: 11, Loss: 0.4413261413574219\n",
      "Epoch: 11, Loss: 0.42349857091903687\n",
      "Epoch: 12, Loss: 0.48339930176734924\n",
      "Epoch: 12, Loss: 0.29987582564353943\n",
      "Epoch: 13, Loss: 0.29881617426872253\n",
      "Epoch: 13, Loss: 0.2972351610660553\n",
      "Epoch: 14, Loss: 0.322969913482666\n",
      "Epoch: 14, Loss: 0.34678807854652405\n",
      "Epoch: 15, Loss: 0.43741345405578613\n",
      "Epoch: 15, Loss: 0.34796491265296936\n",
      "Epoch: 16, Loss: 0.45105472207069397\n",
      "Epoch: 16, Loss: 0.586017906665802\n",
      "Epoch: 17, Loss: 0.313170462846756\n",
      "Epoch: 17, Loss: 0.5796434283256531\n",
      "Epoch: 18, Loss: 0.30151137709617615\n",
      "Epoch: 18, Loss: 0.5759750604629517\n",
      "Epoch: 19, Loss: 0.374428391456604\n",
      "Epoch: 19, Loss: 0.34634828567504883\n",
      "Epoch: 20, Loss: 0.351130872964859\n",
      "Epoch: 20, Loss: 0.5020383596420288\n",
      "Epoch: 21, Loss: 0.44383570551872253\n",
      "Epoch: 21, Loss: 0.40429213643074036\n",
      "Epoch: 22, Loss: 0.4626600444316864\n",
      "Epoch: 22, Loss: 0.39165839552879333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:26:09,215] Trial 46 finished with value: 0.4321959930101861 and parameters: {'hidden_size': 292, 'dropout': 0.3287838839318462, 'lr': 0.0006667473067940841}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.696659505367279\n",
      "Epoch: 0, Loss: 0.5397957563400269\n",
      "Epoch: 1, Loss: 0.41091325879096985\n",
      "Epoch: 1, Loss: 0.40210825204849243\n",
      "Epoch: 2, Loss: 0.46452072262763977\n",
      "Epoch: 2, Loss: 0.3655879497528076\n",
      "Epoch: 3, Loss: 0.32143041491508484\n",
      "Epoch: 3, Loss: 0.3086499273777008\n",
      "Epoch: 4, Loss: 0.2737557888031006\n",
      "Epoch: 4, Loss: 0.4580695927143097\n",
      "Epoch: 5, Loss: 0.45522958040237427\n",
      "Epoch: 5, Loss: 0.5660635828971863\n",
      "Epoch: 6, Loss: 0.4563301205635071\n",
      "Epoch: 6, Loss: 0.24002724885940552\n",
      "Epoch: 7, Loss: 0.44636884331703186\n",
      "Epoch: 7, Loss: 0.4061492681503296\n",
      "Epoch: 8, Loss: 0.6256473660469055\n",
      "Epoch: 8, Loss: 0.41181156039237976\n",
      "Epoch: 9, Loss: 0.3780532777309418\n",
      "Epoch: 9, Loss: 0.47379055619239807\n",
      "Epoch: 10, Loss: 0.5798885226249695\n",
      "Epoch: 10, Loss: 0.29359662532806396\n",
      "Epoch: 11, Loss: 0.5153504610061646\n",
      "Epoch: 11, Loss: 0.38268545269966125\n",
      "Epoch: 12, Loss: 0.5596291422843933\n",
      "Epoch: 12, Loss: 0.39060744643211365\n",
      "Epoch: 13, Loss: 0.5201485753059387\n",
      "Epoch: 13, Loss: 0.42600253224372864\n",
      "Epoch: 14, Loss: 0.2919448912143707\n",
      "Epoch: 14, Loss: 0.3509291112422943\n",
      "Epoch: 15, Loss: 0.3330087959766388\n",
      "Epoch: 15, Loss: 0.3682222366333008\n",
      "Epoch: 16, Loss: 0.5399726033210754\n",
      "Epoch: 16, Loss: 0.49408063292503357\n",
      "Epoch: 17, Loss: 0.49147534370422363\n",
      "Epoch: 17, Loss: 0.2746049761772156\n",
      "Epoch: 18, Loss: 0.38445961475372314\n",
      "Epoch: 18, Loss: 0.4378688931465149\n",
      "Epoch: 19, Loss: 0.3486245274543762\n",
      "Epoch: 19, Loss: 0.3609219193458557\n",
      "Epoch: 20, Loss: 0.4030979871749878\n",
      "Epoch: 20, Loss: 0.3400750160217285\n",
      "Epoch: 21, Loss: 0.4347764253616333\n",
      "Epoch: 21, Loss: 0.36351078748703003\n",
      "Epoch: 22, Loss: 0.37124553322792053\n",
      "Epoch: 22, Loss: 0.5331109762191772\n",
      "Epoch: 23, Loss: 0.34682878851890564\n",
      "Epoch: 23, Loss: 0.38374853134155273\n",
      "Epoch: 24, Loss: 0.44815850257873535\n",
      "Epoch: 24, Loss: 0.4851825535297394\n",
      "Epoch: 25, Loss: 0.37512749433517456\n",
      "Epoch: 25, Loss: 0.37375667691230774\n",
      "Epoch: 26, Loss: 0.5074143409729004\n",
      "Epoch: 26, Loss: 0.48225101828575134\n",
      "Epoch: 27, Loss: 0.40347179770469666\n",
      "Epoch: 27, Loss: 0.35966211557388306\n",
      "Epoch: 28, Loss: 0.4946029782295227\n",
      "Epoch: 28, Loss: 0.38465777039527893\n",
      "Epoch: 29, Loss: 0.4475087523460388\n",
      "Epoch: 29, Loss: 0.3003956973552704\n",
      "Epoch: 30, Loss: 0.49773114919662476\n",
      "Epoch: 30, Loss: 0.5303339958190918\n",
      "Epoch: 31, Loss: 0.42270711064338684\n",
      "Epoch: 31, Loss: 0.3494615852832794\n",
      "Epoch: 32, Loss: 0.5332079529762268\n",
      "Epoch: 32, Loss: 0.5371549129486084\n",
      "Epoch: 33, Loss: 0.41913530230522156\n",
      "Epoch: 33, Loss: 0.3471846878528595\n",
      "Epoch: 34, Loss: 0.4663083851337433\n",
      "Epoch: 34, Loss: 0.41509100794792175\n",
      "Epoch: 35, Loss: 0.2995232939720154\n",
      "Epoch: 35, Loss: 0.4618440866470337\n",
      "Epoch: 36, Loss: 0.5803896188735962\n",
      "Epoch: 36, Loss: 0.5182203650474548\n",
      "Epoch: 37, Loss: 0.470693439245224\n",
      "Epoch: 37, Loss: 0.39878585934638977\n",
      "Epoch: 38, Loss: 0.3911532461643219\n",
      "Epoch: 38, Loss: 0.29093295335769653\n",
      "Epoch: 39, Loss: 0.49099284410476685\n",
      "Epoch: 39, Loss: 0.4552620053291321\n",
      "Epoch: 40, Loss: 0.4886130690574646\n",
      "Epoch: 40, Loss: 0.4666999280452728\n",
      "Epoch: 41, Loss: 0.2935764491558075\n",
      "Epoch: 41, Loss: 0.3954816460609436\n",
      "Epoch: 42, Loss: 0.48622238636016846\n",
      "Epoch: 42, Loss: 0.3139643371105194\n",
      "Epoch: 43, Loss: 0.32752296328544617\n",
      "Epoch: 43, Loss: 0.34897953271865845\n",
      "Epoch: 44, Loss: 0.43753811717033386\n",
      "Epoch: 44, Loss: 0.38985151052474976\n",
      "Epoch: 45, Loss: 0.3919878304004669\n",
      "Epoch: 45, Loss: 0.49774301052093506\n",
      "Epoch: 46, Loss: 0.307323157787323\n",
      "Epoch: 46, Loss: 0.4268924593925476\n",
      "Epoch: 47, Loss: 0.5394879579544067\n",
      "Epoch: 47, Loss: 0.6397903561592102\n",
      "Epoch: 48, Loss: 0.47992709279060364\n",
      "Epoch: 48, Loss: 0.3814486265182495\n",
      "Epoch: 49, Loss: 0.35447049140930176\n",
      "Epoch: 49, Loss: 0.43637439608573914\n",
      "Epoch: 50, Loss: 0.5328311920166016\n",
      "Epoch: 50, Loss: 0.43799129128456116\n",
      "Epoch: 51, Loss: 0.4763220548629761\n",
      "Epoch: 51, Loss: 0.27523308992385864\n",
      "Epoch: 52, Loss: 0.4615669250488281\n",
      "Epoch: 52, Loss: 0.48533037304878235\n",
      "Epoch: 53, Loss: 0.43931686878204346\n",
      "Epoch: 53, Loss: 0.368897944688797\n",
      "Epoch: 54, Loss: 0.6352388858795166\n",
      "Epoch: 54, Loss: 0.4424138069152832\n",
      "Epoch: 55, Loss: 0.5049766302108765\n",
      "Epoch: 55, Loss: 0.3715534210205078\n",
      "Epoch: 56, Loss: 0.35585108399391174\n",
      "Epoch: 56, Loss: 0.3119199573993683\n",
      "Epoch: 57, Loss: 0.5337679982185364\n",
      "Epoch: 57, Loss: 0.40811383724212646\n",
      "Epoch: 58, Loss: 0.3522244393825531\n",
      "Epoch: 58, Loss: 0.3679784834384918\n",
      "Epoch: 59, Loss: 0.4803627133369446\n",
      "Epoch: 59, Loss: 0.5661907196044922\n",
      "Epoch: 60, Loss: 0.5774940848350525\n",
      "Epoch: 60, Loss: 0.3034960627555847\n",
      "Epoch: 61, Loss: 0.49025821685791016\n",
      "Epoch: 61, Loss: 0.4013957381248474\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:26:26,966] Trial 47 finished with value: 0.4187603226417433 and parameters: {'hidden_size': 233, 'dropout': 0.3455410195238464, 'lr': 5.361887511710794e-05}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.819193959236145\n",
      "Epoch: 0, Loss: 0.37868186831474304\n",
      "Epoch: 1, Loss: 0.2224918007850647\n",
      "Epoch: 1, Loss: 0.5634194016456604\n",
      "Epoch: 2, Loss: 0.448588490486145\n",
      "Epoch: 2, Loss: 0.41938695311546326\n",
      "Epoch: 3, Loss: 0.540790855884552\n",
      "Epoch: 3, Loss: 0.3782906234264374\n",
      "Epoch: 4, Loss: 0.45985284447669983\n",
      "Epoch: 4, Loss: 0.4827806055545807\n",
      "Epoch: 5, Loss: 0.4091682434082031\n",
      "Epoch: 5, Loss: 0.43858450651168823\n",
      "Epoch: 6, Loss: 0.38509616255760193\n",
      "Epoch: 6, Loss: 0.5257836580276489\n",
      "Epoch: 7, Loss: 0.33883848786354065\n",
      "Epoch: 7, Loss: 0.3446692228317261\n",
      "Epoch: 8, Loss: 0.36768192052841187\n",
      "Epoch: 8, Loss: 0.3743918240070343\n",
      "Epoch: 9, Loss: 0.5035917162895203\n",
      "Epoch: 9, Loss: 0.36440473794937134\n",
      "Epoch: 10, Loss: 0.3789946138858795\n",
      "Epoch: 10, Loss: 0.3985324501991272\n",
      "Epoch: 11, Loss: 0.42013466358184814\n",
      "Epoch: 11, Loss: 0.4733745753765106\n",
      "Epoch: 12, Loss: 0.413896381855011\n",
      "Epoch: 12, Loss: 0.484529584646225\n",
      "Epoch: 13, Loss: 0.5250592231750488\n",
      "Epoch: 13, Loss: 0.5311992168426514\n",
      "Epoch: 14, Loss: 0.347621351480484\n",
      "Epoch: 14, Loss: 0.5414462685585022\n",
      "Epoch: 15, Loss: 0.4946654140949249\n",
      "Epoch: 15, Loss: 0.481212317943573\n",
      "Epoch: 16, Loss: 0.5725507140159607\n",
      "Epoch: 16, Loss: 0.4015503227710724\n",
      "Epoch: 17, Loss: 0.36635202169418335\n",
      "Epoch: 17, Loss: 0.3773781955242157\n",
      "Epoch: 18, Loss: 0.3754102885723114\n",
      "Epoch: 18, Loss: 0.293750137090683\n",
      "Epoch: 19, Loss: 0.4520123600959778\n",
      "Epoch: 19, Loss: 0.30874720215797424\n",
      "Epoch: 20, Loss: 0.4585650563240051\n",
      "Epoch: 20, Loss: 0.387223482131958\n",
      "Epoch: 21, Loss: 0.38062822818756104\n",
      "Epoch: 21, Loss: 0.47542592883110046\n",
      "Epoch: 22, Loss: 0.3387387990951538\n",
      "Epoch: 22, Loss: 0.3721260726451874\n",
      "Epoch: 23, Loss: 0.4248807430267334\n",
      "Epoch: 23, Loss: 0.49467602372169495\n",
      "Epoch: 24, Loss: 0.448240727186203\n",
      "Epoch: 24, Loss: 0.3730370104312897\n",
      "Epoch: 25, Loss: 0.4642767012119293\n",
      "Epoch: 25, Loss: 0.2768249809741974\n",
      "Epoch: 26, Loss: 0.4382365345954895\n",
      "Epoch: 26, Loss: 0.3882221579551697\n",
      "Epoch: 27, Loss: 0.4506029188632965\n",
      "Epoch: 27, Loss: 0.27718043327331543\n",
      "Epoch: 28, Loss: 0.4388214647769928\n",
      "Epoch: 28, Loss: 0.753669261932373\n",
      "Epoch: 29, Loss: 0.48326602578163147\n",
      "Epoch: 29, Loss: 0.366120308637619\n",
      "Epoch: 30, Loss: 0.35980066657066345\n",
      "Epoch: 30, Loss: 0.3863505721092224\n",
      "Epoch: 31, Loss: 0.4191253185272217\n",
      "Epoch: 31, Loss: 0.37312203645706177\n",
      "Epoch: 32, Loss: 0.3947993218898773\n",
      "Epoch: 32, Loss: 0.41875797510147095\n",
      "Epoch: 33, Loss: 0.4079228639602661\n",
      "Epoch: 33, Loss: 0.37646380066871643\n",
      "Epoch: 34, Loss: 0.3268570303916931\n",
      "Epoch: 34, Loss: 0.3576221168041229\n",
      "Epoch: 35, Loss: 0.47206321358680725\n",
      "Epoch: 35, Loss: 0.43864479660987854\n",
      "Epoch: 36, Loss: 0.3927114009857178\n",
      "Epoch: 36, Loss: 0.49068355560302734\n",
      "Epoch: 37, Loss: 0.4021071195602417\n",
      "Epoch: 37, Loss: 0.3612864911556244\n",
      "Epoch: 38, Loss: 0.44689077138900757\n",
      "Epoch: 38, Loss: 0.48117753863334656\n",
      "Epoch: 39, Loss: 0.3523813784122467\n",
      "Epoch: 39, Loss: 0.33459776639938354\n",
      "Epoch: 40, Loss: 0.3799293041229248\n",
      "Epoch: 40, Loss: 0.2730511426925659\n",
      "Epoch: 41, Loss: 0.6101665496826172\n",
      "Epoch: 41, Loss: 0.3679858446121216\n",
      "Epoch: 42, Loss: 0.423182874917984\n",
      "Epoch: 42, Loss: 0.28771722316741943\n",
      "Epoch: 43, Loss: 0.33526811003685\n",
      "Epoch: 43, Loss: 0.6294664144515991\n",
      "Epoch: 44, Loss: 0.4777040183544159\n",
      "Epoch: 44, Loss: 0.3817974030971527\n",
      "Epoch: 45, Loss: 0.3689422607421875\n",
      "Epoch: 45, Loss: 0.36702337861061096\n",
      "Epoch: 46, Loss: 0.45013803243637085\n",
      "Epoch: 46, Loss: 0.3845784366130829\n",
      "Epoch: 47, Loss: 0.48415958881378174\n",
      "Epoch: 47, Loss: 0.3180921673774719\n",
      "Epoch: 48, Loss: 0.3081873655319214\n",
      "Epoch: 48, Loss: 0.39178168773651123\n",
      "Epoch: 49, Loss: 0.4104776084423065\n",
      "Epoch: 49, Loss: 0.46162304282188416\n",
      "Epoch: 50, Loss: 0.4633314609527588\n",
      "Epoch: 50, Loss: 0.3337104022502899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:26:41,322] Trial 48 finished with value: 0.41597702891583216 and parameters: {'hidden_size': 262, 'dropout': 0.21974287493850617, 'lr': 0.0002444047659671073}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Epoch: 0, Loss: 0.7474607229232788\n",
      "Epoch: 0, Loss: 0.4360671937465668\n",
      "Epoch: 1, Loss: 0.3226762115955353\n",
      "Epoch: 1, Loss: 0.3721334934234619\n",
      "Epoch: 2, Loss: 0.3660534620285034\n",
      "Epoch: 2, Loss: 0.4229695498943329\n",
      "Epoch: 3, Loss: 0.5197665095329285\n",
      "Epoch: 3, Loss: 0.3630763590335846\n",
      "Epoch: 4, Loss: 0.2883955240249634\n",
      "Epoch: 4, Loss: 0.4346677362918854\n",
      "Epoch: 5, Loss: 0.49669063091278076\n",
      "Epoch: 5, Loss: 0.2768736481666565\n",
      "Epoch: 6, Loss: 0.3587530255317688\n",
      "Epoch: 6, Loss: 0.5562158226966858\n",
      "Epoch: 7, Loss: 0.41625258326530457\n",
      "Epoch: 7, Loss: 0.44215330481529236\n",
      "Epoch: 8, Loss: 0.38214632868766785\n",
      "Epoch: 8, Loss: 0.4937567114830017\n",
      "Epoch: 9, Loss: 0.4484466314315796\n",
      "Epoch: 9, Loss: 0.3577127754688263\n",
      "Epoch: 10, Loss: 0.4205235242843628\n",
      "Epoch: 10, Loss: 0.4780934453010559\n",
      "Epoch: 11, Loss: 0.31808555126190186\n",
      "Epoch: 11, Loss: 0.40855133533477783\n",
      "Epoch: 12, Loss: 0.40249010920524597\n",
      "Epoch: 12, Loss: 0.42081716656684875\n",
      "Epoch: 13, Loss: 0.27167925238609314\n",
      "Epoch: 13, Loss: 0.5841218829154968\n",
      "Epoch: 14, Loss: 0.31993570923805237\n",
      "Epoch: 14, Loss: 0.43477314710617065\n",
      "Epoch: 15, Loss: 0.46748027205467224\n",
      "Epoch: 15, Loss: 0.43604397773742676\n",
      "Epoch: 16, Loss: 0.32477548718452454\n",
      "Epoch: 16, Loss: 0.48833584785461426\n",
      "Epoch: 17, Loss: 0.44717687368392944\n",
      "Epoch: 17, Loss: 0.3744044303894043\n",
      "Epoch: 18, Loss: 0.5553910136222839\n",
      "Epoch: 18, Loss: 0.4169371724128723\n",
      "Epoch: 19, Loss: 0.3280770778656006\n",
      "Epoch: 19, Loss: 0.3721469044685364\n",
      "Epoch: 20, Loss: 0.37495553493499756\n",
      "Epoch: 20, Loss: 0.42710238695144653\n",
      "Epoch: 21, Loss: 0.42856594920158386\n",
      "Epoch: 21, Loss: 0.3595561981201172\n",
      "Epoch: 22, Loss: 0.4499078691005707\n",
      "Epoch: 22, Loss: 0.3260406255722046\n",
      "Epoch: 23, Loss: 0.4708693325519562\n",
      "Epoch: 23, Loss: 0.4554874897003174\n",
      "Epoch: 24, Loss: 0.4473531246185303\n",
      "Epoch: 24, Loss: 0.3375202417373657\n",
      "Epoch: 25, Loss: 0.3061286509037018\n",
      "Epoch: 25, Loss: 0.4487866759300232\n",
      "Epoch: 26, Loss: 0.44222134351730347\n",
      "Epoch: 26, Loss: 0.39178094267845154\n",
      "Epoch: 27, Loss: 0.3981570899486542\n",
      "Epoch: 27, Loss: 0.3099279999732971\n",
      "Epoch: 28, Loss: 0.3667427897453308\n",
      "Epoch: 28, Loss: 0.6771993637084961\n",
      "Epoch: 29, Loss: 0.25409257411956787\n",
      "Epoch: 29, Loss: 0.42256397008895874\n",
      "Epoch: 30, Loss: 0.482179194688797\n",
      "Epoch: 30, Loss: 0.4294443428516388\n",
      "Epoch: 31, Loss: 0.42918190360069275\n",
      "Epoch: 31, Loss: 0.43573176860809326\n",
      "Epoch: 32, Loss: 0.2927457392215729\n",
      "Epoch: 32, Loss: 0.39781349897384644\n",
      "Epoch: 33, Loss: 0.4444808065891266\n",
      "Epoch: 33, Loss: 0.2872627079486847\n",
      "Epoch: 34, Loss: 0.34615322947502136\n",
      "Epoch: 34, Loss: 0.4286087155342102\n",
      "Epoch: 35, Loss: 0.32374218106269836\n",
      "Epoch: 35, Loss: 0.46997249126434326\n",
      "Epoch: 36, Loss: 0.5777912735939026\n",
      "Epoch: 36, Loss: 0.32804811000823975\n",
      "Epoch: 37, Loss: 0.47868812084198\n",
      "Epoch: 37, Loss: 0.5480125546455383\n",
      "Epoch: 38, Loss: 0.3745591640472412\n",
      "Epoch: 38, Loss: 0.5371069312095642\n",
      "Epoch: 39, Loss: 0.2974455952644348\n",
      "Epoch: 39, Loss: 0.5467708110809326\n",
      "Epoch: 40, Loss: 0.4045882821083069\n",
      "Epoch: 40, Loss: 0.34252214431762695\n",
      "Epoch: 41, Loss: 0.47469043731689453\n",
      "Epoch: 41, Loss: 0.44213035702705383\n",
      "Epoch: 42, Loss: 0.34424206614494324\n",
      "Epoch: 42, Loss: 0.43025410175323486\n",
      "Epoch: 43, Loss: 0.3602827787399292\n",
      "Epoch: 43, Loss: 0.6535236835479736\n",
      "Epoch: 44, Loss: 0.33217674493789673\n",
      "Epoch: 44, Loss: 0.4034162759780884\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-03 15:26:54,399] Trial 49 finished with value: 0.41497972464732685 and parameters: {'hidden_size': 276, 'dropout': 0.26739005008931604, 'lr': 0.00014392726926503157}. Best is trial 22 with value: 0.41142106168145626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 185, 'dropout': 0.3267140666269757, 'lr': 0.00025937129423859296}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Setting up the hyperparameters in the objective function\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 50, 300)  # Hidden size between 50 and 300\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.01, 0.4)  # Dropout between 0.01 and 0.4\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)  # Learning rate between 1e-5 and 1e-2\n",
    "\n",
    "    # Setting up the model with the suggested hyperparameters\n",
    "    model = MyNeuralNetwork(hidden_size, dropout_rate, train_loader_20d).to(device)\n",
    "\n",
    "    # Training the model (pass the correct data loaders)\n",
    "    model.fit(train_loader_20d, val_loader=test_loader_20d, epochs=100, learning_rate=learning_rate)\n",
    "\n",
    "    # Evaluate the model on the test_loader (we use test_loader to evaluate performance)\n",
    "    val_loss = model.evaluate(test_loader_20d, criterion = nn.CrossEntropyLoss())  # Make sure I am using test_loader in the evaluation\n",
    "    # what is more, I cannot connect \"crtierion\" with the previous one, so I just connected it with nn.CrossEntropyLoss()\n",
    "\n",
    "    return val_loss  # Minimize the validation loss\n",
    "\n",
    "# Create and run the Optuna optimization study\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize the validation loss\n",
    "study.optimize(objective, n_trials=50)  # Number of trials for optimization\n",
    "\n",
    "# Print the best hyperparameters found by Optuna\n",
    "print(study.best_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee064942",
   "metadata": {},
   "source": [
    "Optuna shows the result that:\n",
    "\n",
    "[I 2025-03-03 15:26:54,399] Trial 49 finished with value: 0.41497972464732685 and parameters: {'hidden_size': 276, 'dropout': 0.26739005008931604, 'lr': 0.00014392726926503157}. Best is trial 22 with value: 0.41142106168145626.\n",
    "\n",
    "{'hidden_size': 185, 'dropout': 0.3267140666269757, 'lr': 0.00025937129423859296}\n",
    "\n",
    "Therefore, I will use the hypoparameter of 'hidden_size': 185, 'dropout': 0.3267140666269757, 'lr': 0.00025937129423859296 for optimized training -- both for 30 d embedding and 768d bert embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "Did you find the hyperparameter search helpful? Does it help to increase the number of trials in the optimization? Note that so far we have used the simplest version of optuna which has many nice features. Can you discover more useful features by browsing the optuna website? (Hint: try pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbffba",
   "metadata": {},
   "source": [
    "The hyperparameter search is very helpdul for it could mark the exact best hyperparameters from the search range listed-- it saves a lost of time because there are a lot of hyperparameters to consider, but optuna could solve them once and for all. \n",
    "\n",
    "It actually would be helpful if I increase the trials in the optimization-- more trial times could mean higher possibilities of finding a better solution. For example, ther was a test run that the best outcome was in the 99th trial. \n",
    "\n",
    "\n",
    "module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "The package \"samplers\" looks very interesting.\n",
    "Autosampler is a package that is loaded from OptunaHub, Optuna is a extension of Optuna used for simplize and automate the sampling method used in hyperparameter optimization. While AutoSampler could be used for choosing the suitable sampling method automatically for optimization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pruning is another thing that is very interesting.\n",
    "This is the code:\n",
    "\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "This is one function that could do \"early stopping\" to end the experiment in advance if the training proformance is not as expected. \n",
    "It could save computational cost and avoid overfitting as it stops the experiment when the loss is increasing in a high rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80318203",
   "metadata": {},
   "source": [
    "final test with 30d embedding\n",
    "\n",
    "Average Training Loss: 0.4365\n",
    "\n",
    "Test Error: \n",
    "\n",
    " Accuracy: 78.5%, Avg loss: 0.425930\n",
    "\n",
    " The accuracy did not change because the demension reduction would lose some information that could be used for classification.\n",
    "\n",
    "Final training with 768d bert embedding:\n",
    "\n",
    "Epoch 1\n",
    "-------------------------------\n",
    "loss: 0.964580  [    0/ 5124]\n",
    "loss: 0.566610  [  400/ 5124]\n",
    "loss: 0.894764  [  800/ 5124]\n",
    "loss: 0.741076  [ 1200/ 5124]\n",
    "loss: 0.510969  [ 1600/ 5124]\n",
    "loss: 0.402552  [ 2000/ 5124]\n",
    "loss: 0.684575  [ 2400/ 5124]\n",
    "loss: 0.250913  [ 2800/ 5124]\n",
    "loss: 0.698820  [ 3200/ 5124]\n",
    "loss: 0.480867  [ 3600/ 5124]\n",
    "loss: 0.509612  [ 4000/ 5124]\n",
    "loss: 0.483319  [ 4400/ 5124]\n",
    "loss: 0.742746  [ 4800/ 5124]\n",
    "Average Training Loss: 0.5555\n",
    "\n",
    "Test Error: \n",
    " Accuracy: 79.9%, Avg loss: 0.456661 \n",
    "\n",
    "Epoch 2\n",
    "-------------------------------\n",
    "loss: 0.509154  [    0/ 5124]\n",
    "loss: 0.479380  [  400/ 5124]\n",
    "loss: 0.626528  [  800/ 5124]\n",
    "loss: 0.397814  [ 1200/ 5124]\n",
    "...\n",
    "\n",
    "Average Training Loss: 0.4304\n",
    "\n",
    "Test Error:\n",
    "\n",
    " Accuracy: 84.6%, Avg loss: 0.318552\n",
    "  \n",
    "For 100 epochs training with the optimized hyperparameters being evaluated by Optuna, the outcome is : the average training loss has dropped to 0.4304 from 0.556, which is obvious. It is obvious that the accuracy and loos drop is sharper in BERT embeddings-- as it increase 768 demensions each tensor, it contains more information for the models to learn and fit.\n",
    "\n",
    "For test error, the accuracy has increased from 79.9% to 84.6%. \n",
    "\n",
    "The test average loss has dropped to 0.318 from 0.457. This is an obvious improvement.\n",
    "\n",
    "Due to the data now used is text embedding, there are still space for improvement: according to previous studies by Hirakawa, M. (2024), Null subjects and long-distance anaphors revisited: What the acquisition of Japanese vs. Chinese contributes to generative approaches to SLA. Japanese Native speakers act very differently in terms of pronouns than English native speakers when speaking English. Therefore, a possible solution in improving the accuracy would be: fine tuning the BERT to have more attention on the patterns of pronouns of both Japanese Natives and English Natives dataset. Therefore, capturing and focus on the differeces on pronoun patterns.\n",
    "\n",
    "Example: \n",
    "English Natives: \n",
    "I bought some apples in the store today.\n",
    "\n",
    "Japanese Natives:\n",
    "Some apples bought in the store today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
