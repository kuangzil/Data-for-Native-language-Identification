{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Model Training and Evaluation with PyTorch Lightning\n",
    "\n",
    "Welcome to Milestone 3 of LIS 640 – Introduction to Applied Deep Learning. In this milestone, you'll build upon your work from Milestones 1 and 2 by upgrading your neural network baseline to a more robust training framework using PyTorch Lightning and TensorBoard logging. You will also be exploring the advantages of different neural architectures (recurrent and convolutional neural networks) and different optimizers.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal of Milestone 3 is to:\n",
    "- **Explore advanced architectures:** The main goal of Milestone 3 is to strengthen your knowledge about and experience with popular neural architectures including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "- **Streamline your model development:** Make sure you are working with easy-to-maintain Lightning modules.\n",
    "- **Enhance experiment tracking:** Integrate TensorBoard to log and visualize training metrics, making it easier to monitor performance and debug issues.\n",
    "- **Investigate optimizer effects:** Experiment with different optimizers (such as Adam, SGD, and RMSprop) to understand their impact on model training and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Benchmarking Feedforward NN vs. RNN on Sequence Data\n",
    "\n",
    "In this step, you'll compare the performance of a Recurrent Neural Network (RNN) against a Feedforward Neural Network (FFNN) on a dataset that contains sequential data. **For this exercise, you must use PyTorch Lightning to build your models and manage the training loop, as well as TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains sequence data**.  \n",
    "  *For example, if your dataset involves time series, text, or any ordered data, it qualifies for this comparison.* In that case you have already done part B and can skip on to part C.\n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include sequence data, search online for and download a dataset that features sequential information (e.g., time series forecasting, text classification, sensor data, etc.). Take inspiration from previous milestones on how to do part B (Data Preparation) for your new dataset.\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your sequence data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, tokenization, padding for sequences).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse implementations from Milestone 2 if that makes sense. The key difference now is that you should implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline feedforward network that treats the sequence data as independent features (e.g., by flattening the sequence).\n",
    "   - Keep the architecture simple to establish a baseline for comparison.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN):**  \n",
    "   - Implement an RNN model (using LSTM or GRU) to handle the sequential nature of the data.\n",
    "   - Ensure that your model processes the sequence appropriately (e.g., using the final hidden state or an attention mechanism for prediction).\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for model training, and configure the module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the RNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard logging to visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific metric.\n",
    "   - Optionally, record additional statistics like training time or convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for the FFNN and RNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why that might be the case.\n",
    "   - Include TensorBoard screenshots or logged results to support your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# making word embeddings for rnn training\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# reading csv file \n",
    "corpus = pd.read_csv(r\"D:\\Applied Deep Learning\\data\\combined_DF_new\\combined_new_df.csv\")\n",
    "\n",
    "max_length = 128  \n",
    "batch_size = 16  \n",
    "#make data into batch of 16, and max length of 128\n",
    "\n",
    "# make bert embeddings\n",
    "def encode_text(text):\n",
    "    encoding = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    return encoding['input_ids'][0], encoding['attention_mask'][0]\n",
    "\n",
    "# convert all the texts into embeddings\n",
    "input_ids, attention_masks = zip(*corpus['text'].map(encode_text))\n",
    "input_ids = torch.stack(input_ids)           # shape: [num_samples, seq_len]\n",
    "attention_masks = torch.stack(attention_masks) # shape: [num_samples, seq_len]\n",
    "\n",
    "# building DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([5124, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids, batch_attention_mask = batch\n",
    "        outputs = bert_model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        # outputs.last_hidden_state: [batch_size, seq_len, 768]\n",
    "        all_embeddings.append(outputs.last_hidden_state)\n",
    "\n",
    "# combine all of the batches to get full embeddings\n",
    "# NOTE：shape of all_embeddings [num_samples, seq_len, 768]\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "print(\"Embeddings shape:\", all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SUMMARY\n",
    "Here, I still used the text dataset for embedding. As the data I am dealing with is text data, building up a time sequence for the tokens are necessary. THerefore, I set up the length for a time sequential data as 128 tokens for the embedding pattern learning (it was 512 at first but the memory exploded). Then. the embedding data were distributed in a form of this: [number of samples, length of tokens numbers for each sequential sample, word embedding demensions]. This data structure is suitable to feed to RNN model because it eaxmines sequential data. But when I am dealing with MLP, it can not work with sequential data, so I used mean(dim=1) to make the data look like this:[number of samples, word embedding demensions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=2):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)# lstm is used here because it is more suitable for sequential data\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)# linear layer to map the hidden state to the number of classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)  # only need the last hidden state\n",
    "        out = self.fc(hidden[-1])  # feeding the last hidden state to the classifier\n",
    "\n",
    "        return self.softmax(out)  # use softmax for multi-class classification\n",
    "\n",
    "\n",
    "# building up the model\n",
    "model = RNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likua\\AppData\\Local\\Temp\\ipykernel_3548\\1611909435.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_embeddings = torch.tensor(all_embeddings).float()  # convert to float tensor\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | RNNClassifier | 460 K  | train\n",
      "------------------------------------------------\n",
      "460 K     Trainable params\n",
      "0         Non-trainable params\n",
      "460 K     Total params\n",
      "1.840     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:01<00:00, 163.80it/s, v_num=2, val_loss_step=0.563, val_acc_step=0.750, val_loss_epoch=0.395, val_acc_epoch=0.917]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:01<00:00, 161.65it/s, v_num=2, val_loss_step=0.563, val_acc_step=0.750, val_loss_epoch=0.395, val_acc_epoch=0.917]\n"
     ]
    }
   ],
   "source": [
    "#use lightning to store and evaluate the model\n",
    "class RNNClassifierLightning(pl.LightningModule):\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=2, lr=0.001):\n",
    "        super(RNNClassifierLightning, self).__init__()\n",
    "\n",
    "        self.model = RNNClassifier(input_size, hidden_size, num_classes)\n",
    "        \n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', (y_hat.argmax(dim=1) == y).float().mean(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "logger = TensorBoardLogger('logs/', name='rnn_classifier')\n",
    "\n",
    "x_embeddings = torch.tensor(all_embeddings).float()  # convert to float tensor\n",
    "y_labels = torch.tensor(corpus['label'].values)  # convert to tensor\n",
    "\n",
    "LightningModel = RNNClassifierLightning()\n",
    "train_dataset = TensorDataset(x_embeddings, y_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "trainer = pl.Trainer(max_epochs=20,accelerator='auto',logger=logger)  # use GPU if available\n",
    "trainer.fit(LightningModel, train_dataloader)\n",
    "# Save the model\n",
    "trainer.save_checkpoint(r\"C:\\Users\\likua\\OneDrive\\Desktop\\Model\\rnn_classifier.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version_0', 'version_1']\n",
      "['epoch=19-step=6420.ckpt']\n",
      "Logs directory: d:\\Applied Deep Learning\\logs\\RNN_classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"logs/RNN_classifier\"))\n",
    "print(os.listdir('logs/RNN_classifier/version_0/checkpoints'))\n",
    "print(\"Logs directory:\", os.path.abspath('logs/RNN_classifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary -- RNN\n",
    "In this model class, I build up the model by using LSTM-- which is common among NLP tasks( I will use transformers everntually). Then, linear layer is used to map the tensors from 128 neurons to 2 numbers of classifications. Finally, softmax was used to convert the output to probabilities: Is the time sequential produced by \"English natives\"? Or it is made by Japanese?  Then, torch lightning has been used for model training and evaluation: not only did I used lightning, but also used logging for metrics evaluation-- learning about validation set loss and accuracy, in the end, I tried to store it in a .ckpt file as a practice of future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likua\\AppData\\Local\\Temp\\ipykernel_24980\\2101745372.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train = torch.tensor(all_embeddings, dtype=torch.float32)  # shape: (num_samples, seq_len, embedding_dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([5124, 128, 768])\n",
      "y_train shape: torch.Size([5124])\n",
      "Epoch 1, Loss: 0.5076, Accuracy: 0.8002\n",
      "Epoch 2, Loss: 0.4877, Accuracy: 0.8160\n",
      "Epoch 3, Loss: 0.4708, Accuracy: 0.8372\n",
      "Epoch 4, Loss: 0.4575, Accuracy: 0.8525\n",
      "Epoch 5, Loss: 0.4505, Accuracy: 0.8583\n",
      "Epoch 6, Loss: 0.4457, Accuracy: 0.8610\n",
      "Epoch 7, Loss: 0.4384, Accuracy: 0.8714\n",
      "Epoch 8, Loss: 0.4315, Accuracy: 0.8765\n",
      "Epoch 9, Loss: 0.4245, Accuracy: 0.8860\n",
      "Epoch 10, Loss: 0.4224, Accuracy: 0.8874\n",
      "Epoch 11, Loss: 0.4199, Accuracy: 0.8915\n",
      "Epoch 12, Loss: 0.4177, Accuracy: 0.8915\n",
      "Epoch 13, Loss: 0.4151, Accuracy: 0.8964\n",
      "Epoch 14, Loss: 0.4074, Accuracy: 0.9028\n",
      "Epoch 15, Loss: 0.4050, Accuracy: 0.9059\n",
      "Epoch 16, Loss: 0.4050, Accuracy: 0.9063\n",
      "Epoch 17, Loss: 0.4052, Accuracy: 0.9055\n",
      "Epoch 18, Loss: 0.4018, Accuracy: 0.9104\n",
      "Epoch 19, Loss: 0.3971, Accuracy: 0.9155\n",
      "Epoch 20, Loss: 0.3969, Accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "x_train = torch.tensor(all_embeddings, dtype=torch.float32)  # shape: (num_samples, seq_len, embedding_dim)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "\n",
    "y_train = torch.tensor(corpus['label'].values, dtype=torch.long)  # shape: (num_samples,)\n",
    "y_train = y_train.squeeze()\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_x = batch_x.to(torch.float32)\n",
    "        batch_y = batch_y.to(torch.long)\n",
    "        \n",
    "        outputs = model(batch_x)  # the outpur would be (batch_size, seq_len, num_classes)\n",
    "        if outputs.dim() == 3:\n",
    "            outputs = outputs[:, -1, :]  # retrive the last time step output\n",
    "        \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # accuracy calculation: the output of the model is (batch_size, num_classes), and the label is (batch_size,)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing it with MLP (FFNN)\n",
    "\n",
    "The following are the code for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfining MLP module class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1) #do not use softmax here, because it is used in the loss function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.mean(dim=1)# the shape of x was  (batch_size, seq_len, embedding_dim),but now[4,768]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # use softmax for multi-class classification\n",
    "logger = TensorBoardLogger('logs/', name='MLP_classifier')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# building up the model\n",
    "model = MLP()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_labels shape: torch.Size([5124])\n",
      "After squeeze, y_labels shape: torch.Size([5124])\n"
     ]
    }
   ],
   "source": [
    "y_labels = torch.tensor(corpus['label'].values).long()\n",
    "print(\"Original y_labels shape:\", y_labels.shape)\n",
    "y_labels = y_labels.squeeze()\n",
    "print(\"After squeeze, y_labels shape:\", y_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likua\\AppData\\Local\\Temp\\ipykernel_25060\\704573453.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_embeddings = torch.tensor(all_embeddings).float()  # convert to float tensor\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | MLP  | 98.7 K | train\n",
      "---------------------------------------\n",
      "98.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.7 K    Total params\n",
      "0.395     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:01<00:00, 196.64it/s, v_num=0, val_loss_step=0.530, val_acc_step=0.750, val_loss_epoch=0.276, val_acc_epoch=0.873] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:01<00:00, 195.46it/s, v_num=0, val_loss_step=0.530, val_acc_step=0.750, val_loss_epoch=0.276, val_acc_epoch=0.873]\n"
     ]
    }
   ],
   "source": [
    "# use lightning to store and evaluate the model\n",
    "class MLPClassifierLightning(pl.LightningModule):\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=2, lr=0.001):\n",
    "        super(MLPClassifierLightning, self).__init__()\n",
    "\n",
    "        self.model = MLP(input_size, hidden_size, num_classes)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)# we use the same loss here for fair comparison\n",
    "        acc= (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "x_embeddings = torch.tensor(all_embeddings).float()  # convert to float tensor\n",
    "y_labels = torch.tensor(corpus['label'].values).long().squeeze()\n",
    "  # convert to tensor, we need to make sure the label is long tensor\n",
    "\n",
    "train_dataset = TensorDataset(x_embeddings, y_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "LightningModel = MLPClassifierLightning()\n",
    "trainer = pl.Trainer(max_epochs=20, accelerator='auto',logger=logger)  # use GPU if available, DO remember to set the logger here, or it will not work\n",
    "trainer.fit(LightningModel, train_dataloader)\n",
    "# Save the model\n",
    "trainer.save_checkpoint(r\"C:\\Users\\likua\\OneDrive\\Desktop\\Model\\mlp_classifier.ckpt\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version_0']\n",
      "['epoch=19-step=6420.ckpt']\n",
      "Logs directory: d:\\Applied Deep Learning\\logs\\MLP_classifier\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"logs/MLP_classifier\"))\n",
    "print(os.listdir('logs/MLP_classifier/version_0/checkpoints'))\n",
    "print(\"Logs directory:\", os.path.abspath('logs/MLP_classifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "Same procedure as RNN but the structure of the model is: Linear, ReLU and Linear, and the data I fed to MLP is different from RNN -- as it can not work with sequential tagged tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN performance:186.54it/s, v_num=19, val_loss_step=0.316, val_acc_step=1.000, val_loss_epoch=0.387, val_acc_epoch=0.926\n",
    "\n",
    "MLP(FFNN) performance: 258.25it/s, v_num=16, val_loss_step=0.233, val_acc_step=1.000, val_loss_epoch=0.278, val_acc_epoch=0.870\n",
    "\n",
    "In this model training session, I setted up learning rate = 0.001, traning epochs=20, loss type as Crossentropy loss, optimizer as Adam.  Both models are trained busing the same parameter. For RNN, I set up the length of sentence as 128 to capture time sequence pattern in BERT embedding while when I am training MLP, I used mean(dim=1) to make the tensor into 1 demension to fit into the shape of MLP(without sequential processing)\n",
    "\n",
    "Performance:\n",
    "For RNN model, the traninig speed is 186.54 it/s while MLP model speed is 258.25it/s, MLP model is faster.\n",
    "For the validation set loss, RNN is 0.387 while MLP is 0.278. There is lower loss for MLP.\n",
    "But look at the validation set accuracy: the accuracy for RNN is 0.926 while the accuracy for MLP is 0.870. This means that when computation resource is allowed, RNN would perform better in accuracy, and it is a giant leap in terms of accuracy in prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Benchmarking Feedforward NN vs. CNN on Image Data\n",
    "\n",
    "In this step, you'll compare the performance of a Convolutional Neural Network (CNN) against a Feedforward Neural Network (FFNN) on an image-based dataset. **For this exercise, you must use PyTorch Lightning to implement your models and manage training, and use TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains image data**.  \n",
    "  *For example, if your dataset involves images for classification, segmentation, or any visual task, it qualifies for this comparison.*\n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include image data, search online for and download an image dataset (e.g., Fashion MNIST, CIFAR-10, or any domain-specific image dataset).\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your image data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, resizing, data augmentation).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and apply shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse or adapt implementations from Milestone 2 as needed. The key requirement is to implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline FFNN that treats image data as a flat vector (i.e., by flattening the image).\n",
    "   - Keep the architecture simple to serve as a baseline for comparison.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN):**  \n",
    "   - Implement a CNN architecture that leverages convolutional layers to capture spatial hierarchies in the image data.\n",
    "   - Typical layers might include convolution, activation (ReLU), pooling, and fully connected layers.\n",
    "   - Ensure that your model architecture is designed to process image data effectively.\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for training and to configure your Lightning module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the CNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard to log and visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific evaluation metric.\n",
    "   - Optionally, record additional details like training time and convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for both the FFNN and the CNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why, supported by TensorBoard screenshots or logged results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# CNN model for CV \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.optim as optim\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.system(\"tensorboard --logdir=lightning_logs\")\n",
    "log_dir=r'C:\\Users\\likua\\logs\\CNN_classifier'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "logger = TensorBoardLogger(log_dir, name='CNN_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='emnist',\n",
      "    full_name='emnist/letters/3.1.0',\n",
      "    description=\"\"\"\n",
      "    The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\n",
      "    \n",
      "    Note: Like the original EMNIST data, images provided here are inverted horizontally and rotated 90 anti-clockwise. You can use `tf.transpose` within `ds.map` to convert the images to a human-friendlier format.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    EMNIST Letters\n",
      "    \"\"\",\n",
      "    homepage='https://www.nist.gov/itl/products-and-services/emnist-dataset',\n",
      "    data_dir='C:\\\\Users\\\\likua\\\\tensorflow_datasets\\\\emnist\\\\letters\\\\3.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=535.73 MiB,\n",
      "    dataset_size=44.14 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=37),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=14800, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=88800, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{cohen_afshar_tapson_schaik_2017,\n",
      "        title={EMNIST: Extending MNIST to handwritten letters},\n",
      "        DOI={10.1109/ijcnn.2017.7966217},\n",
      "        journal={2017 International Joint Conference on Neural Networks (IJCNN)},\n",
      "        author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},\n",
      "        year={2017}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# loading tensorflow datasets for hand written English letters images\n",
    "dataset, info = tfds.load('emnist/letters', split='train',with_info=True,shuffle_files=True, as_supervised=True)\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEZBJREFUeJzt3H2sl3X9x/H3lwOdo0xujmIFOxxG0tIFQ2LhGB6IVejAwi2pppP+8Y9uNubW/RbHNlfDmzKlaVtmhVtzc5A3pJsFhDMmNSeTBgXkYWjGTSCCCAHn+v3xW++loJ7PJRy+HB6PjX/Orte5rnNAnlyin0ZVVVUAQEQMOtMPAEDzEAUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgXOaj09PdFoNOKOO+44ZZ9zzZo10Wg0Ys2aNafsc8LZQhTod7/85S+j0WjEX/7ylzP9KKfF3/72t7j55ptj+vTp0dbWFo1GI3p6ek567UMPPRQ33HBDTJgwIRqNRsyaNatfnxXeShTgFFu3bl3cfffdceDAgbj00kvf8dp77703Hnnkkejo6IiRI0f20xPC2xMFOMU+85nPxKuvvhovvPBCXH/99e947bJly2L//v2xatWqGD16dD89Ibw9UaAp/ec//4nFixfHxz72sRg+fHgMHTo0rrzyyli9evXbbn784x9HZ2dnnHfeeTFz5szYuHHjCdds3rw5Pve5z0V7e3u0tbXF1KlT49FHH33X5zl06FBs3rw59uzZ867Xtre3xwUXXPCu10VEdHR0xKBB/jGkefjVSFN67bXX4uc//3nMmjUrlixZErfcckvs3r075syZE88///wJ1//617+Ou+++O7761a/Gd77zndi4cWPMnj07du7cmdf89a9/jSuuuCI2bdoU3/72t+POO++MoUOHxvz582PFihXv+Dzr16+PSy+9NJYuXXqqv1RoKoPP9APAyYwcOTJ6enrife97X37spptuio985CNxzz33xP333/+m67du3RpbtmyJMWPGRETEVVddFdOmTYslS5bEj370o4iIWLRoUYwdOzb+/Oc/R2tra0REfOUrX4kZM2bEt771rbj22mv76auD5uVNgabU0tKSQejt7Y29e/fGsWPHYurUqfHcc8+dcP38+fMzCBERH//4x2PatGnxu9/9LiIi9u7dG6tWrYoFCxbEgQMHYs+ePbFnz57497//HXPmzIktW7bEyy+//LbPM2vWrKiqKm655ZZT+4VCkxEFmtavfvWrmDRpUrS1tcWFF14Yo0aNipUrV8b+/ftPuHbChAknfOzDH/5w/qegW7dujaqq4nvf+16MGjXqTT+6u7sjImLXrl2n9euBs4F/fURTevDBB+NLX/pSzJ8/P77xjW/ExRdfHC0tLfHDH/4wtm3bVvz5ent7IyLi61//esyZM+ek11xyySXv6ZlhIBAFmtLDDz8c48ePj+XLl0ej0ciP//dP9W+1ZcuWEz7297//PcaNGxcREePHj4+IiCFDhsQnP/nJU//AMED410c0pZaWloiIqKoqP/bss8/GunXrTnr9b3/72zf9ncD69evj2WefjauvvjoiIi6++OKYNWtW/OxnP4tXXnnlhP3u3bvf8XlK/pNUOJt5U+CM+cUvfhFPPvnkCR9ftGhRzJs3L5YvXx7XXnttzJ07N1588cW477774rLLLouDBw+esLnkkktixowZ8eUvfzmOHDkSd911V1x44YXxzW9+M6/56U9/GjNmzIiJEyfGTTfdFOPHj4+dO3fGunXr4qWXXooNGza87bOuX78+PvGJT0R3d/e7/mXz/v3745577omIiGeeeSYiIpYuXRojRoyIESNGxNe+9rW8du3atbF27dqI+P8wvf7663HrrbdGRERXV1d0dXW9473glKugnz3wwANVRLztjx07dlS9vb3VD37wg6qzs7NqbW2tLr/88urxxx+vFi5cWHV2dubnevHFF6uIqG6//fbqzjvvrDo6OqrW1tbqyiuvrDZs2HDCvbdt21bdeOON1Qc+8IFqyJAh1ZgxY6p58+ZVDz/8cF6zevXqKiKq1atXn/Cx7u7ud/36/vtMJ/vxv89eVVXV3d39ttf25V5wqjWq6n/ezwE4p/k7BQCSKACQRAGAJAoAJFEAIIkCAKnP//Pa/x41AMDZpy//B4I3BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApD4fiAdwKg0ePPB+++nt7e3X3engTQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGngnUgFTa7ZD4IbOXJk8aajo6N4M3PmzOJNRMSwYcNq7UrVOaRu7dq1te719NNPF29O1yF63hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDU3Mc1QkQMGlT+Z5fW1tbizZgxY4o3w4cPL950dXUVbyL673TQiRMnFm8mT55cvPngBz9YvImIGDJkSK1dqaNHj/bLfSIinnnmmeKNU1IBOO1EAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgORBvgBk8uHl/SuscUhcRMXv27OJNnUPn5s2bV7yp8zW9//3vL95E9N9BcHUOIKyz6U91Do87duxY8ebgwYPFm2bT3D+TAPQrUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASM17elqTqnMo2YgRI4o3HR0dxZuIiJkzZxZvhg0bVute/XWfz372s8WbsWPHFm+a+TDBZlfn8Lh9+/bVuterr75avHnhhReKN+vWrSverFixongTUe/7d7p4UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQGq6E8BaW1tr7T796U8Xb7q6uoo311xzTfFm5MiR/bKJcKjbe/HYY48Vb55//vniTZ0DEiMiFixYULxpNBrFm3vvvbd4U+d7t2PHjuJNRL0D8Y4fP1686e3tLd4MBN4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQGlVVVX26sMbBWnWMGzeu1u6hhx4q3lx++eXFm/46cO7YsWO1dvv27Sve1DlgrM7BhaNHjy7eRNT7ntf5/k2fPr14s2HDhuJN3cMOr7vuuuLNP//5z+LNE088Ubw5fPhw8Yb+15ff7r0pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqX+O/OwHBw8eLN4cOnSoeLNz587izWuvvVa8WbNmTfEmIuJPf/pT8WbTpk3Fm6uvvrp4s3jx4uJNRMTw4cNr7UodOHCgeHP06NHiza5du4o3ERH33Xdf8aaPhyC/SW9vb/GGgcObAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUtMdiPfyyy/X2i1atKh4c8UVVxRvnn766eLNkSNHijd1Dt6re69Bg8r/bHDBBRcUb84///ziTV11vqaJEycWb7Zu3Vq8OX78ePHmveyghDcFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkpjsQ7+jRo7V2GzduLN5s2rSpeDMQDyVrbW0t3gwbNqx402g0ijd1d3UOxPv85z9fvFm7dm3xZteuXcUb6C/eFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkJruQLz+NBAPt6vjqquuKt7ccMMNxZvBg5v7l9vcuXOLN3UOYlyyZEnxJiLi8OHDtXZQwpsCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQmvvYSvrF9OnTizft7e2n4UlO7tixY8WblpaW4k1bW1vx5pprrineLFu2rHgTEfGPf/yj1g5KeFMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEByIN4AU+dQt0996lPFm8GDy3/p7N27t3gTEfH4448Xb7q6uoo348aNK950dHQUb6ZMmVK8iYjYvn178eb48eO17sW5y5sCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSA/EGmNGjRxdvJkyYcBqe5EQ9PT21dnfccUe/3Gvx4sXFm/b29uLNggULijcREX/84x+LN7t37651L85d3hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAciNekWlpaau2mTJlSvGltbS3evP7668WbRx99tHgTEbFt27Z+udd3v/vd4s3gweX/CE2aNKl4ExExcuTI4o0D8SjlTQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAMmBeE2qs7Oz1u76668/xU9ycqtWrSrePPjgg7Xu9cYbbxRvduzYUbzZt29f8WbUqFHFm/b29uJNRMT06dOLN3W+D3W+3wwc3hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUqKqq6tOFjcbpfpazQp3vw4c+9KHize233168iYiYN29e8Wb79u3Fm5tvvrl4s3LlyuJNRERvb2/xZtCg8j/v3H///cWbL3zhC8Wb1tbW4k1ExOHDh4s3Tz31VPHm97//ffHmN7/5TfFmz549xRvem778du9NAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAafCZfoCzTWdnZ/Gmu7u7eDNnzpziTUS9g+CWLVtWvPnDH/5QvKlzsF1dde511113FW8mT55cvJk0aVLxJiKira2teDN37tzizdSpU4s3L730UvHmkUceKd5E9O+vo3ORNwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKRz+kC8RqNRvFm4cGHxZv78+cWbOoefRUTs2rWreFPnQLxDhw4Vb5rd5s2bize33XZb8eb73/9+8Sai3mGMLS0txZuLLrqoePPRj360ePPYY48VbyIciHe6eVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEA6pw/Eq3PA2I033li8GTp0aPHmjTfeKN5ERDzxxBPFm+3bt9e610Bz5MiR4s2KFSuKNwcPHizeRERMmTKlXzazZ88u3tT9mmg+3hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAGzIF4559/fvFm4cKFxZtx48YVbw4fPly8Wb58efEmIuLWW28t3hw/frzWvaj3c7ty5cpa93ryySeLN5dddlnx5rzzzivePPXUU8WbY8eOFW84/bwpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqVFVVdWnCxuN0/0sERFx0UUX1drddtttxZvrrruuePPKK68Ubx544IHizU9+8pPiTUTEoUOHau3gv1paWoo3Tto9O/Tlt3tvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASIPP9AO81dixY2vtZsyYUbz517/+Vbz54he/WLzp6ekp3jjYjjPF4XbnNm8KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIp/VAvJaWluLNtGnTat1r9+7dxZulS5cWb5577rniTVVVxRuAM8GbAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUqPq42ltjUaj+JMPGlTenMmTJxdvIiKOHDlSvNm6dWu/3AegGfTlt3tvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQDqtp6TWUfc+ffwyAM5ZTkkFoIgoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkwWf6Ad7KwXYAZ443BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApD4fiOegOoCBz5sCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAOn/AFv58l8hASgTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's  examine some samples for the dataset\n",
    "\n",
    "for example, label in dataset.take(5):\n",
    "    plt.imshow(example.numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {label.numpy()}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "# As you can see, the images are 90 degree rotated, so we need to spin them back to the original position.\n",
    "# Also, we need to resize the images to 28x28 and normalize them to [0,1] range. That's why we use the preprocess function below.\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalize to [0,1]\n",
    "    image = tf.transpose(image, perm=[1, 0, 2]) # spin the image to make it (height, width, channels)\n",
    "    image = tf.image.resize(image, [28, 28])  # resize to 28x28 for CNN input\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "logger = TensorBoardLogger('logs/', name='CNN_classifier')# remember to setting up logger variable to store the log model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's check agein the dataset\n",
    "for example, label in dataset.take(5):\n",
    "    plt.imshow(example.numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {label.numpy()}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Now it looks like they are in good shape -- we can recognize what those letters are! Let's initialize dataloader for training and build up the CNN model then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the dataset and dataloader and parameters\n",
    "batch_size = 32\n",
    "epochs=10\n",
    "lr=0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# use GPU if available\n",
    "transform= transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.EMNIST(root='data', split='letters', train=True, download=True, transform=transform)\n",
    "\n",
    "test_dataset = datasets.EMNIST(root='data', split='letters', train=False, download=True, transform=transform)\n",
    "\n",
    "# the original label is from 1 to 26, but we need to make it from 0 to 25 because the label is used as index in the model\n",
    "train_dataset.targets = train_dataset.targets - 1\n",
    "test_dataset.targets = test_dataset.targets - 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | con_layers | Sequential | 18.8 K | train\n",
      "1 | fc_layers  | Sequential | 1.6 M  | train\n",
      "--------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.553     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3900/3900 [00:40<00:00, 97.35it/s, v_num=5, train_loss_step=0.134, train_acc_step=0.938, val_loss_step=7.73e-6, val_acc_step=1.000, val_loss_epoch=0.194, val_acc_epoch=0.944, train_loss_epoch=0.116, train_acc_epoch=0.956]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3900/3900 [00:40<00:00, 97.21it/s, v_num=5, train_loss_step=0.134, train_acc_step=0.938, val_loss_step=7.73e-6, val_acc_step=1.000, val_loss_epoch=0.194, val_acc_epoch=0.944, train_loss_epoch=0.116, train_acc_epoch=0.956]\n"
     ]
    }
   ],
   "source": [
    "class CNNreaderlightning(pl.LightningModule):# be careful with the name, it should be the same as the class name\n",
    "    def __init__(self):\n",
    "        super(CNNreaderlightning, self).__init__()\n",
    "        self.con_layers= nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),#input channel is 1 because the image is grayscale and output channel is 32, knernel size is 3x3, stride is 1, padding is 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 26)# 26 classes for English letters\n",
    "        )\n",
    "# in the CNN model, I defined the convolutional layers and fully connected layers separately, \n",
    "# because the input of the fully connected layer should be 2D tensor, but the output of the convolutional layers is 4D tensor.\n",
    "# So I need to add a flatten layer to make it 2D tensor.\n",
    "    def forward(self, x):\n",
    "        x = self.con_layers(x)\n",
    "        x = self.fc_layers(x)# NOTE to add the flatten layer here, because the input of the fully connected layer should be 2D tensor\n",
    "        return x\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', (y_hat.argmax(dim=1) == y).float().mean(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', (y_hat.argmax(dim=1) == y).float().mean(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "# initializing the model and loss function and optimizer\n",
    "model = CNNreaderlightning().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "logger = TensorBoardLogger('logs/', name='CNN_classifier') \n",
    "# training the model\n",
    "trainer = pl.Trainer(max_epochs=epochs, accelerator='auto', logger=logger)  # use GPU if available\n",
    "trainer.fit(model, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['version_0', 'version_1', 'version_2', 'version_3', 'version_4']\n",
      "['epoch=9-step=39000.ckpt']\n",
      "Logs directory: d:\\Applied Deep Learning\\logs\\CNN_classifier\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"logs/CNN_classifier\"))\n",
    "print(os.listdir('logs/CNN_classifier/version_3/checkpoints'))\n",
    "print(\"Logs directory:\", os.path.abspath('logs/CNN_classifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I used EMNIST dataset for hand written letter classification: it turns out that there are 14800 samples and the images have been fliped for 90 degrees. After I checked the image, I did the normalization work, fliped them and made them the size of 28 x 28 so that CNN could work with it.\n",
    "\n",
    "Then, I converted normalized and resized image into tensors and normalized them into the range of [-1,1]. What is more, I also created Tensorboard to log the traning process(recording loss and accuracy) when using lightning.\n",
    "\n",
    "Then, I splited the tensors into training set and validation set using utils.dataset and dataloader.\n",
    "\n",
    "For the model build-up, I defined two layers of neural networks: the first one is the CNN itself, the second one is the fully connection layer-- it connects the tensors being processed by CNN into the shape of CrossEntropy loss and conducting the multi-classification task.\n",
    "\n",
    "For the CNN model itself, I firstly made the channel from 1 to 32, and 32 to 64 to extract more features using 32 convolutional filters and 64 convolutional filters and then, I used pooling to extract the outstanding features after each filter computation. \n",
    "\n",
    "The result is: 08.10it/s, v_num=0, train_loss_step=0.263, train_acc_step=0.875, val_loss_step=0.00319, val_acc_step=1.000, val_loss_epoch=0.188, val_acc_epoch=0.940, train_loss_epoch=0.124, train_acc_epoch=0.954 \n",
    "\n",
    "It shows that in validation step, the accuracy is 0.94, and the loss is 0.188. There is a relatively high accuracy for the classification task, and there is also a small loss -- meaning it is relatively successful for generalization for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Train Loss: 0.4654 Acc: 0.8534 | Test Loss: 0.2259 Acc: 0.9266\n",
      "Epoch [2/10] | Train Loss: 0.2596 Acc: 0.9143 | Test Loss: 0.1956 Acc: 0.9342\n",
      "Epoch [3/10] | Train Loss: 0.2192 Acc: 0.9264 | Test Loss: 0.1926 Acc: 0.9361\n",
      "Epoch [4/10] | Train Loss: 0.1912 Acc: 0.9355 | Test Loss: 0.1856 Acc: 0.9376\n",
      "Epoch [5/10] | Train Loss: 0.1716 Acc: 0.9406 | Test Loss: 0.1877 Acc: 0.9386\n",
      "Epoch [6/10] | Train Loss: 0.1567 Acc: 0.9445 | Test Loss: 0.1797 Acc: 0.9427\n",
      "Epoch [7/10] | Train Loss: 0.1439 Acc: 0.9481 | Test Loss: 0.1808 Acc: 0.9423\n",
      "Epoch [8/10] | Train Loss: 0.1364 Acc: 0.9512 | Test Loss: 0.1851 Acc: 0.9407\n",
      "Epoch [9/10] | Train Loss: 0.1264 Acc: 0.9533 | Test Loss: 0.1875 Acc: 0.9425\n",
      "Epoch [10/10] | Train Loss: 0.1191 Acc: 0.9551 | Test Loss: 0.2015 Acc: 0.9417\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(loader), correct/total\n",
    "\n",
    "# testing the model\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss/len(loader), correct/total\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Optimizers and Analyzing Training Curves\n",
    "\n",
    "In this step, you'll experiment with different optimizers—SGD, Adam, and RMSProp—to understand how they affect model performance. You will compare their effects using evaluation metrics on held-out test data and analyze the training and validation curves logged in TensorBoard.\n",
    "\n",
    "### A. Experiment Setup\n",
    "\n",
    "1. **Maintain Consistent Training Settings:**  \n",
    "   - Use the same model architecture (whether FFNN, CNN, or RNN from Parts 1 and 2) and dataset for all experiments.\n",
    "   - Ensure that the number of epochs, batch size, learning rate, and other hyperparameters are kept constant across different optimizer runs, aside from the optimizer itself.\n",
    "\n",
    "2. **Implement Optimizer Switching:**  \n",
    "   - Modify the `configure_optimizers` method in your PyTorch Lightning module to easily switch between optimizers:\n",
    "     ```python\n",
    "     def configure_optimizers(self):\n",
    "         # Uncomment the optimizer you want to use\n",
    "         # return torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "         # return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "         # return torch.optim.RMSprop(self.parameters(), lr=1e-3)\n",
    "     ```\n",
    "   - Train your model separately with each optimizer.\n",
    "\n",
    "### B. Evaluation Metrics and Analysis\n",
    "\n",
    "1. **Held-Out Test Evaluation:**  \n",
    "   - After training, evaluate each model on a held-out test set.\n",
    "   - Record quantitative metrics such as loss, accuracy, or any other relevant task-specific metric for each optimizer.\n",
    "\n",
    "2. **TensorBoard Analysis:**  \n",
    "   - Use TensorBoard to review the training and validation curves during training.\n",
    "   - Focus on:\n",
    "     - **Convergence Behavior:** How quickly does each optimizer reduce the loss?\n",
    "     - **Stability:** Are there noticeable fluctuations or instability in the curves?\n",
    "     - **Overfitting/Underfitting:** Do you observe signs of overfitting or underfitting, and how do these behaviors differ across optimizers?\n",
    "\n",
    "### C. Document Your Findings\n",
    "\n",
    "- **Summarize Performance:**  \n",
    "  - Create a table or a brief report comparing the evaluation metrics for SGD, Adam, and RMSProp.\n",
    "- **Include Visual Evidence:**  \n",
    "  - Attach TensorBoard screenshots or summaries of the logged training/validation curves.\n",
    "- **Provide a Comparative Analysis:**  \n",
    "  - Discuss which optimizer provided the best performance on the test set.\n",
    "  - Reflect on the convergence rates and stability differences you observed.\n",
    "  - Explain potential reasons for these differences based on your results.\n",
    "\n",
    "By the end of this exercise, you will have a deeper understanding of how different optimizers affect model training dynamics and performance. This insight is essential for making informed decisions when tuning models in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, I set up log for 3 different logs on tensor board to record different optimizer traning situations. \n",
    "I set up SGD and RMSprop into different logs so that I can make screen shots in a clear way-- the results are in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# For SGD\n",
    "logger_sgd = TensorBoardLogger('logs/', name='rnn_classifier_sgd')\n",
    "\n",
    "# For Adam\n",
    "logger_adam = TensorBoardLogger('logs/', name='rnn_classifier_adam')\n",
    "\n",
    "# For RMSprop\n",
    "logger_rmsprop = TensorBoardLogger('logs/', name='rnn_classifier_rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifierLightning(pl.LightningModule):\n",
    "    def __init__(self, input_size=768, hidden_size=128, num_classes=2, lr=0.001, optimizer_name='adam'):\n",
    "        super(RNNClassifierLightning, self).__init__()\n",
    "        self.model = RNNClassifier(input_size, hidden_size, num_classes)\n",
    "        self.lr = lr\n",
    "        self.optimizer_name = optimizer_name  # <- add this line\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == 'sgd':\n",
    "            return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        elif self.optimizer_name == 'rmsprop':\n",
    "            return torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "        else:  # default to Adam\n",
    "            return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory logs/rnn_classifier_sgd\\version_0\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | RNNClassifier | 460 K  | train\n",
      "------------------------------------------------\n",
      "460 K     Trainable params\n",
      "0         Non-trainable params\n",
      "460 K     Total params\n",
      "1.840     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\likua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:04<00:00, 68.62it/s, v_num=0, train_loss_step=0.667, train_acc_step=0.500, val_loss_step=0.667, val_acc_step=0.500, train_loss_epoch=0.546, train_acc_epoch=0.793, val_loss_epoch=0.546, val_acc_epoch=0.793]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:04<00:00, 68.42it/s, v_num=0, train_loss_step=0.667, train_acc_step=0.500, val_loss_step=0.667, val_acc_step=0.500, train_loss_epoch=0.546, train_acc_epoch=0.793, val_loss_epoch=0.546, val_acc_epoch=0.793]\n"
     ]
    }
   ],
   "source": [
    "LightningModel = RNNClassifierLightning(optimizer_name='sgd')\n",
    "trainer = pl.Trainer(max_epochs=20, logger=logger_sgd)\n",
    "trainer.fit(LightningModel, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | RNNClassifier | 460 K  | train\n",
      "------------------------------------------------\n",
      "460 K     Trainable params\n",
      "0         Non-trainable params\n",
      "460 K     Total params\n",
      "1.840     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:08<00:00, 38.30it/s, v_num=0, train_loss_step=0.557, train_acc_step=0.750, val_loss_step=0.557, val_acc_step=0.750, train_loss_epoch=0.384, train_acc_epoch=0.926, val_loss_epoch=0.384, val_acc_epoch=0.926]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 321/321 [00:08<00:00, 38.10it/s, v_num=0, train_loss_step=0.557, train_acc_step=0.750, val_loss_step=0.557, val_acc_step=0.750, train_loss_epoch=0.384, train_acc_epoch=0.926, val_loss_epoch=0.384, val_acc_epoch=0.926]\n"
     ]
    }
   ],
   "source": [
    "LightningModel = RNNClassifierLightning(optimizer_name='rmsprop')\n",
    "trainer = pl.Trainer(max_epochs=20, logger=logger_rmsprop)# DO remember set up different varibale here!\n",
    "trainer.fit(LightningModel, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "**What to Submit:**\n",
    "\n",
    "1. Your complete iPython notebook for Milestone 3 (including all code, outputs, and markdown explanations).\n",
    "2. A single PDF file that contains your entire report for the milestone, covering:\n",
    "   - Part 1: Benchmarking FFNN vs. RNN on sequence data.\n",
    "   - Part 2: (Any additional tasks, if applicable.)\n",
    "   - Part 3: Comparing optimizers and analyzing training curves.\n",
    "\n",
    "**How to Submit:**\n",
    "\n",
    "- Upload both your iPython notebook and the PDF report to Canvas.\n",
    "- Name your files clearly, for example:\n",
    "  - `YourName_Milestone3.ipynb`\n",
    "  - `YourName_Milestone3_Report.pdf`\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "- All submissions are due **4/18/21**.\n",
    "\n",
    "Happy Deep Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
